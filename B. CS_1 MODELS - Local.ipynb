{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzWR2nDkoQ_q"
   },
   "source": [
    "`PART B: Modeling:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQugFjeV19Pp"
   },
   "source": [
    "# **1. Importing Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20684,
     "status": "ok",
     "timestamp": 1641934679293,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "uexl7HXwS0zW",
    "outputId": "928a9964-26d0-42b7-8cdb-05cdb1d84ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9517,
     "status": "ok",
     "timestamp": 1641934688784,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "bn-eHOGUag5f",
    "outputId": "9efa57b5-e755-44c4-871e-bd50c8cb7e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (1.0.6)\n",
      "Requirement already satisfied: graphviz in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (1.19.2)\n",
      "Requirement already satisfied: plotly in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (5.10.0)\n",
      "Requirement already satisfied: six in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: scipy in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (1.6.2)\n",
      "Requirement already satisfied: matplotlib in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (3.5.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from catboost) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from pandas>=0.24.0->catboost) (2022.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from matplotlib->catboost) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from matplotlib->catboost) (9.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/rahul/anaconda3/envs/latest_python/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4ded40f9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import pickle\n",
    "import catboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''import tensorflow\n",
    "import tensorboard\n",
    "from tensorflow.keras import models\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.impute import SimpleImputer \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LSTM\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint'''\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yn2tH7Dhovw2"
   },
   "outputs": [],
   "source": [
    "#Reference:https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25Pw3kU2opfu"
   },
   "source": [
    "# **2. Evaluation Matrics:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3uQTthM9Lbu"
   },
   "source": [
    "#### **2.1 Root Mean Squared Error (RMSE):**\n",
    "RMSE represents the standard deviation of the residuals. It gives an idea about how large the residuals are being dispersed. Interpretability of RMSE is relatively high as compared with MSE as it possesses same units as that of the datapoints. RSME is useful when the lower residual values are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0re5gWO-7ARH"
   },
   "source": [
    "![rmse.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fS1ZlaVIopfz"
   },
   "outputs": [],
   "source": [
    "def compute_RMSE(y_true, y_pred):\n",
    "  from sklearn.metrics import mean_squared_error\n",
    "  RMSE = np.round(math.sqrt(mean_squared_error(y_true, y_pred)),2)\n",
    "  return RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRei2vh1FmYa"
   },
   "source": [
    "RMSE might be the most appropriate evaluation matrics for this case study because RMSE has below advantages:\n",
    "* The unit of RMSE is same as that of target variable, which makes it relatively more intrepretable in terms of measurement of units as compared to MSE.\n",
    "* RSME gives more weight to greater errors than MAE.  It has more strict behaviour while pennalizing larger errors.\n",
    "* As RMSE takes square of errors, it makes it more sensative to outliers in error distribution. \n",
    "* In this case study, after performing targer transformation (log-transformation), we can see that it follows a kind of normal distribution ***(Refer section 7.3)***. And many studies indicates that RSME is appropriate performance metrics for samples with normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIbIDOds9MCW"
   },
   "source": [
    "#### **2.2 Root Mean Squared Logarithmic Error (RMSLE):**\n",
    "\n",
    "RMSLE is basically a modification of RMSE. As name suggest, errors are calculated on the logarithmic scale.When the actual and predicted values are same, the RMSLE will become undefined as the log of zero is undefined. To prevent this, the constant ‘1’ is added to both actual and predicted values to prevent. Unlike MSE, it doesn’t penalize large errors differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxI0sSYm7H8a"
   },
   "source": [
    "![rmsle.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RazhD1mjopf2"
   },
   "outputs": [],
   "source": [
    "def compute_RMSLE(y_true, y_pred):\n",
    "  from sklearn.metrics import mean_squared_log_error\n",
    "  #Problem: Mean Squared Logarithmic Error cannot be used when targets contain negative values.\n",
    "  #Replace negative predicted values = 0 to avoid value errors.\n",
    "  y_pred_ = np.where(y_pred <0 , 0, y_pred)  \n",
    "  RMSLE = np.round(math.sqrt(mean_squared_log_error(y_true, y_pred_)),2)\n",
    "  return RMSLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Bb8Tw_uP_Xr"
   },
   "source": [
    "RMSLE is usually used when both predicted and true values are huge numbers and  we dont want to pennalise huge residual errors. Below is the comparison between RMSE and RMSLE values. [Ref.](https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/)\n",
    "* If both predicted and actual values are small: RMSE and RMSLE are same.\n",
    "* If either predicted or the actual value is big: RMSE > RMSLE\n",
    "* If both predicted and actual values are big: RMSE > RMSLE (RMSLE becomes almost negligible).\n",
    "\n",
    "<br>RMSLE might also be appropriate evaluation matrics for this case study because RMSLE adress some of the problems faced when we use RMSE as a matrics. [Ref.](https://https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)\n",
    "\n",
    "* **Robustness:** RMSLE is more robust than RMSE. In the case of RMSE, if the data contains extreme outliers, the error term can explode to a very high value. In RMSLE since log term is used, it scale down the effect of outliers. RMSLE errors does not gets affected much by outliers. We have many outliers in our dataset. (***Refer section 8.4.2.*** )\n",
    "* **Biased Penalty:** RMSLE value is high when predicted value is less than Actual value. On the other hand, RMSLE is low when Predicted value is more than the Actual value. This property can be helpful in this caase study as we are predicting the Energy consumptions how much energy any building would have consumed without any retrofit and users are gettng paid according to the difference between the energy consumed after retrofit and would have been consumed without any retrofit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yk7M2mH8cVo"
   },
   "source": [
    "#### **2.3\tCoefficient of Variation of the RMSE CV(RMSE):**\n",
    "Cofficient of variation(CV), is also known as relative standard drviation. It is a standardised measure of dispersion and often expressed as a pecentage. \n",
    "CV(RMSE) is a modified version of RMSE. RMSE is normalized by average of target variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d9FCnpMYS7i"
   },
   "source": [
    "\n",
    "![cvrsme.png](data:image/png;base64,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)\n",
    "\n",
    "<br> Where in, \n",
    "<br>\t`Y ̅i – Average of measured (target) values`\n",
    "<br>`\tN – Number of compared values`\n",
    "<br>\t`P – parameters = 1` \n",
    "<br>\t`Ypred – predicted values by model`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VkCHVevF-Gz-"
   },
   "outputs": [],
   "source": [
    "def compute_RMSE_CV(y_true, y_pred):\n",
    "\n",
    "  if not isinstance(y_true, np.ndarray):\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "  if not isinstance(y_pred, np.ndarray):\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "  sq_diff = list(np.square(y_true - y_pred))\n",
    "  sum_sq_diff = sum(sq_diff)\n",
    "  n = len(sq_diff)\n",
    "  p = 1\n",
    "\n",
    "  RMSE_CV = np.round((np.sqrt(sum_sq_diff / (n-p)))/ y_true.mean(),2)\n",
    "\n",
    "  return RMSE_CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dP4LrKm2m4v"
   },
   "source": [
    "#### **2.4\tManual Score Function in GridSearchCV:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNQusRF3oLs0"
   },
   "source": [
    "* Coefficient of variation explains the relative dispersion of data points in the data series around the mean.\n",
    "* Lower the ration is better. As per ASHRAE Guideline 14, a CV(RMSE) of and below (+/-)25% indicates a good model fit with acceptable predictive capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GGvt-Fmz2XT6"
   },
   "outputs": [],
   "source": [
    "#Manual Score Function in GridSearchCV\n",
    "score = make_scorer(compute_RMSE, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QN9j5-atCQQA"
   },
   "source": [
    "# **3.Loading Train Data:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9-SeMZ4GlMF"
   },
   "source": [
    "#### **3.1 Without Scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VSHzeHzL7OIS"
   },
   "outputs": [],
   "source": [
    "data_cleaned = pd.read_feather(\"preprocessed_data_train.ftr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyGEluymwb_Z"
   },
   "source": [
    "Dropping Unwanted Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6XZavrUceE5S"
   },
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned.drop(['site_id','dew_temperature', 'Ea', 'Es', 'year' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WVN8H-C87ORs"
   },
   "outputs": [],
   "source": [
    "#As the timestamps are available, we can perform Time-Based Splitting.\n",
    "data_cleaned = data_cleaned.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "#Removing Timestamp other unwanted features.\n",
    "data_cleaned = data_cleaned.drop(['meter_reading', 'meter_reading_corrected', 'timestamp', 'floor_count'], axis=1)\n",
    "\n",
    "#Setting up target variable.\n",
    "y = data_cleaned.meter_reading_transformed\n",
    "X = data_cleaned.drop(['meter_reading_transformed'], axis=1)\n",
    "\n",
    "#Splitting Data\n",
    "X_train, X_cv, y_train, y_cv  = train_test_split(X, y, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kI2kwB4NGqZU"
   },
   "source": [
    "#### **3.2 With Scaling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5ojXKHuGpVg"
   },
   "outputs": [],
   "source": [
    "data_cleaned_lr = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/preprocessed_data_train_scaled.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw-wpctf5Y_e"
   },
   "outputs": [],
   "source": [
    "data_cleaned_lr = data_cleaned_lr.drop(['site_id','dew_temperature', 'Ea', 'Es', 'year' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CioF8mpDGtVO"
   },
   "outputs": [],
   "source": [
    "#As the timestamps are available, we can perform Time-Based Splitting.\n",
    "data_cleaned_lr = data_cleaned_lr.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "#Removing Timestamp other unwanted features.\n",
    "data_cleaned_lr = data_cleaned_lr.drop(['meter_reading', 'meter_reading_corrected', 'timestamp', 'floor_count'], axis=1)\n",
    "\n",
    "#Setting up target variable.\n",
    "y_lr = data_cleaned_lr.meter_reading_transformed\n",
    "X_lr = data_cleaned_lr.drop(['meter_reading_transformed'], axis=1)\n",
    "\n",
    "#Splitting Data\n",
    "X_train_lr, X_cv_lr, y_train_lr, y_cv_lr  = train_test_split(X_lr, y_lr, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9HwH2qMHKfk"
   },
   "source": [
    "# **4. Regression Models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRe78EHPC6w_"
   },
   "source": [
    "### **4.1 Base Model:**\n",
    " Using **Linear Regression** as a Base Model:<br>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25438,
     "status": "ok",
     "timestamp": 1639588569331,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "hyBJY7Ar5KDt",
    "outputId": "e4974161-735d-4be9-e813-93fd7a3b0971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "For BASE Model the RMSLE values are \n",
      "    train RMSLE = 0.32 \n",
      "    CV RMSLE = 0.34\n",
      "--------------------------------------------------\n",
      "For BASE Model the RMSE values are \n",
      "    train RMSE = 1.44 \n",
      "    CV RMSE = 1.48\n",
      "--------------------------------------------------\n",
      "For BASE Model the RMSLE(CV) values are \n",
      "    train RMSE(CV) = 0.32  \n",
      "    CV RMSE(CV) = 0.33 \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Defining Model\n",
    "base_model = LinearRegression(n_jobs=-1)\n",
    "\n",
    "#Training the model (on scaled data)\n",
    "base_model.fit(X_train_lr, y_train_lr)\n",
    "\n",
    "#Predictions\n",
    "y_cv_pred = base_model.predict(X_cv_lr)\n",
    "y_train_pred = base_model.predict(X_train_lr)\n",
    "\n",
    "#Evaluation\n",
    "\n",
    "base_cv_RMSE = compute_RMSE(y_cv_lr, y_cv_pred)\n",
    "base_train_RMSE = compute_RMSE(y_train_lr, y_train_pred)\n",
    "\n",
    "base_CV_RMSLE = compute_RMSLE(y_cv_lr, y_cv_pred)\n",
    "base_train_RMSLE = compute_RMSLE(y_train_lr, y_train_pred)\n",
    "\n",
    "base_cv_RMSECV = compute_RMSE_CV(y_cv_lr, y_cv_pred)\n",
    "base_train_RMSECV = compute_RMSE_CV(y_train_lr, y_train_pred)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/base_model_Rev02.sav'\n",
    "joblib.dump(base_model, filename)\n",
    "\n",
    "#Report\n",
    "print(\"--\"*25)\n",
    "print(\"For BASE Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(base_train_RMSLE, base_CV_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"For BASE Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(base_train_RMSE, base_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"For BASE Model the RMSLE(CV) values are \\n    train RMSE(CV) = {0}  \\n    CV RMSE(CV) = {1} \".format(base_train_RMSECV, base_cv_RMSECV))\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhMmzqX87Iph"
   },
   "source": [
    "### **4.2 Decision Tree Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWLMfkguPxue"
   },
   "source": [
    "**Hyperparameter Tuning:**<br>\n",
    "Here we are using custom Scoring Function / Evaluation Metrics. i.e. \n",
    "* RMSLE\n",
    "* RMSE\n",
    "* RMSE-CV\n",
    "<br>Let's pass that in GridSearchCV. \n",
    "<br>***Reference***:*https://stackoverflow.com/questions/32401493/how-to-create-customize-your-own-scorer-function-in-scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70961,
     "status": "ok",
     "timestamp": 1639589048753,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "C187htomP6E7",
    "outputId": "7158b916-87d5-4cf6-8fe7-f987f5a0470f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 9.3 µs\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "The best parameters are : {'max_depth': 27, 'min_samples_split': 100}\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#GridSearchCV parameters\n",
    "dr_reg = DecisionTreeRegressor()\n",
    "hyper_parameters = {'max_depth':[27,50,100],\n",
    "                   'min_samples_split':[10,100,500]}\n",
    "folds = KFold(n_splits=3, shuffle=False, random_state=None)\n",
    "\n",
    "dr_reg_cv = GridSearchCV(estimator=dr_reg,\n",
    "                        param_grid= hyper_parameters,\n",
    "                        scoring=score,\n",
    "                        cv=folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs =15,\n",
    "                        verbose=4)\n",
    "\n",
    "#Training the model\n",
    "dr_reg_cv.fit(X_train[:2000000], y_train[:2000000])\n",
    "\n",
    "#Saving Results\n",
    "dr_reg_cv_df = pd.DataFrame(dr_reg_cv.cv_results_)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/dr_reg_cv_Rev02.pkl'\n",
    "joblib.dump(dr_reg_cv, filename)\n",
    "\n",
    "#saving the results into df\n",
    "cv_results = pd.DataFrame(dr_reg_cv.cv_results_)\n",
    "\n",
    "#Results\n",
    "print(\"The best parameters are : {0}\".format(dr_reg_cv.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3Ki3daL4v06"
   },
   "source": [
    "**Decision Tree with Best Parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268674,
     "status": "ok",
     "timestamp": 1639589317400,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "uarJkpKw4vH1",
    "outputId": "a45447c3-90a1-4a15-d9f8-f7adcc585afe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are : {'max_depth': 27, 'min_samples_split': 100}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/Case Study 01/Data/dt_reg_best_Rev02.sav']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Parameters are : {}\".format(dr_reg_cv.best_params_))\n",
    "print(\"--\"*25)\n",
    "\n",
    "dt_reg_best = dr_reg_cv.best_estimator_\n",
    "dt_reg_best.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = dt_reg_best.predict(X_train)\n",
    "y_cv_pred = dt_reg_best.predict(X_cv)\n",
    "\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/dt_reg_best_Rev02.sav'\n",
    "joblib.dump(dt_reg_best, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5845,
     "status": "ok",
     "timestamp": 1639589323211,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "b7Yuk4sLJnnQ",
    "outputId": "ec1aca3c-0185-4727-908f-385d3c6ff06a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For Decision Tree Regression Model the RMSLE values are \n",
      "    train RMSLE = 0.08 \n",
      "    CV RMSLE = 0.17\n",
      "--------------------------------------------------\n",
      "2.For Decision Tree Regression Model the RMSE values are \n",
      "    train RMSE = 0.32 \n",
      "    CV RMSE = 0.68\n",
      "--------------------------------------------------\n",
      "3.For Decision Tree Regression Model the RMSLE(CV) values are \n",
      "    train RMSE(CV) = 0.07  \n",
      "    CV RMSE(CV) = 0.15 \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "dt_reg_cv_RMSLE = compute_RMSLE(y_cv, y_cv_pred)\n",
    "dt_reg_train_RMSLE = compute_RMSLE(y_train, y_train_pred)\n",
    "\n",
    "dt_reg_cv_RMSE = compute_RMSE(y_cv, y_cv_pred)\n",
    "dt_reg_train_RMSE = compute_RMSE(y_train, y_train_pred)\n",
    "\n",
    "dt_reg_cv_RMSECV = compute_RMSE_CV(y_cv, y_cv_pred)\n",
    "dt_reg_train_RMSECV = compute_RMSE_CV(y_train, y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"1.For Decision Tree Regression Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(dt_reg_train_RMSLE, dt_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For Decision Tree Regression Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(dt_reg_train_RMSE, dt_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"3.For Decision Tree Regression Model the RMSLE(CV) values are \\n    train RMSE(CV) = {0}  \\n    CV RMSE(CV) = {1} \".format(dt_reg_train_RMSECV, dt_reg_cv_RMSECV))\n",
    "print(\"--\"*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNSEKP8jWdNL"
   },
   "source": [
    "### **4.3 Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnpXCtOdijkL"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 721654,
     "status": "ok",
     "timestamp": 1639590573109,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "MIhAamVFijkM",
    "outputId": "553dd372-29a0-4824-d7f2-5f531c146e25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 250building tree 2 of 250\n",
      "\n",
      "building tree 3 of 250\n",
      "building tree 4 of 250\n",
      "building tree 5 of 250\n",
      "building tree 6 of 250\n",
      "building tree 7 of 250\n",
      "building tree 8 of 250\n",
      "building tree 9 of 250\n",
      "building tree 10 of 250\n",
      "building tree 11 of 250\n",
      "building tree 12 of 250\n",
      "building tree 13 of 250\n",
      "building tree 14 of 250\n",
      "building tree 15 of 250\n",
      "building tree 16 of 250\n",
      "building tree 17 of 250\n",
      "building tree 18 of 250\n",
      "building tree 19 of 250\n",
      "building tree 20 of 250\n",
      "building tree 21 of 250\n",
      "building tree 22 of 250\n",
      "building tree 23 of 250\n",
      "building tree 24 of 250\n",
      "building tree 25 of 250\n",
      "building tree 26 of 250\n",
      "building tree 27 of 250\n",
      "building tree 28 of 250\n",
      "building tree 29 of 250\n",
      "building tree 30 of 250\n",
      "building tree 31 of 250\n",
      "building tree 32 of 250\n",
      "building tree 33 of 250\n",
      "building tree 34 of 250building tree 35 of 250\n",
      "\n",
      "building tree 36 of 250\n",
      "building tree 37 of 250\n",
      "building tree 38 of 250\n",
      "building tree 39 of 250\n",
      "building tree 40 of 250\n",
      "building tree 41 of 250\n",
      "building tree 42 of 250\n",
      "building tree 43 of 250\n",
      "building tree 44 of 250\n",
      "building tree 45 of 250\n",
      "building tree 46 of 250\n",
      "building tree 47 of 250\n",
      "building tree 48 of 250\n",
      "building tree 49 of 250\n",
      "building tree 50 of 250\n",
      "building tree 51 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   1 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=50)]: Done   2 tasks      | elapsed:  2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 52 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   3 tasks      | elapsed:  2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 53 of 250\n",
      "building tree 54 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   4 tasks      | elapsed:  2.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 55 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   5 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=50)]: Done   6 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 56 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   7 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 57 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   8 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 58 of 250\n",
      "building tree 59 of 250\n",
      "building tree 60 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done   9 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=50)]: Done  10 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=50)]: Done  11 tasks      | elapsed:  2.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 61 of 250\n",
      "building tree 62 of 250\n",
      "building tree 63 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  12 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=50)]: Done  13 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 64 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  14 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=50)]: Done  15 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 65 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  16 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 66 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  17 tasks      | elapsed:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 67 of 250\n",
      "building tree 68 of 250\n",
      "building tree 69 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  18 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  19 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 70 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  20 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  21 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 71 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  22 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 72 of 250\n",
      "building tree 73 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  23 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 74 of 250\n",
      "building tree 75 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  24 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  25 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  26 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 76 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  27 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  28 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 77 of 250\n",
      "building tree 78 of 250\n",
      "building tree 79 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  29 tasks      | elapsed:  2.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 80 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  30 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done  31 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 81 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  32 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=50)]: Done  33 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 82 of 250\n",
      "building tree 83 of 250\n",
      "building tree 84 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  34 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 85 of 250\n",
      "building tree 86 of 250\n",
      "building tree 87 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  35 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=50)]: Done  36 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=50)]: Done  37 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 88 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  38 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=50)]: Done  39 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 89 of 250\n",
      "building tree 90 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  40 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=50)]: Done  41 tasks      | elapsed:  2.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 91 of 250\n",
      "building tree 92 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  42 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 93 of 250\n",
      "building tree 94 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  43 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=50)]: Done  44 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=50)]: Done  45 tasks      | elapsed:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 95 of 250\n",
      "building tree 96 of 250building tree 97 of 250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  46 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=50)]: Done  47 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=50)]: Done  48 tasks      | elapsed:  2.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 98 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  49 tasks      | elapsed:  2.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 99 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  50 tasks      | elapsed:  3.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 100 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  51 tasks      | elapsed:  4.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 101 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  52 tasks      | elapsed:  4.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 102 of 250\n",
      "building tree 103 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  53 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=50)]: Done  54 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=50)]: Done  55 tasks      | elapsed:  4.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 104 of 250\n",
      "building tree 105 of 250\n",
      "building tree 106 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  56 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=50)]: Done  57 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=50)]: Done  58 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 107 of 250\n",
      "building tree 108 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  59 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 109 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  60 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 110 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  61 tasks      | elapsed:  4.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 111 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  62 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 112 of 250\n",
      "building tree 113 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  63 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=50)]: Done  64 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 114 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  65 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 115 of 250\n",
      "building tree 116 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  66 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 117 of 250building tree 118 of 250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  67 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=50)]: Done  68 tasks      | elapsed:  4.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 119 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  69 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=50)]: Done  70 tasks      | elapsed:  4.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 120 of 250\n",
      "building tree 121 of 250\n",
      "building tree 122 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  71 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=50)]: Done  72 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=50)]: Done  73 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 123 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  74 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 124 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  75 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 125 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  76 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 126 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  77 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 127 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  78 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=50)]: Done  79 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 128 of 250\n",
      "building tree 129 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  80 tasks      | elapsed:  4.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 130 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  81 tasks      | elapsed:  4.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 131 of 250\n",
      "building tree 132 of 250\n",
      "building tree 133 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  82 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=50)]: Done  83 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=50)]: Done  84 tasks      | elapsed:  4.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 134 of 250\n",
      "building tree 135 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  85 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=50)]: Done  86 tasks      | elapsed:  4.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 136 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  87 tasks      | elapsed:  4.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 137 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  88 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=50)]: Done  89 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=50)]: Done  90 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 138 of 250\n",
      "building tree 139 of 250\n",
      "building tree 140 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  91 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 141 of 250\n",
      "building tree 142 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  92 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 143 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  93 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=50)]: Done  94 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 144 of 250\n",
      "building tree 145 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  95 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 146 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  96 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 147 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  97 tasks      | elapsed:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 148 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done  98 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=50)]: Done  99 tasks      | elapsed:  5.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 149 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:  5.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 150 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 101 tasks      | elapsed:  6.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 151 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 102 tasks      | elapsed:  6.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 152 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 103 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 153 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 104 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 154 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 105 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=50)]: Done 106 tasks      | elapsed:  6.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 155 of 250\n",
      "building tree 156 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 107 tasks      | elapsed:  6.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 157 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 108 tasks      | elapsed:  6.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 158 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 109 tasks      | elapsed:  6.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 159 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 110 tasks      | elapsed:  6.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 160 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 111 tasks      | elapsed:  6.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 161 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 112 tasks      | elapsed:  6.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 162 of 250\n",
      "building tree 163 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 113 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=50)]: Done 114 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 164 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 115 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 165 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 116 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 166 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 117 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 167 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 118 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 168 of 250\n",
      "building tree 169 of 250\n",
      "building tree 170 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 119 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=50)]: Done 120 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=50)]: Done 121 tasks      | elapsed:  7.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 171 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 122 tasks      | elapsed:  7.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 172 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 123 tasks      | elapsed:  7.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 173 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 124 tasks      | elapsed:  7.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 174 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 125 tasks      | elapsed:  7.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 175 of 250\n",
      "building tree 176 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 126 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=50)]: Done 127 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=50)]: Done 128 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=50)]: Done 129 tasks      | elapsed:  7.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 177 of 250\n",
      "building tree 178 of 250\n",
      "building tree 179 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 130 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=50)]: Done 131 tasks      | elapsed:  7.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 180 of 250building tree 181 of 250\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 132 tasks      | elapsed:  7.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 182 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 133 tasks      | elapsed:  7.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 183 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 134 tasks      | elapsed:  7.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 184 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 135 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=50)]: Done 136 tasks      | elapsed:  7.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 185 of 250\n",
      "building tree 186 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 137 tasks      | elapsed:  7.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 187 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 138 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=50)]: Done 139 tasks      | elapsed:  7.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 188 of 250\n",
      "building tree 189 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 140 tasks      | elapsed:  7.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 190 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 141 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=50)]: Done 142 tasks      | elapsed:  7.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 191 of 250\n",
      "building tree 192 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 143 tasks      | elapsed:  7.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 193 of 250\n",
      "building tree 194 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 144 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=50)]: Done 145 tasks      | elapsed:  7.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 195 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 146 tasks      | elapsed:  7.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 196 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 147 tasks      | elapsed:  7.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 197 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 148 tasks      | elapsed:  7.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 198 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 149 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=50)]: Done 150 tasks      | elapsed:  7.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 199 of 250\n",
      "building tree 200 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 151 tasks      | elapsed:  8.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 201 of 250\n",
      "building tree 202 of 250\n",
      "building tree 203 of 250\n",
      "building tree 204 of 250\n",
      "building tree 205 of 250\n",
      "building tree 206 of 250\n",
      "building tree 207 of 250\n",
      "building tree 208 of 250\n",
      "building tree 209 of 250\n",
      "building tree 210 of 250\n",
      "building tree 211 of 250\n",
      "building tree 212 of 250\n",
      "building tree 213 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 162 out of 250 | elapsed:  9.2min remaining:  5.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 214 of 250\n",
      "building tree 215 of 250\n",
      "building tree 216 of 250\n",
      "building tree 217 of 250\n",
      "building tree 218 of 250\n",
      "building tree 219 of 250\n",
      "building tree 220 of 250\n",
      "building tree 221 of 250\n",
      "building tree 222 of 250\n",
      "building tree 223 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 173 out of 250 | elapsed:  9.4min remaining:  4.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 224 of 250\n",
      "building tree 225 of 250\n",
      "building tree 226 of 250\n",
      "building tree 227 of 250building tree 228 of 250\n",
      "\n",
      "building tree 229 of 250\n",
      "building tree 230 of 250\n",
      "building tree 231 of 250\n",
      "building tree 232 of 250\n",
      "building tree 233 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 184 out of 250 | elapsed:  9.7min remaining:  3.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 234 of 250\n",
      "building tree 235 of 250\n",
      "building tree 236 of 250\n",
      "building tree 237 of 250\n",
      "building tree 238 of 250\n",
      "building tree 239 of 250\n",
      "building tree 240 of 250\n",
      "building tree 241 of 250\n",
      "building tree 242 of 250\n",
      "building tree 243 of 250\n",
      "building tree 244 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 195 out of 250 | elapsed: 10.0min remaining:  2.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 245 of 250\n",
      "building tree 246 of 250\n",
      "building tree 247 of 250\n",
      "building tree 248 of 250\n",
      "building tree 249 of 250\n",
      "building tree 250 of 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 206 out of 250 | elapsed: 11.3min remaining:  2.4min\n",
      "[Parallel(n_jobs=50)]: Done 217 out of 250 | elapsed: 11.6min remaining:  1.8min\n",
      "[Parallel(n_jobs=50)]: Done 228 out of 250 | elapsed: 11.7min remaining:  1.1min\n",
      "[Parallel(n_jobs=50)]: Done 239 out of 250 | elapsed: 11.8min remaining:   32.5s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed: 12.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/Case Study 01/Data/RF_reg_depth15_Rev02.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Defining Model\n",
    "RF_reg = RandomForestRegressor(n_estimators=250, min_samples_split=50, max_features=0.3, \n",
    "                               max_depth=15, n_jobs=50, bootstrap=True, verbose=25)\n",
    "\n",
    "#Training the model (on scaled data)\n",
    "#RF_reg.fit(X_train, y_train)\n",
    "RF_reg.fit(X_train[:], y_train[:])\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/RF_reg_depth15_Rev02.sav'\n",
    "joblib.dump(RF_reg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6aBpZoIijkN"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "RF_reg = joblib.load('/content/drive/MyDrive/Case Study 01/Data/RF_reg_depth15_Rev02.sav')\n",
    "\n",
    "#Predictions\n",
    "y_cv_pred = RF_reg.predict(X_cv)\n",
    "y_train_pred = RF_reg.predict(X_train)\n",
    "\n",
    "\n",
    "#Evaluation\n",
    "RF_reg_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "RF_reg_train_RMSLE = compute_RMSLE(y_train[:], y_train_pred)\n",
    "\n",
    "RF_reg_cv_RMSE = compute_RMSE(y_cv[:], y_cv_pred)\n",
    "RF_reg_train_RMSE = compute_RMSE(y_train[:], y_train_pred)\n",
    "\n",
    "RF_reg_cv_RMSE_CV = compute_RMSE_CV(y_cv[:], y_cv_pred)\n",
    "RF_reg_train_RMSE_CV = compute_RMSE_CV(y_train[:], y_train_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1639590611399,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "0xs_4MKFKOjh",
    "outputId": "5011d963-04b8-45fd-cd8b-87f7e90de17c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "1.For Random Forest the RMSLE values are \n",
      "    train RMSLE = 0.17 \n",
      "    CV RMSLE = 0.2\n",
      "--------------------------------------------------\n",
      "2.For Random Forest the RMSE values are \n",
      "    train RMSE = 0.65 \n",
      "    CV RMSLE = 0.76\n",
      "--------------------------------------------------\n",
      "3.For Random Forest Regression Model the RMSLE(CV) values are \n",
      "    train RMSE(CV) = 0.14  \n",
      "    CV RMSE(CV) = 0.17 \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Report\n",
    "print(\"--\"*25)\n",
    "print(\"1.For Random Forest the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(RF_reg_train_RMSLE, RF_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For Random Forest the RMSE values are \\n    train RMSE = {0} \\n    CV RMSLE = {1}\".format(RF_reg_train_RMSE, RF_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"3.For Random Forest Regression Model the RMSLE(CV) values are \\n    train RMSE(CV) = {0}  \\n    CV RMSE(CV) = {1} \".format(RF_reg_train_RMSE_CV, RF_reg_cv_RMSE_CV))\n",
    "print(\"--\"*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvisReBV2fL3"
   },
   "source": [
    "### **4.4 LGBM Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2yQIkcFaBsg"
   },
   "source": [
    "##### **4.4.1 Model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "troUMS3TtE-w"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530093,
     "status": "ok",
     "timestamp": 1639591333884,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "1wPXZrvRtFB0",
    "outputId": "59a52e59-cebf-4ff8-ef31-61b5003f4f73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LGBM_reg_Rev02_Aug15.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reference: https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "LGBM_reg = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='rmse', n_estimators=2100)\n",
    "\n",
    "LGBM_reg.fit(X_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'LGBM_reg_Rev02_Aug15.sav'\n",
    "joblib.dump(LGBM_reg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7--lvsuDrfH"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "LGBM_reg = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_Rev02.sav')\n",
    "\n",
    "#Predictions\n",
    "y_cv_pred = LGBM_reg.predict(X_cv)\n",
    "y_train_pred = LGBM_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105287,
     "status": "ok",
     "timestamp": 1639591439155,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "X6YPIWILBOS0",
    "outputId": "5935ff4a-5e2d-4ffc-aab4-615566b8b19e"
   },
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "LGBM_reg_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSLE = compute_RMSLE(y_train[:], y_train_pred)\n",
    "\n",
    "LGBM_reg_cv_RMSE = compute_RMSE(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSE = compute_RMSE(y_train[:], y_train_pred)\n",
    "\n",
    "LGBM_reg_cv_RMSECV = compute_RMSE_CV(y_cv, y_cv_pred)\n",
    "LGBM_reg_train_RMSECV = compute_RMSE_CV(y_train, y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"1.For LGBM_reg the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(LGBM_reg_train_RMSLE, LGBM_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For LGBM_reg the RMSE values are \\n    train RMSE = {0} \\n    CV RMSLE = {1}\".format(LGBM_reg_train_RMSE, LGBM_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"3.For LGBM_reg the RMSLE(CV) values are \\n    train RMSE(CV) = {0}  \\n    CV RMSE(CV) = {1} \".format(LGBM_reg_train_RMSECV, LGBM_reg_cv_RMSECV))\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff3R0MXmGCca"
   },
   "source": [
    "##### **4.4.2 Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7325800,
     "status": "ok",
     "timestamp": 1639604847595,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "DFMXwdmWBlg8",
    "outputId": "590bb1fd-6fe8-4872-aca0-b3d2c4f3d494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  55.9s\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  53.6s\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  52.3s\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.3min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.3min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=2000;, score=(train=-0.580, test=-0.580) total time= 3.2min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.8min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.7min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=20, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.7min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=500;, score=(train=-0.760, test=-0.760) total time=  54.6s\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  54.9s\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=500;, score=(train=-0.760, test=-0.760) total time=  54.2s\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=2000;, score=(train=-0.580, test=-0.590) total time= 3.1min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.8min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=2500;, score=(train=-0.560, test=-0.570) total time= 3.7min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=31, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.8min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=500;, score=(train=-0.760, test=-0.760) total time=  54.9s\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=500;, score=(train=-0.760, test=-0.760) total time=  55.8s\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  55.5s\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.5min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.5min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=2000;, score=(train=-0.580, test=-0.590) total time= 3.1min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.2min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.7min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=2500;, score=(train=-0.570, test=-0.570) total time= 3.7min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=50, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.8min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  54.4s\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=500;, score=(train=-0.760, test=-0.760) total time=  55.8s\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=500;, score=(train=-0.750, test=-0.750) total time=  54.7s\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=1500;, score=(train=-0.620, test=-0.620) total time= 2.4min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=2000;, score=(train=-0.590, test=-0.590) total time= 3.1min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=2000;, score=(train=-0.580, test=-0.590) total time= 3.1min\n",
      "[CV 1/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.8min\n",
      "[CV 2/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=2500;, score=(train=-0.560, test=-0.570) total time= 3.8min\n",
      "[CV 3/3] END boosting_type=gbdt, min_child_samples=100, n_estimators=2500;, score=(train=-0.560, test=-0.560) total time= 3.8min\n",
      "The best parameters are : {'boosting_type': 'gbdt', 'min_child_samples': 20, 'n_estimators': 2500}\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV parameters\n",
    "from lightgbm import LGBMRegressor\n",
    "LGBM_reg = LGBMRegressor(subsample=0.3, colsample_bytree=0.3, subsample_freq=10, objective='rmse', n_jobd=50)\n",
    "#LGBM_reg = LGBMRegressor()\n",
    "\n",
    "hyper_parameters = {'boosting_type':['gbdt'],\n",
    "                    'n_estimators':[500,1500,2000,2500],\n",
    "                    'min_child_samples':[20, 31, 50, 100]}\n",
    "folds = KFold(n_splits=3, shuffle=True, random_state=50)\n",
    "\n",
    "LGBM_reg_cv = GridSearchCV(estimator=LGBM_reg,\n",
    "                        param_grid= hyper_parameters,\n",
    "                        scoring=score,\n",
    "                        cv=folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs =1,\n",
    "                        verbose=5)\n",
    "\n",
    "#Training the model\n",
    "LGBM_reg_cv.fit(X_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_cv_Rev02.pkl'\n",
    "joblib.dump(LGBM_reg_cv, filename)\n",
    "\n",
    "#saving the results into df\n",
    "LGBM_reg_cv_cv_results = pd.DataFrame(LGBM_reg_cv.cv_results_)\n",
    "LGBM_reg_cv_cv_results.to_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_cv_results_Rev02.csv')\n",
    "\n",
    "\n",
    "#Results\n",
    "print(\"The best parameters are : {0}\".format(LGBM_reg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnKVNkczJQ-S"
   },
   "source": [
    "**Training Model with Best Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272501,
     "status": "ok",
     "timestamp": 1639605120083,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "7f6CwfvvJQ-T",
    "outputId": "b913986c-4810-4d87-9a99-128385b422b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are : {'boosting_type': 'gbdt', 'min_child_samples': 20, 'n_estimators': 2500}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev02.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Parameters are : {}\".format(LGBM_reg_cv.best_params_))\n",
    "print(\"--\"*25)\n",
    "\n",
    "LGBM_reg_best = LGBM_reg_cv.best_estimator_\n",
    "LGBM_reg_best.fit(X_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev02.sav'\n",
    "joblib.dump(LGBM_reg_best, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154467,
     "status": "ok",
     "timestamp": 1639622593716,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "v8vAB28AJTRG",
    "outputId": "c23bc0aa-944c-4276-b410-5c174e4eaab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For LightGBM Regression Model the RMSLE values are \n",
      "    train RMSLE = 0.15 \n",
      "    CV RMSLE = 0.17\n",
      "--------------------------------------------------\n",
      "2.For LightGBM Regression Model the RMSE values are \n",
      "    train RMSE = 0.56 \n",
      "    CV RMSE = 0.67\n",
      "--------------------------------------------------\n",
      "2.For LightGBM Regression Model the RMSE(CV) values are \n",
      "    train RMSE(CV) = 0.12 \n",
      "    CV RMSE(CV) = 0.15\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "LGBM_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev02.sav')\n",
    "\n",
    "y_train_pred = LGBM_reg_best.predict(X_train[:])\n",
    "y_cv_pred = LGBM_reg_best.predict(X_cv[:])\n",
    "\n",
    "#Evaluation\n",
    "LGBM_reg_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSLE = compute_RMSLE(y_train[:], y_train_pred)\n",
    "\n",
    "LGBM_reg_cv_RMSE = compute_RMSE(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSE = compute_RMSE(y_train[:], y_train_pred)\n",
    "\n",
    "LGBM_reg_cv_RMSE_CV = compute_RMSE_CV(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSE_CV = compute_RMSE_CV(y_train[:], y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"1.For LightGBM Regression Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(LGBM_reg_train_RMSLE, LGBM_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For LightGBM Regression Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(LGBM_reg_train_RMSE, LGBM_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For LightGBM Regression Model the RMSE(CV) values are \\n    train RMSE(CV) = {0} \\n    CV RMSE(CV) = {1}\".format(LGBM_reg_train_RMSE_CV, LGBM_reg_cv_RMSE_CV))\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Pqk2ybgPeiM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6ejQnELPohc"
   },
   "source": [
    "##### **4.4.3 Model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjcUGeaJRe5H"
   },
   "outputs": [],
   "source": [
    "data_cleaned = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/preprocessed_data_train.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrpLsTyvRe5I"
   },
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned.drop(['site_id','dew_temperature', 'Ea', 'Es', 'year' ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ww80_t6Re5J"
   },
   "outputs": [],
   "source": [
    "#As the timestamps are available, we can perform Time-Based Splitting.\n",
    "data_cleaned = data_cleaned.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "#Removing Timestamp other unwanted features.\n",
    "data_cleaned = data_cleaned.drop(['meter_reading', 'meter_reading_corrected', 'timestamp', 'floor_count'], axis=1)\n",
    "\n",
    "#Setting up target variable.\n",
    "y_train = data_cleaned.meter_reading_transformed\n",
    "X_train = data_cleaned.drop(['meter_reading_transformed'], axis=1)\n",
    "\n",
    "#Splitting Data\n",
    "#X_train, X_cv, y_train, y_cv  = train_test_split(X, y, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 34696,
     "status": "error",
     "timestamp": 1640548892581,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "DhT6q-rxPohc",
    "outputId": "43696fed-988b-44b6-9b71-aea608690b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3] END boosting_type=gbdt, num_boost_round=1000;, score=(train=nan, test=nan) total time=  11.5s\n",
      "[CV 2/3] END boosting_type=gbdt, num_boost_round=1000;, score=(train=nan, test=nan) total time=  11.3s\n",
      "[CV 3/3] END boosting_type=gbdt, num_boost_round=1000;, score=(train=nan, test=nan) total time=  11.3s\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b98dff37664d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mLGBM_reg_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# save the model to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# of out will be done in `_insert_error_scores`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                     \u001b[0m_insert_error_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_insert_error_scores\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccessful_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All estimators failed to fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccessful_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: All estimators failed to fit"
     ]
    }
   ],
   "source": [
    "#GridSearchCV parameters\n",
    "from lightgbm import LGBMRegressor\n",
    "LGBM_reg = LGBMRegressor(objective ='regression', num_leaves=1280, \n",
    "                         learning_rate=0.05, feature_fraction = 0.85,\n",
    "                         reg_lambda=2, metric='rmse', num_threads=2, \n",
    "                         verbose_eval=25, early_stopping_rounds=50, n_jobd=50)\n",
    "\n",
    "hyper_parameters = {'boosting_type':['gbdt'],\n",
    "                    \"num_boost_round\":[1000]}\n",
    "\n",
    "\n",
    "folds = KFold(n_splits=3, shuffle=False, random_state=None)\n",
    "\n",
    "LGBM_reg_cv = GridSearchCV(estimator=LGBM_reg,\n",
    "                        param_grid= hyper_parameters,\n",
    "                        scoring=score,\n",
    "                        cv=folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs =1,\n",
    "                        verbose=5)\n",
    "\n",
    "#Training the model\n",
    "LGBM_reg_cv.fit(X_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_cv_Rev03.pkl'\n",
    "joblib.dump(LGBM_reg_cv, filename)\n",
    "\n",
    "#saving the results into df\n",
    "LGBM_reg_cv_cv_results = pd.DataFrame(LGBM_reg_cv.cv_results_)\n",
    "LGBM_reg_cv_cv_results.to_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_cv_results_Rev03.csv')\n",
    "\n",
    "\n",
    "#Results\n",
    "print(\"The best parameters are : {0}\".format(LGBM_reg_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "error",
     "timestamp": 1640549243062,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "Zj6IvxuZVddd",
    "outputId": "592a5267-1b36-4d66-b130-2e7bd27e4b77"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4491ebadf17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     'min_child_samples':[20]}\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m LGBM_reg_cv = GridSearchCV(estimator=LGBM_reg,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'KFold' is not defined"
     ]
    }
   ],
   "source": [
    "#GridSearchCV parameters\n",
    "from lightgbm import LGBMRegressor\n",
    "LGBM_reg = LGBMRegressor(subsample=0.8, colsample_bytree=0.8, subsample_freq=10, objective='rmse', n_jobd=50)\n",
    "#LGBM_reg = LGBMRegressor()\n",
    "\n",
    "\n",
    "hyper_parameters = {'boosting_type':['gbdt'],\n",
    "                    'n_estimators':[2500],\n",
    "                    'min_child_samples':[20]}\n",
    "folds = KFold(n_splits=3, shuffle=True, random_state=50)\n",
    "\n",
    "LGBM_reg_cv = GridSearchCV(estimator=LGBM_reg,\n",
    "                        param_grid= hyper_parameters,\n",
    "                        scoring=score,\n",
    "                        cv=folds,\n",
    "                        return_train_score=True,\n",
    "                        n_jobs =1,\n",
    "                        verbose=5)\n",
    "\n",
    "#Training the model\n",
    "LGBM_reg_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4EQW10nPohd"
   },
   "source": [
    "**Training Model with Best Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272501,
     "status": "ok",
     "timestamp": 1639605120083,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "6yqf37JXPohd",
    "outputId": "b913986c-4810-4d87-9a99-128385b422b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are : {'boosting_type': 'gbdt', 'min_child_samples': 20, 'n_estimators': 2500}\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev02.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Best Parameters are : {}\".format(LGBM_reg_cv.best_params_))\n",
    "print(\"--\"*25)\n",
    "\n",
    "LGBM_reg_best = LGBM_reg_cv.best_estimator_\n",
    "LGBM_reg_best.fit(X_train, y_train)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev03.sav'\n",
    "joblib.dump(LGBM_reg_best, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154467,
     "status": "ok",
     "timestamp": 1639622593716,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "hOmFoGfqPohe",
    "outputId": "c23bc0aa-944c-4276-b410-5c174e4eaab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For LightGBM Regression Model the RMSLE values are \n",
      "    train RMSLE = 0.15 \n",
      "    CV RMSLE = 0.17\n",
      "--------------------------------------------------\n",
      "2.For LightGBM Regression Model the RMSE values are \n",
      "    train RMSE = 0.56 \n",
      "    CV RMSE = 0.67\n",
      "--------------------------------------------------\n",
      "2.For LightGBM Regression Model the RMSE(CV) values are \n",
      "    train RMSE(CV) = 0.12 \n",
      "    CV RMSE(CV) = 0.15\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "LGBM_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev03.sav')\n",
    "\n",
    "y_train_pred = LGBM_reg_best.predict(X_train[:])\n",
    "#y_cv_pred = LGBM_reg_best.predict(X_cv[:])\n",
    "\n",
    "#Evaluation\n",
    "#LGBM_reg_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSLE = compute_RMSLE(y_train[:], y_train_pred)\n",
    "\n",
    "#LGBM_reg_cv_RMSE = compute_RMSE(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSE = compute_RMSE(y_train[:], y_train_pred)\n",
    "\n",
    "LGBM_reg_cv_RMSE_CV = compute_RMSE_CV(y_cv[:], y_cv_pred)\n",
    "LGBM_reg_train_RMSE_CV = compute_RMSE_CV(y_train[:], y_train_pred)\n",
    "\n",
    "#Report\n",
    "#print(\"1.For LightGBM Regression Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(LGBM_reg_train_RMSLE, LGBM_reg_cv_RMSLE))\n",
    "#print(\"--\"*25)\n",
    "#print(\"2.For LightGBM Regression Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(LGBM_reg_train_RMSE, LGBM_reg_cv_RMSE))\n",
    "#print(\"--\"*25)\n",
    "#print(\"2.For LightGBM Regression Model the RMSE(CV) values are \\n    train RMSE(CV) = {0} \\n    CV RMSE(CV) = {1}\".format(LGBM_reg_train_RMSE_CV, LGBM_reg_cv_RMSE_CV))\n",
    "#print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4MymI1XHeaO"
   },
   "source": [
    "### **4.5 Catboost Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqzjopfTexhH"
   },
   "source": [
    "##### **4.5.1 Model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpJlXAtQfoUl"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46646,
     "status": "ok",
     "timestamp": 1639647532372,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "qxvRcxJpeuOo",
    "outputId": "1d34e629-3767-430b-9fc0-5910dbf477dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1300:\tlearn: 0.6656011\ttotal: 1m 15s\tremaining: 46.6s\n",
      "1301:\tlearn: 0.6654508\ttotal: 1m 15s\tremaining: 46.5s\n",
      "1302:\tlearn: 0.6652881\ttotal: 1m 15s\tremaining: 46.5s\n",
      "1303:\tlearn: 0.6651617\ttotal: 1m 16s\tremaining: 46.4s\n",
      "1304:\tlearn: 0.6651042\ttotal: 1m 16s\tremaining: 46.3s\n",
      "1305:\tlearn: 0.6650391\ttotal: 1m 16s\tremaining: 46.3s\n",
      "1306:\tlearn: 0.6649462\ttotal: 1m 16s\tremaining: 46.2s\n",
      "1307:\tlearn: 0.6648864\ttotal: 1m 16s\tremaining: 46.2s\n",
      "1308:\tlearn: 0.6647956\ttotal: 1m 16s\tremaining: 46.1s\n",
      "1309:\tlearn: 0.6647323\ttotal: 1m 16s\tremaining: 46.1s\n",
      "1310:\tlearn: 0.6646877\ttotal: 1m 16s\tremaining: 46s\n",
      "1311:\tlearn: 0.6645752\ttotal: 1m 16s\tremaining: 45.9s\n",
      "1312:\tlearn: 0.6644565\ttotal: 1m 16s\tremaining: 45.9s\n",
      "1313:\tlearn: 0.6642685\ttotal: 1m 16s\tremaining: 45.8s\n",
      "1314:\tlearn: 0.6641749\ttotal: 1m 16s\tremaining: 45.8s\n",
      "1315:\tlearn: 0.6640846\ttotal: 1m 16s\tremaining: 45.7s\n",
      "1316:\tlearn: 0.6639494\ttotal: 1m 16s\tremaining: 45.6s\n",
      "1317:\tlearn: 0.6638240\ttotal: 1m 16s\tremaining: 45.6s\n",
      "1318:\tlearn: 0.6637462\ttotal: 1m 16s\tremaining: 45.5s\n",
      "1319:\tlearn: 0.6636516\ttotal: 1m 16s\tremaining: 45.5s\n",
      "1320:\tlearn: 0.6635290\ttotal: 1m 17s\tremaining: 45.4s\n",
      "1321:\tlearn: 0.6633762\ttotal: 1m 17s\tremaining: 45.4s\n",
      "1322:\tlearn: 0.6633014\ttotal: 1m 17s\tremaining: 45.3s\n",
      "1323:\tlearn: 0.6632135\ttotal: 1m 17s\tremaining: 45.2s\n",
      "1324:\tlearn: 0.6631129\ttotal: 1m 17s\tremaining: 45.2s\n",
      "1325:\tlearn: 0.6630263\ttotal: 1m 17s\tremaining: 45.1s\n",
      "1326:\tlearn: 0.6629748\ttotal: 1m 17s\tremaining: 45.1s\n",
      "1327:\tlearn: 0.6628944\ttotal: 1m 17s\tremaining: 45s\n",
      "1328:\tlearn: 0.6628529\ttotal: 1m 17s\tremaining: 44.9s\n",
      "1329:\tlearn: 0.6627783\ttotal: 1m 17s\tremaining: 44.9s\n",
      "1330:\tlearn: 0.6627200\ttotal: 1m 17s\tremaining: 44.8s\n",
      "1331:\tlearn: 0.6626275\ttotal: 1m 17s\tremaining: 44.8s\n",
      "1332:\tlearn: 0.6625396\ttotal: 1m 17s\tremaining: 44.7s\n",
      "1333:\tlearn: 0.6624493\ttotal: 1m 17s\tremaining: 44.7s\n",
      "1334:\tlearn: 0.6623313\ttotal: 1m 17s\tremaining: 44.6s\n",
      "1335:\tlearn: 0.6622945\ttotal: 1m 17s\tremaining: 44.5s\n",
      "1336:\tlearn: 0.6622604\ttotal: 1m 17s\tremaining: 44.5s\n",
      "1337:\tlearn: 0.6621844\ttotal: 1m 17s\tremaining: 44.4s\n",
      "1338:\tlearn: 0.6621495\ttotal: 1m 18s\tremaining: 44.4s\n",
      "1339:\tlearn: 0.6620797\ttotal: 1m 18s\tremaining: 44.3s\n",
      "1340:\tlearn: 0.6620298\ttotal: 1m 18s\tremaining: 44.2s\n",
      "1341:\tlearn: 0.6618650\ttotal: 1m 18s\tremaining: 44.2s\n",
      "1342:\tlearn: 0.6618166\ttotal: 1m 18s\tremaining: 44.1s\n",
      "1343:\tlearn: 0.6616992\ttotal: 1m 18s\tremaining: 44.1s\n",
      "1344:\tlearn: 0.6616297\ttotal: 1m 18s\tremaining: 44s\n",
      "1345:\tlearn: 0.6614007\ttotal: 1m 18s\tremaining: 44s\n",
      "1346:\tlearn: 0.6613185\ttotal: 1m 18s\tremaining: 43.9s\n",
      "1347:\tlearn: 0.6612828\ttotal: 1m 18s\tremaining: 43.8s\n",
      "1348:\tlearn: 0.6611806\ttotal: 1m 18s\tremaining: 43.8s\n",
      "1349:\tlearn: 0.6611292\ttotal: 1m 18s\tremaining: 43.7s\n",
      "1350:\tlearn: 0.6610244\ttotal: 1m 18s\tremaining: 43.7s\n",
      "1351:\tlearn: 0.6609152\ttotal: 1m 18s\tremaining: 43.6s\n",
      "1352:\tlearn: 0.6606687\ttotal: 1m 18s\tremaining: 43.5s\n",
      "1353:\tlearn: 0.6606047\ttotal: 1m 18s\tremaining: 43.5s\n",
      "1354:\tlearn: 0.6605068\ttotal: 1m 18s\tremaining: 43.4s\n",
      "1355:\tlearn: 0.6604465\ttotal: 1m 19s\tremaining: 43.4s\n",
      "1356:\tlearn: 0.6603652\ttotal: 1m 19s\tremaining: 43.3s\n",
      "1357:\tlearn: 0.6602059\ttotal: 1m 19s\tremaining: 43.3s\n",
      "1358:\tlearn: 0.6600864\ttotal: 1m 19s\tremaining: 43.2s\n",
      "1359:\tlearn: 0.6599820\ttotal: 1m 19s\tremaining: 43.1s\n",
      "1360:\tlearn: 0.6598980\ttotal: 1m 19s\tremaining: 43.1s\n",
      "1361:\tlearn: 0.6598359\ttotal: 1m 19s\tremaining: 43s\n",
      "1362:\tlearn: 0.6597922\ttotal: 1m 19s\tremaining: 43s\n",
      "1363:\tlearn: 0.6596071\ttotal: 1m 19s\tremaining: 42.9s\n",
      "1364:\tlearn: 0.6595303\ttotal: 1m 19s\tremaining: 42.9s\n",
      "1365:\tlearn: 0.6594447\ttotal: 1m 19s\tremaining: 42.8s\n",
      "1366:\tlearn: 0.6593183\ttotal: 1m 19s\tremaining: 42.7s\n",
      "1367:\tlearn: 0.6592169\ttotal: 1m 19s\tremaining: 42.7s\n",
      "1368:\tlearn: 0.6591354\ttotal: 1m 19s\tremaining: 42.6s\n",
      "1369:\tlearn: 0.6590458\ttotal: 1m 19s\tremaining: 42.6s\n",
      "1370:\tlearn: 0.6589398\ttotal: 1m 19s\tremaining: 42.5s\n",
      "1371:\tlearn: 0.6588653\ttotal: 1m 19s\tremaining: 42.4s\n",
      "1372:\tlearn: 0.6587903\ttotal: 1m 20s\tremaining: 42.4s\n",
      "1373:\tlearn: 0.6587148\ttotal: 1m 20s\tremaining: 42.3s\n",
      "1374:\tlearn: 0.6585127\ttotal: 1m 20s\tremaining: 42.3s\n",
      "1375:\tlearn: 0.6584317\ttotal: 1m 20s\tremaining: 42.2s\n",
      "1376:\tlearn: 0.6583698\ttotal: 1m 20s\tremaining: 42.2s\n",
      "1377:\tlearn: 0.6582372\ttotal: 1m 20s\tremaining: 42.1s\n",
      "1378:\tlearn: 0.6581354\ttotal: 1m 20s\tremaining: 42s\n",
      "1379:\tlearn: 0.6579949\ttotal: 1m 20s\tremaining: 42s\n",
      "1380:\tlearn: 0.6579212\ttotal: 1m 20s\tremaining: 41.9s\n",
      "1381:\tlearn: 0.6578450\ttotal: 1m 20s\tremaining: 41.9s\n",
      "1382:\tlearn: 0.6577853\ttotal: 1m 20s\tremaining: 41.8s\n",
      "1383:\tlearn: 0.6576448\ttotal: 1m 20s\tremaining: 41.7s\n",
      "1384:\tlearn: 0.6575568\ttotal: 1m 20s\tremaining: 41.7s\n",
      "1385:\tlearn: 0.6574057\ttotal: 1m 20s\tremaining: 41.6s\n",
      "1386:\tlearn: 0.6573243\ttotal: 1m 20s\tremaining: 41.6s\n",
      "1387:\tlearn: 0.6572915\ttotal: 1m 20s\tremaining: 41.5s\n",
      "1388:\tlearn: 0.6572247\ttotal: 1m 20s\tremaining: 41.5s\n",
      "1389:\tlearn: 0.6571423\ttotal: 1m 21s\tremaining: 41.4s\n",
      "1390:\tlearn: 0.6570993\ttotal: 1m 21s\tremaining: 41.3s\n",
      "1391:\tlearn: 0.6569937\ttotal: 1m 21s\tremaining: 41.3s\n",
      "1392:\tlearn: 0.6569389\ttotal: 1m 21s\tremaining: 41.2s\n",
      "1393:\tlearn: 0.6568613\ttotal: 1m 21s\tremaining: 41.2s\n",
      "1394:\tlearn: 0.6565977\ttotal: 1m 21s\tremaining: 41.1s\n",
      "1395:\tlearn: 0.6565464\ttotal: 1m 21s\tremaining: 41s\n",
      "1396:\tlearn: 0.6564333\ttotal: 1m 21s\tremaining: 41s\n",
      "1397:\tlearn: 0.6563173\ttotal: 1m 21s\tremaining: 40.9s\n",
      "1398:\tlearn: 0.6562336\ttotal: 1m 21s\tremaining: 40.9s\n",
      "1399:\tlearn: 0.6561401\ttotal: 1m 21s\tremaining: 40.8s\n",
      "1400:\tlearn: 0.6560326\ttotal: 1m 21s\tremaining: 40.8s\n",
      "1401:\tlearn: 0.6559489\ttotal: 1m 21s\tremaining: 40.7s\n",
      "1402:\tlearn: 0.6558719\ttotal: 1m 21s\tremaining: 40.6s\n",
      "1403:\tlearn: 0.6558442\ttotal: 1m 21s\tremaining: 40.6s\n",
      "1404:\tlearn: 0.6557849\ttotal: 1m 21s\tremaining: 40.5s\n",
      "1405:\tlearn: 0.6557328\ttotal: 1m 21s\tremaining: 40.5s\n",
      "1406:\tlearn: 0.6556777\ttotal: 1m 22s\tremaining: 40.4s\n",
      "1407:\tlearn: 0.6555795\ttotal: 1m 22s\tremaining: 40.4s\n",
      "1408:\tlearn: 0.6555469\ttotal: 1m 22s\tremaining: 40.3s\n",
      "1409:\tlearn: 0.6554913\ttotal: 1m 22s\tremaining: 40.2s\n",
      "1410:\tlearn: 0.6553877\ttotal: 1m 22s\tremaining: 40.2s\n",
      "1411:\tlearn: 0.6552729\ttotal: 1m 22s\tremaining: 40.1s\n",
      "1412:\tlearn: 0.6551108\ttotal: 1m 22s\tremaining: 40.1s\n",
      "1413:\tlearn: 0.6549728\ttotal: 1m 22s\tremaining: 40s\n",
      "1414:\tlearn: 0.6548516\ttotal: 1m 22s\tremaining: 39.9s\n",
      "1415:\tlearn: 0.6547657\ttotal: 1m 22s\tremaining: 39.9s\n",
      "1416:\tlearn: 0.6547292\ttotal: 1m 22s\tremaining: 39.8s\n",
      "1417:\tlearn: 0.6545975\ttotal: 1m 22s\tremaining: 39.8s\n",
      "1418:\tlearn: 0.6545394\ttotal: 1m 22s\tremaining: 39.7s\n",
      "1419:\tlearn: 0.6544277\ttotal: 1m 22s\tremaining: 39.7s\n",
      "1420:\tlearn: 0.6543600\ttotal: 1m 22s\tremaining: 39.6s\n",
      "1421:\tlearn: 0.6542350\ttotal: 1m 22s\tremaining: 39.5s\n",
      "1422:\tlearn: 0.6541086\ttotal: 1m 22s\tremaining: 39.5s\n",
      "1423:\tlearn: 0.6540013\ttotal: 1m 23s\tremaining: 39.4s\n",
      "1424:\tlearn: 0.6539411\ttotal: 1m 23s\tremaining: 39.4s\n",
      "1425:\tlearn: 0.6538370\ttotal: 1m 23s\tremaining: 39.3s\n",
      "1426:\tlearn: 0.6537503\ttotal: 1m 23s\tremaining: 39.2s\n",
      "1427:\tlearn: 0.6537099\ttotal: 1m 23s\tremaining: 39.2s\n",
      "1428:\tlearn: 0.6535825\ttotal: 1m 23s\tremaining: 39.1s\n",
      "1429:\tlearn: 0.6534784\ttotal: 1m 23s\tremaining: 39.1s\n",
      "1430:\tlearn: 0.6533301\ttotal: 1m 23s\tremaining: 39s\n",
      "1431:\tlearn: 0.6532026\ttotal: 1m 23s\tremaining: 39s\n",
      "1432:\tlearn: 0.6531339\ttotal: 1m 23s\tremaining: 38.9s\n",
      "1433:\tlearn: 0.6530282\ttotal: 1m 23s\tremaining: 38.8s\n",
      "1434:\tlearn: 0.6529544\ttotal: 1m 23s\tremaining: 38.8s\n",
      "1435:\tlearn: 0.6529292\ttotal: 1m 23s\tremaining: 38.7s\n",
      "1436:\tlearn: 0.6528704\ttotal: 1m 23s\tremaining: 38.7s\n",
      "1437:\tlearn: 0.6527342\ttotal: 1m 23s\tremaining: 38.6s\n",
      "1438:\tlearn: 0.6526671\ttotal: 1m 23s\tremaining: 38.5s\n",
      "1439:\tlearn: 0.6526232\ttotal: 1m 23s\tremaining: 38.5s\n",
      "1440:\tlearn: 0.6525451\ttotal: 1m 24s\tremaining: 38.4s\n",
      "1441:\tlearn: 0.6524179\ttotal: 1m 24s\tremaining: 38.4s\n",
      "1442:\tlearn: 0.6523805\ttotal: 1m 24s\tremaining: 38.3s\n",
      "1443:\tlearn: 0.6523166\ttotal: 1m 24s\tremaining: 38.3s\n",
      "1444:\tlearn: 0.6521999\ttotal: 1m 24s\tremaining: 38.2s\n",
      "1445:\tlearn: 0.6521425\ttotal: 1m 24s\tremaining: 38.1s\n",
      "1446:\tlearn: 0.6521108\ttotal: 1m 24s\tremaining: 38.1s\n",
      "1447:\tlearn: 0.6520615\ttotal: 1m 24s\tremaining: 38s\n",
      "1448:\tlearn: 0.6519278\ttotal: 1m 24s\tremaining: 38s\n",
      "1449:\tlearn: 0.6518021\ttotal: 1m 24s\tremaining: 37.9s\n",
      "1450:\tlearn: 0.6517554\ttotal: 1m 24s\tremaining: 37.8s\n",
      "1451:\tlearn: 0.6516794\ttotal: 1m 24s\tremaining: 37.8s\n",
      "1452:\tlearn: 0.6516227\ttotal: 1m 24s\tremaining: 37.7s\n",
      "1453:\tlearn: 0.6513628\ttotal: 1m 24s\tremaining: 37.7s\n",
      "1454:\tlearn: 0.6513257\ttotal: 1m 24s\tremaining: 37.6s\n",
      "1455:\tlearn: 0.6511780\ttotal: 1m 24s\tremaining: 37.5s\n",
      "1456:\tlearn: 0.6510668\ttotal: 1m 24s\tremaining: 37.5s\n",
      "1457:\tlearn: 0.6510131\ttotal: 1m 25s\tremaining: 37.4s\n",
      "1458:\tlearn: 0.6509453\ttotal: 1m 25s\tremaining: 37.4s\n",
      "1459:\tlearn: 0.6508533\ttotal: 1m 25s\tremaining: 37.3s\n",
      "1460:\tlearn: 0.6508167\ttotal: 1m 25s\tremaining: 37.3s\n",
      "1461:\tlearn: 0.6507241\ttotal: 1m 25s\tremaining: 37.2s\n",
      "1462:\tlearn: 0.6506084\ttotal: 1m 25s\tremaining: 37.1s\n",
      "1463:\tlearn: 0.6505485\ttotal: 1m 25s\tremaining: 37.1s\n",
      "1464:\tlearn: 0.6504321\ttotal: 1m 25s\tremaining: 37s\n",
      "1465:\tlearn: 0.6503148\ttotal: 1m 25s\tremaining: 37s\n",
      "1466:\tlearn: 0.6502775\ttotal: 1m 25s\tremaining: 36.9s\n",
      "1467:\tlearn: 0.6502201\ttotal: 1m 25s\tremaining: 36.9s\n",
      "1468:\tlearn: 0.6501590\ttotal: 1m 25s\tremaining: 36.8s\n",
      "1469:\tlearn: 0.6500976\ttotal: 1m 25s\tremaining: 36.7s\n",
      "1470:\tlearn: 0.6500374\ttotal: 1m 25s\tremaining: 36.7s\n",
      "1471:\tlearn: 0.6499366\ttotal: 1m 25s\tremaining: 36.6s\n",
      "1472:\tlearn: 0.6498608\ttotal: 1m 25s\tremaining: 36.6s\n",
      "1473:\tlearn: 0.6497981\ttotal: 1m 25s\tremaining: 36.5s\n",
      "1474:\tlearn: 0.6497129\ttotal: 1m 26s\tremaining: 36.4s\n",
      "1475:\tlearn: 0.6496256\ttotal: 1m 26s\tremaining: 36.4s\n",
      "1476:\tlearn: 0.6495319\ttotal: 1m 26s\tremaining: 36.3s\n",
      "1477:\tlearn: 0.6494606\ttotal: 1m 26s\tremaining: 36.3s\n",
      "1478:\tlearn: 0.6494038\ttotal: 1m 26s\tremaining: 36.2s\n",
      "1479:\tlearn: 0.6493264\ttotal: 1m 26s\tremaining: 36.2s\n",
      "1480:\tlearn: 0.6492529\ttotal: 1m 26s\tremaining: 36.1s\n",
      "1481:\tlearn: 0.6492100\ttotal: 1m 26s\tremaining: 36s\n",
      "1482:\tlearn: 0.6490617\ttotal: 1m 26s\tremaining: 36s\n",
      "1483:\tlearn: 0.6489575\ttotal: 1m 26s\tremaining: 35.9s\n",
      "1484:\tlearn: 0.6488918\ttotal: 1m 26s\tremaining: 35.9s\n",
      "1485:\tlearn: 0.6487572\ttotal: 1m 26s\tremaining: 35.8s\n",
      "1486:\tlearn: 0.6486613\ttotal: 1m 26s\tremaining: 35.7s\n",
      "1487:\tlearn: 0.6485697\ttotal: 1m 26s\tremaining: 35.7s\n",
      "1488:\tlearn: 0.6485036\ttotal: 1m 26s\tremaining: 35.6s\n",
      "1489:\tlearn: 0.6484307\ttotal: 1m 26s\tremaining: 35.6s\n",
      "1490:\tlearn: 0.6483528\ttotal: 1m 26s\tremaining: 35.5s\n",
      "1491:\tlearn: 0.6482336\ttotal: 1m 27s\tremaining: 35.5s\n",
      "1492:\tlearn: 0.6480288\ttotal: 1m 27s\tremaining: 35.4s\n",
      "1493:\tlearn: 0.6479662\ttotal: 1m 27s\tremaining: 35.3s\n",
      "1494:\tlearn: 0.6478702\ttotal: 1m 27s\tremaining: 35.3s\n",
      "1495:\tlearn: 0.6478145\ttotal: 1m 27s\tremaining: 35.2s\n",
      "1496:\tlearn: 0.6477654\ttotal: 1m 27s\tremaining: 35.2s\n",
      "1497:\tlearn: 0.6476992\ttotal: 1m 27s\tremaining: 35.1s\n",
      "1498:\tlearn: 0.6476404\ttotal: 1m 27s\tremaining: 35.1s\n",
      "1499:\tlearn: 0.6475944\ttotal: 1m 27s\tremaining: 35s\n",
      "1500:\tlearn: 0.6475344\ttotal: 1m 27s\tremaining: 34.9s\n",
      "1501:\tlearn: 0.6474577\ttotal: 1m 27s\tremaining: 34.9s\n",
      "1502:\tlearn: 0.6473807\ttotal: 1m 27s\tremaining: 34.8s\n",
      "1503:\tlearn: 0.6473158\ttotal: 1m 27s\tremaining: 34.8s\n",
      "1504:\tlearn: 0.6472169\ttotal: 1m 27s\tremaining: 34.7s\n",
      "1505:\tlearn: 0.6471855\ttotal: 1m 27s\tremaining: 34.6s\n",
      "1506:\tlearn: 0.6470819\ttotal: 1m 27s\tremaining: 34.6s\n",
      "1507:\tlearn: 0.6470173\ttotal: 1m 27s\tremaining: 34.5s\n",
      "1508:\tlearn: 0.6469686\ttotal: 1m 28s\tremaining: 34.5s\n",
      "1509:\tlearn: 0.6468800\ttotal: 1m 28s\tremaining: 34.4s\n",
      "1510:\tlearn: 0.6467640\ttotal: 1m 28s\tremaining: 34.4s\n",
      "1511:\tlearn: 0.6467200\ttotal: 1m 28s\tremaining: 34.3s\n",
      "1512:\tlearn: 0.6466155\ttotal: 1m 28s\tremaining: 34.2s\n",
      "1513:\tlearn: 0.6465705\ttotal: 1m 28s\tremaining: 34.2s\n",
      "1514:\tlearn: 0.6465194\ttotal: 1m 28s\tremaining: 34.1s\n",
      "1515:\tlearn: 0.6464670\ttotal: 1m 28s\tremaining: 34.1s\n",
      "1516:\tlearn: 0.6463611\ttotal: 1m 28s\tremaining: 34s\n",
      "1517:\tlearn: 0.6462758\ttotal: 1m 28s\tremaining: 33.9s\n",
      "1518:\tlearn: 0.6461815\ttotal: 1m 28s\tremaining: 33.9s\n",
      "1519:\tlearn: 0.6461196\ttotal: 1m 28s\tremaining: 33.8s\n",
      "1520:\tlearn: 0.6460495\ttotal: 1m 28s\tremaining: 33.8s\n",
      "1521:\tlearn: 0.6459388\ttotal: 1m 28s\tremaining: 33.7s\n",
      "1522:\tlearn: 0.6458801\ttotal: 1m 28s\tremaining: 33.7s\n",
      "1523:\tlearn: 0.6457958\ttotal: 1m 28s\tremaining: 33.6s\n",
      "1524:\tlearn: 0.6456810\ttotal: 1m 28s\tremaining: 33.5s\n",
      "1525:\tlearn: 0.6456189\ttotal: 1m 29s\tremaining: 33.5s\n",
      "1526:\tlearn: 0.6455470\ttotal: 1m 29s\tremaining: 33.4s\n",
      "1527:\tlearn: 0.6454617\ttotal: 1m 29s\tremaining: 33.4s\n",
      "1528:\tlearn: 0.6453854\ttotal: 1m 29s\tremaining: 33.3s\n",
      "1529:\tlearn: 0.6452653\ttotal: 1m 29s\tremaining: 33.2s\n",
      "1530:\tlearn: 0.6450948\ttotal: 1m 29s\tremaining: 33.2s\n",
      "1531:\tlearn: 0.6449875\ttotal: 1m 29s\tremaining: 33.1s\n",
      "1532:\tlearn: 0.6448960\ttotal: 1m 29s\tremaining: 33.1s\n",
      "1533:\tlearn: 0.6447733\ttotal: 1m 29s\tremaining: 33s\n",
      "1534:\tlearn: 0.6447131\ttotal: 1m 29s\tremaining: 33s\n",
      "1535:\tlearn: 0.6446326\ttotal: 1m 29s\tremaining: 32.9s\n",
      "1536:\tlearn: 0.6445928\ttotal: 1m 29s\tremaining: 32.8s\n",
      "1537:\tlearn: 0.6444641\ttotal: 1m 29s\tremaining: 32.8s\n",
      "1538:\tlearn: 0.6443874\ttotal: 1m 29s\tremaining: 32.7s\n",
      "1539:\tlearn: 0.6443135\ttotal: 1m 29s\tremaining: 32.7s\n",
      "1540:\tlearn: 0.6442388\ttotal: 1m 29s\tremaining: 32.6s\n",
      "1541:\tlearn: 0.6441498\ttotal: 1m 29s\tremaining: 32.5s\n",
      "1542:\tlearn: 0.6441102\ttotal: 1m 29s\tremaining: 32.5s\n",
      "1543:\tlearn: 0.6440234\ttotal: 1m 30s\tremaining: 32.4s\n",
      "1544:\tlearn: 0.6438832\ttotal: 1m 30s\tremaining: 32.4s\n",
      "1545:\tlearn: 0.6438065\ttotal: 1m 30s\tremaining: 32.3s\n",
      "1546:\tlearn: 0.6437458\ttotal: 1m 30s\tremaining: 32.3s\n",
      "1547:\tlearn: 0.6437138\ttotal: 1m 30s\tremaining: 32.2s\n",
      "1548:\tlearn: 0.6436107\ttotal: 1m 30s\tremaining: 32.1s\n",
      "1549:\tlearn: 0.6435386\ttotal: 1m 30s\tremaining: 32.1s\n",
      "1550:\tlearn: 0.6434494\ttotal: 1m 30s\tremaining: 32s\n",
      "1551:\tlearn: 0.6433901\ttotal: 1m 30s\tremaining: 32s\n",
      "1552:\tlearn: 0.6433241\ttotal: 1m 30s\tremaining: 31.9s\n",
      "1553:\tlearn: 0.6432529\ttotal: 1m 30s\tremaining: 31.8s\n",
      "1554:\tlearn: 0.6431339\ttotal: 1m 30s\tremaining: 31.8s\n",
      "1555:\tlearn: 0.6430724\ttotal: 1m 30s\tremaining: 31.7s\n",
      "1556:\tlearn: 0.6429251\ttotal: 1m 30s\tremaining: 31.7s\n",
      "1557:\tlearn: 0.6428496\ttotal: 1m 30s\tremaining: 31.6s\n",
      "1558:\tlearn: 0.6427452\ttotal: 1m 30s\tremaining: 31.6s\n",
      "1559:\tlearn: 0.6426299\ttotal: 1m 30s\tremaining: 31.5s\n",
      "1560:\tlearn: 0.6424750\ttotal: 1m 31s\tremaining: 31.4s\n",
      "1561:\tlearn: 0.6423439\ttotal: 1m 31s\tremaining: 31.4s\n",
      "1562:\tlearn: 0.6422778\ttotal: 1m 31s\tremaining: 31.3s\n",
      "1563:\tlearn: 0.6421667\ttotal: 1m 31s\tremaining: 31.3s\n",
      "1564:\tlearn: 0.6421207\ttotal: 1m 31s\tremaining: 31.2s\n",
      "1565:\tlearn: 0.6420349\ttotal: 1m 31s\tremaining: 31.1s\n",
      "1566:\tlearn: 0.6420089\ttotal: 1m 31s\tremaining: 31.1s\n",
      "1567:\tlearn: 0.6419241\ttotal: 1m 31s\tremaining: 31s\n",
      "1568:\tlearn: 0.6419026\ttotal: 1m 31s\tremaining: 31s\n",
      "1569:\tlearn: 0.6418583\ttotal: 1m 31s\tremaining: 30.9s\n",
      "1570:\tlearn: 0.6417353\ttotal: 1m 31s\tremaining: 30.9s\n",
      "1571:\tlearn: 0.6416859\ttotal: 1m 31s\tremaining: 30.8s\n",
      "1572:\tlearn: 0.6415500\ttotal: 1m 31s\tremaining: 30.7s\n",
      "1573:\tlearn: 0.6414942\ttotal: 1m 31s\tremaining: 30.7s\n",
      "1574:\tlearn: 0.6413752\ttotal: 1m 31s\tremaining: 30.6s\n",
      "1575:\tlearn: 0.6413367\ttotal: 1m 31s\tremaining: 30.6s\n",
      "1576:\tlearn: 0.6411818\ttotal: 1m 31s\tremaining: 30.5s\n",
      "1577:\tlearn: 0.6410975\ttotal: 1m 32s\tremaining: 30.4s\n",
      "1578:\tlearn: 0.6410481\ttotal: 1m 32s\tremaining: 30.4s\n",
      "1579:\tlearn: 0.6410122\ttotal: 1m 32s\tremaining: 30.3s\n",
      "1580:\tlearn: 0.6409798\ttotal: 1m 32s\tremaining: 30.3s\n",
      "1581:\tlearn: 0.6409231\ttotal: 1m 32s\tremaining: 30.2s\n",
      "1582:\tlearn: 0.6408043\ttotal: 1m 32s\tremaining: 30.2s\n",
      "1583:\tlearn: 0.6407511\ttotal: 1m 32s\tremaining: 30.1s\n",
      "1584:\tlearn: 0.6406927\ttotal: 1m 32s\tremaining: 30s\n",
      "1585:\tlearn: 0.6406130\ttotal: 1m 32s\tremaining: 30s\n",
      "1586:\tlearn: 0.6404558\ttotal: 1m 32s\tremaining: 29.9s\n",
      "1587:\tlearn: 0.6403104\ttotal: 1m 32s\tremaining: 29.9s\n",
      "1588:\tlearn: 0.6402751\ttotal: 1m 32s\tremaining: 29.8s\n",
      "1589:\tlearn: 0.6401036\ttotal: 1m 32s\tremaining: 29.7s\n",
      "1590:\tlearn: 0.6399720\ttotal: 1m 32s\tremaining: 29.7s\n",
      "1591:\tlearn: 0.6399165\ttotal: 1m 32s\tremaining: 29.6s\n",
      "1592:\tlearn: 0.6397504\ttotal: 1m 32s\tremaining: 29.6s\n",
      "1593:\tlearn: 0.6397049\ttotal: 1m 32s\tremaining: 29.5s\n",
      "1594:\tlearn: 0.6396535\ttotal: 1m 33s\tremaining: 29.5s\n",
      "1595:\tlearn: 0.6396132\ttotal: 1m 33s\tremaining: 29.4s\n",
      "1596:\tlearn: 0.6395495\ttotal: 1m 33s\tremaining: 29.3s\n",
      "1597:\tlearn: 0.6394297\ttotal: 1m 33s\tremaining: 29.3s\n",
      "1598:\tlearn: 0.6393093\ttotal: 1m 33s\tremaining: 29.2s\n",
      "1599:\tlearn: 0.6392177\ttotal: 1m 33s\tremaining: 29.2s\n",
      "1600:\tlearn: 0.6391893\ttotal: 1m 33s\tremaining: 29.1s\n",
      "1601:\tlearn: 0.6390935\ttotal: 1m 33s\tremaining: 29s\n",
      "1602:\tlearn: 0.6390408\ttotal: 1m 33s\tremaining: 29s\n",
      "1603:\tlearn: 0.6389701\ttotal: 1m 33s\tremaining: 28.9s\n",
      "1604:\tlearn: 0.6389230\ttotal: 1m 33s\tremaining: 28.9s\n",
      "1605:\tlearn: 0.6388853\ttotal: 1m 33s\tremaining: 28.8s\n",
      "1606:\tlearn: 0.6388299\ttotal: 1m 33s\tremaining: 28.8s\n",
      "1607:\tlearn: 0.6387401\ttotal: 1m 33s\tremaining: 28.7s\n",
      "1608:\tlearn: 0.6387020\ttotal: 1m 33s\tremaining: 28.6s\n",
      "1609:\tlearn: 0.6386783\ttotal: 1m 33s\tremaining: 28.6s\n",
      "1610:\tlearn: 0.6385770\ttotal: 1m 33s\tremaining: 28.5s\n",
      "1611:\tlearn: 0.6385074\ttotal: 1m 34s\tremaining: 28.5s\n",
      "1612:\tlearn: 0.6382320\ttotal: 1m 34s\tremaining: 28.4s\n",
      "1613:\tlearn: 0.6381848\ttotal: 1m 34s\tremaining: 28.3s\n",
      "1614:\tlearn: 0.6381356\ttotal: 1m 34s\tremaining: 28.3s\n",
      "1615:\tlearn: 0.6380659\ttotal: 1m 34s\tremaining: 28.2s\n",
      "1616:\tlearn: 0.6380307\ttotal: 1m 34s\tremaining: 28.2s\n",
      "1617:\tlearn: 0.6379775\ttotal: 1m 34s\tremaining: 28.1s\n",
      "1618:\tlearn: 0.6379243\ttotal: 1m 34s\tremaining: 28.1s\n",
      "1619:\tlearn: 0.6378793\ttotal: 1m 34s\tremaining: 28s\n",
      "1620:\tlearn: 0.6378197\ttotal: 1m 34s\tremaining: 27.9s\n",
      "1621:\tlearn: 0.6377399\ttotal: 1m 34s\tremaining: 27.9s\n",
      "1622:\tlearn: 0.6376210\ttotal: 1m 34s\tremaining: 27.8s\n",
      "1623:\tlearn: 0.6375485\ttotal: 1m 34s\tremaining: 27.8s\n",
      "1624:\tlearn: 0.6374966\ttotal: 1m 34s\tremaining: 27.7s\n",
      "1625:\tlearn: 0.6374406\ttotal: 1m 34s\tremaining: 27.6s\n",
      "1626:\tlearn: 0.6373988\ttotal: 1m 34s\tremaining: 27.6s\n",
      "1627:\tlearn: 0.6373569\ttotal: 1m 34s\tremaining: 27.5s\n",
      "1628:\tlearn: 0.6371698\ttotal: 1m 35s\tremaining: 27.5s\n",
      "1629:\tlearn: 0.6370779\ttotal: 1m 35s\tremaining: 27.4s\n",
      "1630:\tlearn: 0.6370301\ttotal: 1m 35s\tremaining: 27.4s\n",
      "1631:\tlearn: 0.6369359\ttotal: 1m 35s\tremaining: 27.3s\n",
      "1632:\tlearn: 0.6368769\ttotal: 1m 35s\tremaining: 27.2s\n",
      "1633:\tlearn: 0.6368032\ttotal: 1m 35s\tremaining: 27.2s\n",
      "1634:\tlearn: 0.6367452\ttotal: 1m 35s\tremaining: 27.1s\n",
      "1635:\tlearn: 0.6366543\ttotal: 1m 35s\tremaining: 27.1s\n",
      "1636:\tlearn: 0.6365642\ttotal: 1m 35s\tremaining: 27s\n",
      "1637:\tlearn: 0.6364911\ttotal: 1m 35s\tremaining: 26.9s\n",
      "1638:\tlearn: 0.6364565\ttotal: 1m 35s\tremaining: 26.9s\n",
      "1639:\tlearn: 0.6364198\ttotal: 1m 35s\tremaining: 26.8s\n",
      "1640:\tlearn: 0.6363376\ttotal: 1m 35s\tremaining: 26.8s\n",
      "1641:\tlearn: 0.6362213\ttotal: 1m 35s\tremaining: 26.7s\n",
      "1642:\tlearn: 0.6361969\ttotal: 1m 35s\tremaining: 26.7s\n",
      "1643:\tlearn: 0.6361368\ttotal: 1m 35s\tremaining: 26.6s\n",
      "1644:\tlearn: 0.6360917\ttotal: 1m 35s\tremaining: 26.5s\n",
      "1645:\tlearn: 0.6359834\ttotal: 1m 35s\tremaining: 26.5s\n",
      "1646:\tlearn: 0.6359352\ttotal: 1m 36s\tremaining: 26.4s\n",
      "1647:\tlearn: 0.6358680\ttotal: 1m 36s\tremaining: 26.4s\n",
      "1648:\tlearn: 0.6358092\ttotal: 1m 36s\tremaining: 26.3s\n",
      "1649:\tlearn: 0.6357385\ttotal: 1m 36s\tremaining: 26.2s\n",
      "1650:\tlearn: 0.6356535\ttotal: 1m 36s\tremaining: 26.2s\n",
      "1651:\tlearn: 0.6355746\ttotal: 1m 36s\tremaining: 26.1s\n",
      "1652:\tlearn: 0.6355240\ttotal: 1m 36s\tremaining: 26.1s\n",
      "1653:\tlearn: 0.6353944\ttotal: 1m 36s\tremaining: 26s\n",
      "1654:\tlearn: 0.6353526\ttotal: 1m 36s\tremaining: 25.9s\n",
      "1655:\tlearn: 0.6352961\ttotal: 1m 36s\tremaining: 25.9s\n",
      "1656:\tlearn: 0.6352432\ttotal: 1m 36s\tremaining: 25.8s\n",
      "1657:\tlearn: 0.6351996\ttotal: 1m 36s\tremaining: 25.8s\n",
      "1658:\tlearn: 0.6351476\ttotal: 1m 36s\tremaining: 25.7s\n",
      "1659:\tlearn: 0.6350232\ttotal: 1m 36s\tremaining: 25.7s\n",
      "1660:\tlearn: 0.6350027\ttotal: 1m 36s\tremaining: 25.6s\n",
      "1661:\tlearn: 0.6349397\ttotal: 1m 36s\tremaining: 25.5s\n",
      "1662:\tlearn: 0.6348545\ttotal: 1m 36s\tremaining: 25.5s\n",
      "1663:\tlearn: 0.6347969\ttotal: 1m 37s\tremaining: 25.4s\n",
      "1664:\tlearn: 0.6347278\ttotal: 1m 37s\tremaining: 25.4s\n",
      "1665:\tlearn: 0.6346222\ttotal: 1m 37s\tremaining: 25.3s\n",
      "1666:\tlearn: 0.6345943\ttotal: 1m 37s\tremaining: 25.2s\n",
      "1667:\tlearn: 0.6344704\ttotal: 1m 37s\tremaining: 25.2s\n",
      "1668:\tlearn: 0.6344139\ttotal: 1m 37s\tremaining: 25.1s\n",
      "1669:\tlearn: 0.6343652\ttotal: 1m 37s\tremaining: 25.1s\n",
      "1670:\tlearn: 0.6343377\ttotal: 1m 37s\tremaining: 25s\n",
      "1671:\tlearn: 0.6342896\ttotal: 1m 37s\tremaining: 25s\n",
      "1672:\tlearn: 0.6342215\ttotal: 1m 37s\tremaining: 24.9s\n",
      "1673:\tlearn: 0.6341690\ttotal: 1m 37s\tremaining: 24.8s\n",
      "1674:\tlearn: 0.6341430\ttotal: 1m 37s\tremaining: 24.8s\n",
      "1675:\tlearn: 0.6340893\ttotal: 1m 37s\tremaining: 24.7s\n",
      "1676:\tlearn: 0.6340661\ttotal: 1m 37s\tremaining: 24.7s\n",
      "1677:\tlearn: 0.6339942\ttotal: 1m 37s\tremaining: 24.6s\n",
      "1678:\tlearn: 0.6339473\ttotal: 1m 37s\tremaining: 24.6s\n",
      "1679:\tlearn: 0.6338936\ttotal: 1m 37s\tremaining: 24.5s\n",
      "1680:\tlearn: 0.6338141\ttotal: 1m 38s\tremaining: 24.4s\n",
      "1681:\tlearn: 0.6337275\ttotal: 1m 38s\tremaining: 24.4s\n",
      "1682:\tlearn: 0.6336509\ttotal: 1m 38s\tremaining: 24.3s\n",
      "1683:\tlearn: 0.6336301\ttotal: 1m 38s\tremaining: 24.3s\n",
      "1684:\tlearn: 0.6335515\ttotal: 1m 38s\tremaining: 24.2s\n",
      "1685:\tlearn: 0.6334797\ttotal: 1m 38s\tremaining: 24.1s\n",
      "1686:\tlearn: 0.6334042\ttotal: 1m 38s\tremaining: 24.1s\n",
      "1687:\tlearn: 0.6332671\ttotal: 1m 38s\tremaining: 24s\n",
      "1688:\tlearn: 0.6331757\ttotal: 1m 38s\tremaining: 24s\n",
      "1689:\tlearn: 0.6331534\ttotal: 1m 38s\tremaining: 23.9s\n",
      "1690:\tlearn: 0.6331098\ttotal: 1m 38s\tremaining: 23.8s\n",
      "1691:\tlearn: 0.6330277\ttotal: 1m 38s\tremaining: 23.8s\n",
      "1692:\tlearn: 0.6329821\ttotal: 1m 38s\tremaining: 23.7s\n",
      "1693:\tlearn: 0.6329389\ttotal: 1m 38s\tremaining: 23.7s\n",
      "1694:\tlearn: 0.6328435\ttotal: 1m 38s\tremaining: 23.6s\n",
      "1695:\tlearn: 0.6327714\ttotal: 1m 38s\tremaining: 23.6s\n",
      "1696:\tlearn: 0.6327292\ttotal: 1m 38s\tremaining: 23.5s\n",
      "1697:\tlearn: 0.6327157\ttotal: 1m 39s\tremaining: 23.4s\n",
      "1698:\tlearn: 0.6326173\ttotal: 1m 39s\tremaining: 23.4s\n",
      "1699:\tlearn: 0.6324620\ttotal: 1m 39s\tremaining: 23.3s\n",
      "1700:\tlearn: 0.6323328\ttotal: 1m 39s\tremaining: 23.3s\n",
      "1701:\tlearn: 0.6323095\ttotal: 1m 39s\tremaining: 23.2s\n",
      "1702:\tlearn: 0.6322751\ttotal: 1m 39s\tremaining: 23.1s\n",
      "1703:\tlearn: 0.6322202\ttotal: 1m 39s\tremaining: 23.1s\n",
      "1704:\tlearn: 0.6321625\ttotal: 1m 39s\tremaining: 23s\n",
      "1705:\tlearn: 0.6321025\ttotal: 1m 39s\tremaining: 23s\n",
      "1706:\tlearn: 0.6320197\ttotal: 1m 39s\tremaining: 22.9s\n",
      "1707:\tlearn: 0.6319463\ttotal: 1m 39s\tremaining: 22.9s\n",
      "1708:\tlearn: 0.6318642\ttotal: 1m 39s\tremaining: 22.8s\n",
      "1709:\tlearn: 0.6318109\ttotal: 1m 39s\tremaining: 22.7s\n",
      "1710:\tlearn: 0.6317717\ttotal: 1m 39s\tremaining: 22.7s\n",
      "1711:\tlearn: 0.6317233\ttotal: 1m 39s\tremaining: 22.6s\n",
      "1712:\tlearn: 0.6316592\ttotal: 1m 39s\tremaining: 22.6s\n",
      "1713:\tlearn: 0.6316066\ttotal: 1m 39s\tremaining: 22.5s\n",
      "1714:\tlearn: 0.6315381\ttotal: 1m 40s\tremaining: 22.4s\n",
      "1715:\tlearn: 0.6314907\ttotal: 1m 40s\tremaining: 22.4s\n",
      "1716:\tlearn: 0.6314484\ttotal: 1m 40s\tremaining: 22.3s\n",
      "1717:\tlearn: 0.6314319\ttotal: 1m 40s\tremaining: 22.3s\n",
      "1718:\tlearn: 0.6313209\ttotal: 1m 40s\tremaining: 22.2s\n",
      "1719:\tlearn: 0.6313041\ttotal: 1m 40s\tremaining: 22.2s\n",
      "1720:\tlearn: 0.6310990\ttotal: 1m 40s\tremaining: 22.1s\n",
      "1721:\tlearn: 0.6310496\ttotal: 1m 40s\tremaining: 22s\n",
      "1722:\tlearn: 0.6309774\ttotal: 1m 40s\tremaining: 22s\n",
      "1723:\tlearn: 0.6309566\ttotal: 1m 40s\tremaining: 21.9s\n",
      "1724:\tlearn: 0.6308841\ttotal: 1m 40s\tremaining: 21.9s\n",
      "1725:\tlearn: 0.6307968\ttotal: 1m 40s\tremaining: 21.8s\n",
      "1726:\tlearn: 0.6307468\ttotal: 1m 40s\tremaining: 21.8s\n",
      "1727:\tlearn: 0.6306903\ttotal: 1m 40s\tremaining: 21.7s\n",
      "1728:\tlearn: 0.6306547\ttotal: 1m 40s\tremaining: 21.6s\n",
      "1729:\tlearn: 0.6305926\ttotal: 1m 40s\tremaining: 21.6s\n",
      "1730:\tlearn: 0.6305574\ttotal: 1m 40s\tremaining: 21.5s\n",
      "1731:\tlearn: 0.6305073\ttotal: 1m 41s\tremaining: 21.5s\n",
      "1732:\tlearn: 0.6304675\ttotal: 1m 41s\tremaining: 21.4s\n",
      "1733:\tlearn: 0.6303202\ttotal: 1m 41s\tremaining: 21.3s\n",
      "1734:\tlearn: 0.6301916\ttotal: 1m 41s\tremaining: 21.3s\n",
      "1735:\tlearn: 0.6301237\ttotal: 1m 41s\tremaining: 21.2s\n",
      "1736:\tlearn: 0.6300958\ttotal: 1m 41s\tremaining: 21.2s\n",
      "1737:\tlearn: 0.6300173\ttotal: 1m 41s\tremaining: 21.1s\n",
      "1738:\tlearn: 0.6299049\ttotal: 1m 41s\tremaining: 21.1s\n",
      "1739:\tlearn: 0.6298926\ttotal: 1m 41s\tremaining: 21s\n",
      "1740:\tlearn: 0.6297552\ttotal: 1m 41s\tremaining: 20.9s\n",
      "1741:\tlearn: 0.6296592\ttotal: 1m 41s\tremaining: 20.9s\n",
      "1742:\tlearn: 0.6296264\ttotal: 1m 41s\tremaining: 20.8s\n",
      "1743:\tlearn: 0.6295533\ttotal: 1m 41s\tremaining: 20.8s\n",
      "1744:\tlearn: 0.6294796\ttotal: 1m 41s\tremaining: 20.7s\n",
      "1745:\tlearn: 0.6294486\ttotal: 1m 41s\tremaining: 20.6s\n",
      "1746:\tlearn: 0.6294037\ttotal: 1m 41s\tremaining: 20.6s\n",
      "1747:\tlearn: 0.6293657\ttotal: 1m 41s\tremaining: 20.5s\n",
      "1748:\tlearn: 0.6293183\ttotal: 1m 41s\tremaining: 20.5s\n",
      "1749:\tlearn: 0.6292099\ttotal: 1m 42s\tremaining: 20.4s\n",
      "1750:\tlearn: 0.6291668\ttotal: 1m 42s\tremaining: 20.4s\n",
      "1751:\tlearn: 0.6290627\ttotal: 1m 42s\tremaining: 20.3s\n",
      "1752:\tlearn: 0.6289990\ttotal: 1m 42s\tremaining: 20.2s\n",
      "1753:\tlearn: 0.6289280\ttotal: 1m 42s\tremaining: 20.2s\n",
      "1754:\tlearn: 0.6288993\ttotal: 1m 42s\tremaining: 20.1s\n",
      "1755:\tlearn: 0.6288420\ttotal: 1m 42s\tremaining: 20.1s\n",
      "1756:\tlearn: 0.6287486\ttotal: 1m 42s\tremaining: 20s\n",
      "1757:\tlearn: 0.6286677\ttotal: 1m 42s\tremaining: 19.9s\n",
      "1758:\tlearn: 0.6286274\ttotal: 1m 42s\tremaining: 19.9s\n",
      "1759:\tlearn: 0.6285603\ttotal: 1m 42s\tremaining: 19.8s\n",
      "1760:\tlearn: 0.6285398\ttotal: 1m 42s\tremaining: 19.8s\n",
      "1761:\tlearn: 0.6284671\ttotal: 1m 42s\tremaining: 19.7s\n",
      "1762:\tlearn: 0.6283367\ttotal: 1m 42s\tremaining: 19.6s\n",
      "1763:\tlearn: 0.6283016\ttotal: 1m 42s\tremaining: 19.6s\n",
      "1764:\tlearn: 0.6282154\ttotal: 1m 42s\tremaining: 19.5s\n",
      "1765:\tlearn: 0.6281458\ttotal: 1m 42s\tremaining: 19.5s\n",
      "1766:\tlearn: 0.6280773\ttotal: 1m 43s\tremaining: 19.4s\n",
      "1767:\tlearn: 0.6279857\ttotal: 1m 43s\tremaining: 19.4s\n",
      "1768:\tlearn: 0.6279285\ttotal: 1m 43s\tremaining: 19.3s\n",
      "1769:\tlearn: 0.6278889\ttotal: 1m 43s\tremaining: 19.2s\n",
      "1770:\tlearn: 0.6277807\ttotal: 1m 43s\tremaining: 19.2s\n",
      "1771:\tlearn: 0.6277305\ttotal: 1m 43s\tremaining: 19.1s\n",
      "1772:\tlearn: 0.6275989\ttotal: 1m 43s\tremaining: 19.1s\n",
      "1773:\tlearn: 0.6275507\ttotal: 1m 43s\tremaining: 19s\n",
      "1774:\tlearn: 0.6274850\ttotal: 1m 43s\tremaining: 19s\n",
      "1775:\tlearn: 0.6274513\ttotal: 1m 43s\tremaining: 18.9s\n",
      "1776:\tlearn: 0.6273984\ttotal: 1m 43s\tremaining: 18.8s\n",
      "1777:\tlearn: 0.6273097\ttotal: 1m 43s\tremaining: 18.8s\n",
      "1778:\tlearn: 0.6272756\ttotal: 1m 43s\tremaining: 18.7s\n",
      "1779:\tlearn: 0.6270413\ttotal: 1m 43s\tremaining: 18.7s\n",
      "1780:\tlearn: 0.6270136\ttotal: 1m 43s\tremaining: 18.6s\n",
      "1781:\tlearn: 0.6269315\ttotal: 1m 43s\tremaining: 18.5s\n",
      "1782:\tlearn: 0.6268929\ttotal: 1m 43s\tremaining: 18.5s\n",
      "1783:\tlearn: 0.6268735\ttotal: 1m 44s\tremaining: 18.4s\n",
      "1784:\tlearn: 0.6267825\ttotal: 1m 44s\tremaining: 18.4s\n",
      "1785:\tlearn: 0.6267343\ttotal: 1m 44s\tremaining: 18.3s\n",
      "1786:\tlearn: 0.6266754\ttotal: 1m 44s\tremaining: 18.3s\n",
      "1787:\tlearn: 0.6266284\ttotal: 1m 44s\tremaining: 18.2s\n",
      "1788:\tlearn: 0.6264989\ttotal: 1m 44s\tremaining: 18.1s\n",
      "1789:\tlearn: 0.6264522\ttotal: 1m 44s\tremaining: 18.1s\n",
      "1790:\tlearn: 0.6264307\ttotal: 1m 44s\tremaining: 18s\n",
      "1791:\tlearn: 0.6263493\ttotal: 1m 44s\tremaining: 18s\n",
      "1792:\tlearn: 0.6262635\ttotal: 1m 44s\tremaining: 17.9s\n",
      "1793:\tlearn: 0.6262396\ttotal: 1m 44s\tremaining: 17.8s\n",
      "1794:\tlearn: 0.6262100\ttotal: 1m 44s\tremaining: 17.8s\n",
      "1795:\tlearn: 0.6261306\ttotal: 1m 44s\tremaining: 17.7s\n",
      "1796:\tlearn: 0.6260578\ttotal: 1m 44s\tremaining: 17.7s\n",
      "1797:\tlearn: 0.6259808\ttotal: 1m 44s\tremaining: 17.6s\n",
      "1798:\tlearn: 0.6259399\ttotal: 1m 44s\tremaining: 17.6s\n",
      "1799:\tlearn: 0.6258672\ttotal: 1m 44s\tremaining: 17.5s\n",
      "1800:\tlearn: 0.6258124\ttotal: 1m 45s\tremaining: 17.4s\n",
      "1801:\tlearn: 0.6257729\ttotal: 1m 45s\tremaining: 17.4s\n",
      "1802:\tlearn: 0.6256611\ttotal: 1m 45s\tremaining: 17.3s\n",
      "1803:\tlearn: 0.6256457\ttotal: 1m 45s\tremaining: 17.3s\n",
      "1804:\tlearn: 0.6255572\ttotal: 1m 45s\tremaining: 17.2s\n",
      "1805:\tlearn: 0.6254451\ttotal: 1m 45s\tremaining: 17.1s\n",
      "1806:\tlearn: 0.6254048\ttotal: 1m 45s\tremaining: 17.1s\n",
      "1807:\tlearn: 0.6253492\ttotal: 1m 45s\tremaining: 17s\n",
      "1808:\tlearn: 0.6253156\ttotal: 1m 45s\tremaining: 17s\n",
      "1809:\tlearn: 0.6251240\ttotal: 1m 45s\tremaining: 16.9s\n",
      "1810:\tlearn: 0.6250019\ttotal: 1m 45s\tremaining: 16.9s\n",
      "1811:\tlearn: 0.6249178\ttotal: 1m 45s\tremaining: 16.8s\n",
      "1812:\tlearn: 0.6248756\ttotal: 1m 45s\tremaining: 16.7s\n",
      "1813:\tlearn: 0.6248330\ttotal: 1m 45s\tremaining: 16.7s\n",
      "1814:\tlearn: 0.6247826\ttotal: 1m 45s\tremaining: 16.6s\n",
      "1815:\tlearn: 0.6247414\ttotal: 1m 45s\tremaining: 16.6s\n",
      "1816:\tlearn: 0.6246134\ttotal: 1m 45s\tremaining: 16.5s\n",
      "1817:\tlearn: 0.6245615\ttotal: 1m 46s\tremaining: 16.4s\n",
      "1818:\tlearn: 0.6244927\ttotal: 1m 46s\tremaining: 16.4s\n",
      "1819:\tlearn: 0.6244535\ttotal: 1m 46s\tremaining: 16.3s\n",
      "1820:\tlearn: 0.6243822\ttotal: 1m 46s\tremaining: 16.3s\n",
      "1821:\tlearn: 0.6242974\ttotal: 1m 46s\tremaining: 16.2s\n",
      "1822:\tlearn: 0.6242143\ttotal: 1m 46s\tremaining: 16.1s\n",
      "1823:\tlearn: 0.6241487\ttotal: 1m 46s\tremaining: 16.1s\n",
      "1824:\tlearn: 0.6240670\ttotal: 1m 46s\tremaining: 16s\n",
      "1825:\tlearn: 0.6240473\ttotal: 1m 46s\tremaining: 16s\n",
      "1826:\tlearn: 0.6239341\ttotal: 1m 46s\tremaining: 15.9s\n",
      "1827:\tlearn: 0.6238893\ttotal: 1m 46s\tremaining: 15.9s\n",
      "1828:\tlearn: 0.6237874\ttotal: 1m 46s\tremaining: 15.8s\n",
      "1829:\tlearn: 0.6237071\ttotal: 1m 46s\tremaining: 15.7s\n",
      "1830:\tlearn: 0.6236512\ttotal: 1m 46s\tremaining: 15.7s\n",
      "1831:\tlearn: 0.6235525\ttotal: 1m 46s\tremaining: 15.6s\n",
      "1832:\tlearn: 0.6235082\ttotal: 1m 46s\tremaining: 15.6s\n",
      "1833:\tlearn: 0.6234594\ttotal: 1m 46s\tremaining: 15.5s\n",
      "1834:\tlearn: 0.6234008\ttotal: 1m 46s\tremaining: 15.5s\n",
      "1835:\tlearn: 0.6233865\ttotal: 1m 47s\tremaining: 15.4s\n",
      "1836:\tlearn: 0.6233419\ttotal: 1m 47s\tremaining: 15.3s\n",
      "1837:\tlearn: 0.6233131\ttotal: 1m 47s\tremaining: 15.3s\n",
      "1838:\tlearn: 0.6232868\ttotal: 1m 47s\tremaining: 15.2s\n",
      "1839:\tlearn: 0.6232569\ttotal: 1m 47s\tremaining: 15.2s\n",
      "1840:\tlearn: 0.6232100\ttotal: 1m 47s\tremaining: 15.1s\n",
      "1841:\tlearn: 0.6231538\ttotal: 1m 47s\tremaining: 15s\n",
      "1842:\tlearn: 0.6230891\ttotal: 1m 47s\tremaining: 15s\n",
      "1843:\tlearn: 0.6229861\ttotal: 1m 47s\tremaining: 14.9s\n",
      "1844:\tlearn: 0.6228816\ttotal: 1m 47s\tremaining: 14.9s\n",
      "1845:\tlearn: 0.6228488\ttotal: 1m 47s\tremaining: 14.8s\n",
      "1846:\tlearn: 0.6228201\ttotal: 1m 47s\tremaining: 14.8s\n",
      "1847:\tlearn: 0.6227750\ttotal: 1m 47s\tremaining: 14.7s\n",
      "1848:\tlearn: 0.6227198\ttotal: 1m 47s\tremaining: 14.6s\n",
      "1849:\tlearn: 0.6226727\ttotal: 1m 47s\tremaining: 14.6s\n",
      "1850:\tlearn: 0.6226224\ttotal: 1m 47s\tremaining: 14.5s\n",
      "1851:\tlearn: 0.6226021\ttotal: 1m 47s\tremaining: 14.5s\n",
      "1852:\tlearn: 0.6225280\ttotal: 1m 48s\tremaining: 14.4s\n",
      "1853:\tlearn: 0.6224973\ttotal: 1m 48s\tremaining: 14.3s\n",
      "1854:\tlearn: 0.6224579\ttotal: 1m 48s\tremaining: 14.3s\n",
      "1855:\tlearn: 0.6224253\ttotal: 1m 48s\tremaining: 14.2s\n",
      "1856:\tlearn: 0.6223939\ttotal: 1m 48s\tremaining: 14.2s\n",
      "1857:\tlearn: 0.6223833\ttotal: 1m 48s\tremaining: 14.1s\n",
      "1858:\tlearn: 0.6223535\ttotal: 1m 48s\tremaining: 14.1s\n",
      "1859:\tlearn: 0.6222670\ttotal: 1m 48s\tremaining: 14s\n",
      "1860:\tlearn: 0.6222168\ttotal: 1m 48s\tremaining: 13.9s\n",
      "1861:\tlearn: 0.6221412\ttotal: 1m 48s\tremaining: 13.9s\n",
      "1862:\tlearn: 0.6220919\ttotal: 1m 48s\tremaining: 13.8s\n",
      "1863:\tlearn: 0.6220659\ttotal: 1m 48s\tremaining: 13.8s\n",
      "1864:\tlearn: 0.6219778\ttotal: 1m 48s\tremaining: 13.7s\n",
      "1865:\tlearn: 0.6218841\ttotal: 1m 48s\tremaining: 13.6s\n",
      "1866:\tlearn: 0.6218426\ttotal: 1m 48s\tremaining: 13.6s\n",
      "1867:\tlearn: 0.6217880\ttotal: 1m 48s\tremaining: 13.5s\n",
      "1868:\tlearn: 0.6217100\ttotal: 1m 48s\tremaining: 13.5s\n",
      "1869:\tlearn: 0.6215512\ttotal: 1m 49s\tremaining: 13.4s\n",
      "1870:\tlearn: 0.6215007\ttotal: 1m 49s\tremaining: 13.4s\n",
      "1871:\tlearn: 0.6214129\ttotal: 1m 49s\tremaining: 13.3s\n",
      "1872:\tlearn: 0.6213558\ttotal: 1m 49s\tremaining: 13.2s\n",
      "1873:\tlearn: 0.6212913\ttotal: 1m 49s\tremaining: 13.2s\n",
      "1874:\tlearn: 0.6212498\ttotal: 1m 49s\tremaining: 13.1s\n",
      "1875:\tlearn: 0.6211914\ttotal: 1m 49s\tremaining: 13.1s\n",
      "1876:\tlearn: 0.6210212\ttotal: 1m 49s\tremaining: 13s\n",
      "1877:\tlearn: 0.6209341\ttotal: 1m 49s\tremaining: 12.9s\n",
      "1878:\tlearn: 0.6208643\ttotal: 1m 49s\tremaining: 12.9s\n",
      "1879:\tlearn: 0.6207982\ttotal: 1m 49s\tremaining: 12.8s\n",
      "1880:\tlearn: 0.6207080\ttotal: 1m 49s\tremaining: 12.8s\n",
      "1881:\tlearn: 0.6206056\ttotal: 1m 49s\tremaining: 12.7s\n",
      "1882:\tlearn: 0.6205882\ttotal: 1m 49s\tremaining: 12.7s\n",
      "1883:\tlearn: 0.6205343\ttotal: 1m 49s\tremaining: 12.6s\n",
      "1884:\tlearn: 0.6204860\ttotal: 1m 49s\tremaining: 12.5s\n",
      "1885:\tlearn: 0.6204602\ttotal: 1m 49s\tremaining: 12.5s\n",
      "1886:\tlearn: 0.6203994\ttotal: 1m 50s\tremaining: 12.4s\n",
      "1887:\tlearn: 0.6203639\ttotal: 1m 50s\tremaining: 12.4s\n",
      "1888:\tlearn: 0.6202448\ttotal: 1m 50s\tremaining: 12.3s\n",
      "1889:\tlearn: 0.6201253\ttotal: 1m 50s\tremaining: 12.2s\n",
      "1890:\tlearn: 0.6200600\ttotal: 1m 50s\tremaining: 12.2s\n",
      "1891:\tlearn: 0.6200213\ttotal: 1m 50s\tremaining: 12.1s\n",
      "1892:\tlearn: 0.6199799\ttotal: 1m 50s\tremaining: 12.1s\n",
      "1893:\tlearn: 0.6199262\ttotal: 1m 50s\tremaining: 12s\n",
      "1894:\tlearn: 0.6198622\ttotal: 1m 50s\tremaining: 12s\n",
      "1895:\tlearn: 0.6197761\ttotal: 1m 50s\tremaining: 11.9s\n",
      "1896:\tlearn: 0.6196927\ttotal: 1m 50s\tremaining: 11.8s\n",
      "1897:\tlearn: 0.6196355\ttotal: 1m 50s\tremaining: 11.8s\n",
      "1898:\tlearn: 0.6195743\ttotal: 1m 50s\tremaining: 11.7s\n",
      "1899:\tlearn: 0.6194976\ttotal: 1m 50s\tremaining: 11.7s\n",
      "1900:\tlearn: 0.6194516\ttotal: 1m 50s\tremaining: 11.6s\n",
      "1901:\tlearn: 0.6193800\ttotal: 1m 50s\tremaining: 11.5s\n",
      "1902:\tlearn: 0.6193404\ttotal: 1m 50s\tremaining: 11.5s\n",
      "1903:\tlearn: 0.6192824\ttotal: 1m 51s\tremaining: 11.4s\n",
      "1904:\tlearn: 0.6192199\ttotal: 1m 51s\tremaining: 11.4s\n",
      "1905:\tlearn: 0.6191775\ttotal: 1m 51s\tremaining: 11.3s\n",
      "1906:\tlearn: 0.6191090\ttotal: 1m 51s\tremaining: 11.3s\n",
      "1907:\tlearn: 0.6190339\ttotal: 1m 51s\tremaining: 11.2s\n",
      "1908:\tlearn: 0.6189815\ttotal: 1m 51s\tremaining: 11.1s\n",
      "1909:\tlearn: 0.6189216\ttotal: 1m 51s\tremaining: 11.1s\n",
      "1910:\tlearn: 0.6188295\ttotal: 1m 51s\tremaining: 11s\n",
      "1911:\tlearn: 0.6187923\ttotal: 1m 51s\tremaining: 11s\n",
      "1912:\tlearn: 0.6187768\ttotal: 1m 51s\tremaining: 10.9s\n",
      "1913:\tlearn: 0.6187006\ttotal: 1m 51s\tremaining: 10.8s\n",
      "1914:\tlearn: 0.6185880\ttotal: 1m 51s\tremaining: 10.8s\n",
      "1915:\tlearn: 0.6184940\ttotal: 1m 51s\tremaining: 10.7s\n",
      "1916:\tlearn: 0.6183998\ttotal: 1m 51s\tremaining: 10.7s\n",
      "1917:\tlearn: 0.6182959\ttotal: 1m 51s\tremaining: 10.6s\n",
      "1918:\tlearn: 0.6182891\ttotal: 1m 51s\tremaining: 10.6s\n",
      "1919:\tlearn: 0.6182092\ttotal: 1m 51s\tremaining: 10.5s\n",
      "1920:\tlearn: 0.6180306\ttotal: 1m 52s\tremaining: 10.4s\n",
      "1921:\tlearn: 0.6178807\ttotal: 1m 52s\tremaining: 10.4s\n",
      "1922:\tlearn: 0.6177873\ttotal: 1m 52s\tremaining: 10.3s\n",
      "1923:\tlearn: 0.6177378\ttotal: 1m 52s\tremaining: 10.3s\n",
      "1924:\tlearn: 0.6177091\ttotal: 1m 52s\tremaining: 10.2s\n",
      "1925:\tlearn: 0.6176240\ttotal: 1m 52s\tremaining: 10.1s\n",
      "1926:\tlearn: 0.6175648\ttotal: 1m 52s\tremaining: 10.1s\n",
      "1927:\tlearn: 0.6174977\ttotal: 1m 52s\tremaining: 10s\n",
      "1928:\tlearn: 0.6174659\ttotal: 1m 52s\tremaining: 9.97s\n",
      "1929:\tlearn: 0.6174364\ttotal: 1m 52s\tremaining: 9.91s\n",
      "1930:\tlearn: 0.6173360\ttotal: 1m 52s\tremaining: 9.85s\n",
      "1931:\tlearn: 0.6173061\ttotal: 1m 52s\tremaining: 9.79s\n",
      "1932:\tlearn: 0.6171360\ttotal: 1m 52s\tremaining: 9.74s\n",
      "1933:\tlearn: 0.6170255\ttotal: 1m 52s\tremaining: 9.68s\n",
      "1934:\tlearn: 0.6169584\ttotal: 1m 52s\tremaining: 9.62s\n",
      "1935:\tlearn: 0.6169147\ttotal: 1m 52s\tremaining: 9.56s\n",
      "1936:\tlearn: 0.6168913\ttotal: 1m 52s\tremaining: 9.5s\n",
      "1937:\tlearn: 0.6168540\ttotal: 1m 52s\tremaining: 9.44s\n",
      "1938:\tlearn: 0.6168234\ttotal: 1m 53s\tremaining: 9.39s\n",
      "1939:\tlearn: 0.6166984\ttotal: 1m 53s\tremaining: 9.33s\n",
      "1940:\tlearn: 0.6166392\ttotal: 1m 53s\tremaining: 9.27s\n",
      "1941:\tlearn: 0.6165840\ttotal: 1m 53s\tremaining: 9.21s\n",
      "1942:\tlearn: 0.6165767\ttotal: 1m 53s\tremaining: 9.15s\n",
      "1943:\tlearn: 0.6165442\ttotal: 1m 53s\tremaining: 9.09s\n",
      "1944:\tlearn: 0.6165186\ttotal: 1m 53s\tremaining: 9.04s\n",
      "1945:\tlearn: 0.6163659\ttotal: 1m 53s\tremaining: 8.98s\n",
      "1946:\tlearn: 0.6163110\ttotal: 1m 53s\tremaining: 8.92s\n",
      "1947:\tlearn: 0.6162676\ttotal: 1m 53s\tremaining: 8.86s\n",
      "1948:\tlearn: 0.6162145\ttotal: 1m 53s\tremaining: 8.8s\n",
      "1949:\tlearn: 0.6161781\ttotal: 1m 53s\tremaining: 8.74s\n",
      "1950:\tlearn: 0.6160921\ttotal: 1m 53s\tremaining: 8.69s\n",
      "1951:\tlearn: 0.6160459\ttotal: 1m 53s\tremaining: 8.63s\n",
      "1952:\tlearn: 0.6159339\ttotal: 1m 53s\tremaining: 8.57s\n",
      "1953:\tlearn: 0.6158768\ttotal: 1m 53s\tremaining: 8.51s\n",
      "1954:\tlearn: 0.6158268\ttotal: 1m 53s\tremaining: 8.45s\n",
      "1955:\tlearn: 0.6157554\ttotal: 1m 54s\tremaining: 8.39s\n",
      "1956:\tlearn: 0.6156783\ttotal: 1m 54s\tremaining: 8.34s\n",
      "1957:\tlearn: 0.6156197\ttotal: 1m 54s\tremaining: 8.28s\n",
      "1958:\tlearn: 0.6155535\ttotal: 1m 54s\tremaining: 8.22s\n",
      "1959:\tlearn: 0.6154943\ttotal: 1m 54s\tremaining: 8.16s\n",
      "1960:\tlearn: 0.6154092\ttotal: 1m 54s\tremaining: 8.1s\n",
      "1961:\tlearn: 0.6153765\ttotal: 1m 54s\tremaining: 8.04s\n",
      "1962:\tlearn: 0.6153307\ttotal: 1m 54s\tremaining: 7.99s\n",
      "1963:\tlearn: 0.6153033\ttotal: 1m 54s\tremaining: 7.93s\n",
      "1964:\tlearn: 0.6152408\ttotal: 1m 54s\tremaining: 7.87s\n",
      "1965:\tlearn: 0.6151732\ttotal: 1m 54s\tremaining: 7.81s\n",
      "1966:\tlearn: 0.6150740\ttotal: 1m 54s\tremaining: 7.75s\n",
      "1967:\tlearn: 0.6149280\ttotal: 1m 54s\tremaining: 7.69s\n",
      "1968:\tlearn: 0.6148452\ttotal: 1m 54s\tremaining: 7.64s\n",
      "1969:\tlearn: 0.6148172\ttotal: 1m 54s\tremaining: 7.58s\n",
      "1970:\tlearn: 0.6147897\ttotal: 1m 54s\tremaining: 7.52s\n",
      "1971:\tlearn: 0.6147428\ttotal: 1m 54s\tremaining: 7.46s\n",
      "1972:\tlearn: 0.6146516\ttotal: 1m 55s\tremaining: 7.4s\n",
      "1973:\tlearn: 0.6146231\ttotal: 1m 55s\tremaining: 7.34s\n",
      "1974:\tlearn: 0.6144722\ttotal: 1m 55s\tremaining: 7.29s\n",
      "1975:\tlearn: 0.6144375\ttotal: 1m 55s\tremaining: 7.23s\n",
      "1976:\tlearn: 0.6144018\ttotal: 1m 55s\tremaining: 7.17s\n",
      "1977:\tlearn: 0.6143202\ttotal: 1m 55s\tremaining: 7.11s\n",
      "1978:\tlearn: 0.6142804\ttotal: 1m 55s\tremaining: 7.05s\n",
      "1979:\tlearn: 0.6142414\ttotal: 1m 55s\tremaining: 7s\n",
      "1980:\tlearn: 0.6141783\ttotal: 1m 55s\tremaining: 6.94s\n",
      "1981:\tlearn: 0.6141417\ttotal: 1m 55s\tremaining: 6.88s\n",
      "1982:\tlearn: 0.6141108\ttotal: 1m 55s\tremaining: 6.82s\n",
      "1983:\tlearn: 0.6140627\ttotal: 1m 55s\tremaining: 6.76s\n",
      "1984:\tlearn: 0.6140188\ttotal: 1m 55s\tremaining: 6.7s\n",
      "1985:\tlearn: 0.6139638\ttotal: 1m 55s\tremaining: 6.64s\n",
      "1986:\tlearn: 0.6138892\ttotal: 1m 55s\tremaining: 6.59s\n",
      "1987:\tlearn: 0.6137706\ttotal: 1m 55s\tremaining: 6.53s\n",
      "1988:\tlearn: 0.6136359\ttotal: 1m 55s\tremaining: 6.47s\n",
      "1989:\tlearn: 0.6135329\ttotal: 1m 56s\tremaining: 6.41s\n",
      "1990:\tlearn: 0.6134559\ttotal: 1m 56s\tremaining: 6.35s\n",
      "1991:\tlearn: 0.6134189\ttotal: 1m 56s\tremaining: 6.3s\n",
      "1992:\tlearn: 0.6133295\ttotal: 1m 56s\tremaining: 6.24s\n",
      "1993:\tlearn: 0.6133151\ttotal: 1m 56s\tremaining: 6.18s\n",
      "1994:\tlearn: 0.6132444\ttotal: 1m 56s\tremaining: 6.12s\n",
      "1995:\tlearn: 0.6131864\ttotal: 1m 56s\tremaining: 6.06s\n",
      "1996:\tlearn: 0.6131481\ttotal: 1m 56s\tremaining: 6s\n",
      "1997:\tlearn: 0.6130888\ttotal: 1m 56s\tremaining: 5.95s\n",
      "1998:\tlearn: 0.6130239\ttotal: 1m 56s\tremaining: 5.89s\n",
      "1999:\tlearn: 0.6129563\ttotal: 1m 56s\tremaining: 5.83s\n",
      "2000:\tlearn: 0.6128850\ttotal: 1m 56s\tremaining: 5.77s\n",
      "2001:\tlearn: 0.6128471\ttotal: 1m 56s\tremaining: 5.71s\n",
      "2002:\tlearn: 0.6127880\ttotal: 1m 56s\tremaining: 5.65s\n",
      "2003:\tlearn: 0.6127216\ttotal: 1m 56s\tremaining: 5.6s\n",
      "2004:\tlearn: 0.6126770\ttotal: 1m 56s\tremaining: 5.54s\n",
      "2005:\tlearn: 0.6126467\ttotal: 1m 56s\tremaining: 5.48s\n",
      "2006:\tlearn: 0.6126110\ttotal: 1m 57s\tremaining: 5.42s\n",
      "2007:\tlearn: 0.6125369\ttotal: 1m 57s\tremaining: 5.36s\n",
      "2008:\tlearn: 0.6124940\ttotal: 1m 57s\tremaining: 5.3s\n",
      "2009:\tlearn: 0.6124298\ttotal: 1m 57s\tremaining: 5.25s\n",
      "2010:\tlearn: 0.6123744\ttotal: 1m 57s\tremaining: 5.19s\n",
      "2011:\tlearn: 0.6123337\ttotal: 1m 57s\tremaining: 5.13s\n",
      "2012:\tlearn: 0.6122844\ttotal: 1m 57s\tremaining: 5.07s\n",
      "2013:\tlearn: 0.6122147\ttotal: 1m 57s\tremaining: 5.01s\n",
      "2014:\tlearn: 0.6121401\ttotal: 1m 57s\tremaining: 4.96s\n",
      "2015:\tlearn: 0.6120425\ttotal: 1m 57s\tremaining: 4.9s\n",
      "2016:\tlearn: 0.6120326\ttotal: 1m 57s\tremaining: 4.84s\n",
      "2017:\tlearn: 0.6120013\ttotal: 1m 57s\tremaining: 4.78s\n",
      "2018:\tlearn: 0.6119833\ttotal: 1m 57s\tremaining: 4.72s\n",
      "2019:\tlearn: 0.6119499\ttotal: 1m 57s\tremaining: 4.66s\n",
      "2020:\tlearn: 0.6118679\ttotal: 1m 57s\tremaining: 4.61s\n",
      "2021:\tlearn: 0.6118133\ttotal: 1m 57s\tremaining: 4.55s\n",
      "2022:\tlearn: 0.6117522\ttotal: 1m 57s\tremaining: 4.49s\n",
      "2023:\tlearn: 0.6116889\ttotal: 1m 57s\tremaining: 4.43s\n",
      "2024:\tlearn: 0.6116568\ttotal: 1m 58s\tremaining: 4.37s\n",
      "2025:\tlearn: 0.6115698\ttotal: 1m 58s\tremaining: 4.31s\n",
      "2026:\tlearn: 0.6115128\ttotal: 1m 58s\tremaining: 4.25s\n",
      "2027:\tlearn: 0.6115042\ttotal: 1m 58s\tremaining: 4.2s\n",
      "2028:\tlearn: 0.6113897\ttotal: 1m 58s\tremaining: 4.14s\n",
      "2029:\tlearn: 0.6113392\ttotal: 1m 58s\tremaining: 4.08s\n",
      "2030:\tlearn: 0.6113141\ttotal: 1m 58s\tremaining: 4.02s\n",
      "2031:\tlearn: 0.6112588\ttotal: 1m 58s\tremaining: 3.96s\n",
      "2032:\tlearn: 0.6111671\ttotal: 1m 58s\tremaining: 3.91s\n",
      "2033:\tlearn: 0.6111447\ttotal: 1m 58s\tremaining: 3.85s\n",
      "2034:\tlearn: 0.6109312\ttotal: 1m 58s\tremaining: 3.79s\n",
      "2035:\tlearn: 0.6108784\ttotal: 1m 58s\tremaining: 3.73s\n",
      "2036:\tlearn: 0.6108395\ttotal: 1m 58s\tremaining: 3.67s\n",
      "2037:\tlearn: 0.6107344\ttotal: 1m 58s\tremaining: 3.61s\n",
      "2038:\tlearn: 0.6107020\ttotal: 1m 58s\tremaining: 3.56s\n",
      "2039:\tlearn: 0.6105945\ttotal: 1m 58s\tremaining: 3.5s\n",
      "2040:\tlearn: 0.6103966\ttotal: 1m 58s\tremaining: 3.44s\n",
      "2041:\tlearn: 0.6103063\ttotal: 1m 59s\tremaining: 3.38s\n",
      "2042:\tlearn: 0.6102573\ttotal: 1m 59s\tremaining: 3.32s\n",
      "2043:\tlearn: 0.6101807\ttotal: 1m 59s\tremaining: 3.26s\n",
      "2044:\tlearn: 0.6101092\ttotal: 1m 59s\tremaining: 3.21s\n",
      "2045:\tlearn: 0.6100791\ttotal: 1m 59s\tremaining: 3.15s\n",
      "2046:\tlearn: 0.6100115\ttotal: 1m 59s\tremaining: 3.09s\n",
      "2047:\tlearn: 0.6099935\ttotal: 1m 59s\tremaining: 3.03s\n",
      "2048:\tlearn: 0.6099339\ttotal: 1m 59s\tremaining: 2.97s\n",
      "2049:\tlearn: 0.6099075\ttotal: 1m 59s\tremaining: 2.92s\n",
      "2050:\tlearn: 0.6098605\ttotal: 1m 59s\tremaining: 2.86s\n",
      "2051:\tlearn: 0.6098252\ttotal: 1m 59s\tremaining: 2.8s\n",
      "2052:\tlearn: 0.6097681\ttotal: 1m 59s\tremaining: 2.74s\n",
      "2053:\tlearn: 0.6097372\ttotal: 1m 59s\tremaining: 2.68s\n",
      "2054:\tlearn: 0.6095985\ttotal: 1m 59s\tremaining: 2.62s\n",
      "2055:\tlearn: 0.6095315\ttotal: 1m 59s\tremaining: 2.56s\n",
      "2056:\tlearn: 0.6094689\ttotal: 1m 59s\tremaining: 2.51s\n",
      "2057:\tlearn: 0.6094316\ttotal: 1m 59s\tremaining: 2.45s\n",
      "2058:\tlearn: 0.6093360\ttotal: 2m\tremaining: 2.39s\n",
      "2059:\tlearn: 0.6093105\ttotal: 2m\tremaining: 2.33s\n",
      "2060:\tlearn: 0.6092860\ttotal: 2m\tremaining: 2.27s\n",
      "2061:\tlearn: 0.6091861\ttotal: 2m\tremaining: 2.21s\n",
      "2062:\tlearn: 0.6091203\ttotal: 2m\tremaining: 2.16s\n",
      "2063:\tlearn: 0.6090452\ttotal: 2m\tremaining: 2.1s\n",
      "2064:\tlearn: 0.6090349\ttotal: 2m\tremaining: 2.04s\n",
      "2065:\tlearn: 0.6089838\ttotal: 2m\tremaining: 1.98s\n",
      "2066:\tlearn: 0.6089777\ttotal: 2m\tremaining: 1.92s\n",
      "2067:\tlearn: 0.6089022\ttotal: 2m\tremaining: 1.86s\n",
      "2068:\tlearn: 0.6088348\ttotal: 2m\tremaining: 1.81s\n",
      "2069:\tlearn: 0.6087596\ttotal: 2m\tremaining: 1.75s\n",
      "2070:\tlearn: 0.6086720\ttotal: 2m\tremaining: 1.69s\n",
      "2071:\tlearn: 0.6086121\ttotal: 2m\tremaining: 1.63s\n",
      "2072:\tlearn: 0.6085342\ttotal: 2m\tremaining: 1.57s\n",
      "2073:\tlearn: 0.6084792\ttotal: 2m\tremaining: 1.52s\n",
      "2074:\tlearn: 0.6084280\ttotal: 2m\tremaining: 1.46s\n",
      "2075:\tlearn: 0.6083760\ttotal: 2m 1s\tremaining: 1.4s\n",
      "2076:\tlearn: 0.6083218\ttotal: 2m 1s\tremaining: 1.34s\n",
      "2077:\tlearn: 0.6082793\ttotal: 2m 1s\tremaining: 1.28s\n",
      "2078:\tlearn: 0.6082290\ttotal: 2m 1s\tremaining: 1.22s\n",
      "2079:\tlearn: 0.6081156\ttotal: 2m 1s\tremaining: 1.17s\n",
      "2080:\tlearn: 0.6079970\ttotal: 2m 1s\tremaining: 1.11s\n",
      "2081:\tlearn: 0.6079615\ttotal: 2m 1s\tremaining: 1.05s\n",
      "2082:\tlearn: 0.6079139\ttotal: 2m 1s\tremaining: 991ms\n",
      "2083:\tlearn: 0.6078164\ttotal: 2m 1s\tremaining: 933ms\n",
      "2084:\tlearn: 0.6077713\ttotal: 2m 1s\tremaining: 875ms\n",
      "2085:\tlearn: 0.6077356\ttotal: 2m 1s\tremaining: 816ms\n",
      "2086:\tlearn: 0.6076655\ttotal: 2m 1s\tremaining: 758ms\n",
      "2087:\tlearn: 0.6075824\ttotal: 2m 1s\tremaining: 700ms\n",
      "2088:\tlearn: 0.6075492\ttotal: 2m 1s\tremaining: 641ms\n",
      "2089:\tlearn: 0.6074965\ttotal: 2m 1s\tremaining: 583ms\n",
      "2090:\tlearn: 0.6074772\ttotal: 2m 1s\tremaining: 525ms\n",
      "2091:\tlearn: 0.6074362\ttotal: 2m 1s\tremaining: 466ms\n",
      "2092:\tlearn: 0.6073744\ttotal: 2m 2s\tremaining: 408ms\n",
      "2093:\tlearn: 0.6073495\ttotal: 2m 2s\tremaining: 350ms\n",
      "2094:\tlearn: 0.6073045\ttotal: 2m 2s\tremaining: 292ms\n",
      "2095:\tlearn: 0.6071636\ttotal: 2m 2s\tremaining: 233ms\n",
      "2096:\tlearn: 0.6070896\ttotal: 2m 2s\tremaining: 175ms\n",
      "2097:\tlearn: 0.6070551\ttotal: 2m 2s\tremaining: 117ms\n",
      "2098:\tlearn: 0.6069552\ttotal: 2m 2s\tremaining: 58.3ms\n",
      "2099:\tlearn: 0.6069420\ttotal: 2m 2s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/Case Study 01/Data/catboost_reg_Rev02.sav']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reference: https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/\n",
    "\n",
    "catboost_reg = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 2100,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU')\n",
    "\n",
    "catboost_reg.fit(X_train[:], y_train[:])\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/catboost_reg_Rev02.sav'\n",
    "joblib.dump(catboost_reg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHJXE_LiDkX0"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "catboost_reg = joblib.load('/content/drive/MyDrive/Case Study 01/Data/catboost_reg_Rev02.sav')\n",
    "\n",
    "#Predictions\n",
    "y_cv_pred = catboost_reg.predict(X_cv)\n",
    "y_train_pred = catboost_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103131,
     "status": "ok",
     "timestamp": 1639647635487,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "YBZCGWZmeuOr",
    "outputId": "aefdf8e6-03ad-429d-8c17-6984fe3286b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For catboost_reg the RMSLE values are \n",
      "    train RMSLE = 0.16 \n",
      "    CV RMSLE = 0.18\n",
      "--------------------------------------------------\n",
      "2.For catboost_reg the RMSE values are \n",
      "    train RMSE = 0.61 \n",
      "    CV RMSLE = 0.69\n",
      "--------------------------------------------------\n",
      "3.For catboost_reg the RMSLE(CV) values are \n",
      "    train RMSE(CV) = 0.13  \n",
      "    CV RMSE(CV) = 0.16 \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "catboost_reg_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "catboost_reg_train_RMSLE = compute_RMSLE(y_train[:], y_train_pred)\n",
    "\n",
    "catboost_reg_cv_RMSE = compute_RMSE(y_cv[:], y_cv_pred)\n",
    "catboost_reg_train_RMSE = compute_RMSE(y_train[:], y_train_pred)\n",
    "\n",
    "catboost_reg_cv_RMSECV = compute_RMSE_CV(y_cv, y_cv_pred)\n",
    "catboost_reg_train_RMSECV = compute_RMSE_CV(y_train, y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"1.For catboost_reg the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(catboost_reg_train_RMSLE, catboost_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For catboost_reg the RMSE values are \\n    train RMSE = {0} \\n    CV RMSLE = {1}\".format(catboost_reg_train_RMSE, catboost_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"3.For catboost_reg the RMSLE(CV) values are \\n    train RMSE(CV) = {0}  \\n    CV RMSE(CV) = {1} \".format(catboost_reg_train_RMSECV, catboost_reg_cv_RMSECV))\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgFUlYrxqWeV"
   },
   "source": [
    "##### **4.5.2 Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQFjz2JBDqZa"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpSKRA9CD6hR"
   },
   "outputs": [],
   "source": [
    "X_train_arr = np.array(X_train)\n",
    "y_train_arr = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 611122,
     "status": "ok",
     "timestamp": 1639656067526,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "GxWACh8yDqZa",
    "outputId": "15fb4f2e-418e-44fc-ad47-b0b7ba1f1b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.7449410\ttotal: 208ms\tremaining: 8m 39s\n",
      "1:\tlearn: 1.7341488\ttotal: 412ms\tremaining: 8m 34s\n",
      "2:\tlearn: 1.7234973\ttotal: 616ms\tremaining: 8m 32s\n",
      "3:\tlearn: 1.7129519\ttotal: 821ms\tremaining: 8m 32s\n",
      "4:\tlearn: 1.7025616\ttotal: 1.02s\tremaining: 8m 31s\n",
      "5:\tlearn: 1.6923215\ttotal: 1.23s\tremaining: 8m 31s\n",
      "6:\tlearn: 1.6822465\ttotal: 1.44s\tremaining: 8m 31s\n",
      "7:\tlearn: 1.6722687\ttotal: 1.64s\tremaining: 8m 30s\n",
      "8:\tlearn: 1.6624509\ttotal: 1.84s\tremaining: 8m 29s\n",
      "9:\tlearn: 1.6528128\ttotal: 2.04s\tremaining: 8m 29s\n",
      "10:\tlearn: 1.6431848\ttotal: 2.25s\tremaining: 8m 28s\n",
      "11:\tlearn: 1.6338008\ttotal: 2.46s\tremaining: 8m 29s\n",
      "12:\tlearn: 1.6244559\ttotal: 2.66s\tremaining: 8m 28s\n",
      "13:\tlearn: 1.6153389\ttotal: 2.86s\tremaining: 8m 27s\n",
      "14:\tlearn: 1.6063394\ttotal: 3.06s\tremaining: 8m 27s\n",
      "15:\tlearn: 1.5974545\ttotal: 3.27s\tremaining: 8m 27s\n",
      "16:\tlearn: 1.5887209\ttotal: 3.47s\tremaining: 8m 27s\n",
      "17:\tlearn: 1.5799721\ttotal: 3.68s\tremaining: 8m 27s\n",
      "18:\tlearn: 1.5713747\ttotal: 3.88s\tremaining: 8m 26s\n",
      "19:\tlearn: 1.5629530\ttotal: 4.08s\tremaining: 8m 26s\n",
      "20:\tlearn: 1.5545933\ttotal: 4.29s\tremaining: 8m 26s\n",
      "21:\tlearn: 1.5462891\ttotal: 4.49s\tremaining: 8m 25s\n",
      "22:\tlearn: 1.5381790\ttotal: 4.69s\tremaining: 8m 25s\n",
      "23:\tlearn: 1.5299936\ttotal: 4.9s\tremaining: 8m 25s\n",
      "24:\tlearn: 1.5219524\ttotal: 5.11s\tremaining: 8m 25s\n",
      "25:\tlearn: 1.5140647\ttotal: 5.31s\tremaining: 8m 25s\n",
      "26:\tlearn: 1.5062451\ttotal: 5.52s\tremaining: 8m 25s\n",
      "27:\tlearn: 1.4984502\ttotal: 5.73s\tremaining: 8m 25s\n",
      "28:\tlearn: 1.4907797\ttotal: 5.93s\tremaining: 8m 25s\n",
      "29:\tlearn: 1.4833978\ttotal: 6.14s\tremaining: 8m 25s\n",
      "30:\tlearn: 1.4761355\ttotal: 6.34s\tremaining: 8m 25s\n",
      "31:\tlearn: 1.4687357\ttotal: 6.55s\tremaining: 8m 25s\n",
      "32:\tlearn: 1.4615283\ttotal: 6.76s\tremaining: 8m 25s\n",
      "33:\tlearn: 1.4543556\ttotal: 6.96s\tremaining: 8m 25s\n",
      "34:\tlearn: 1.4474520\ttotal: 7.17s\tremaining: 8m 25s\n",
      "35:\tlearn: 1.4405607\ttotal: 7.38s\tremaining: 8m 24s\n",
      "36:\tlearn: 1.4337377\ttotal: 7.58s\tremaining: 8m 24s\n",
      "37:\tlearn: 1.4270174\ttotal: 7.79s\tremaining: 8m 24s\n",
      "38:\tlearn: 1.4202703\ttotal: 7.99s\tremaining: 8m 24s\n",
      "39:\tlearn: 1.4137278\ttotal: 8.19s\tremaining: 8m 23s\n",
      "40:\tlearn: 1.4071941\ttotal: 8.4s\tremaining: 8m 23s\n",
      "41:\tlearn: 1.4007344\ttotal: 8.61s\tremaining: 8m 23s\n",
      "42:\tlearn: 1.3943327\ttotal: 8.81s\tremaining: 8m 23s\n",
      "43:\tlearn: 1.3880435\ttotal: 9.02s\tremaining: 8m 23s\n",
      "44:\tlearn: 1.3818719\ttotal: 9.23s\tremaining: 8m 23s\n",
      "45:\tlearn: 1.3758772\ttotal: 9.43s\tremaining: 8m 22s\n",
      "46:\tlearn: 1.3698362\ttotal: 9.63s\tremaining: 8m 22s\n",
      "47:\tlearn: 1.3639223\ttotal: 9.84s\tremaining: 8m 22s\n",
      "48:\tlearn: 1.3581273\ttotal: 10s\tremaining: 8m 22s\n",
      "49:\tlearn: 1.3523619\ttotal: 10.2s\tremaining: 8m 22s\n",
      "50:\tlearn: 1.3466994\ttotal: 10.5s\tremaining: 8m 21s\n",
      "51:\tlearn: 1.3411562\ttotal: 10.7s\tremaining: 8m 21s\n",
      "52:\tlearn: 1.3356671\ttotal: 10.9s\tremaining: 8m 21s\n",
      "53:\tlearn: 1.3302644\ttotal: 11.1s\tremaining: 8m 20s\n",
      "54:\tlearn: 1.3248941\ttotal: 11.3s\tremaining: 8m 20s\n",
      "55:\tlearn: 1.3195909\ttotal: 11.5s\tremaining: 8m 20s\n",
      "56:\tlearn: 1.3143411\ttotal: 11.7s\tremaining: 8m 20s\n",
      "57:\tlearn: 1.3091255\ttotal: 11.9s\tremaining: 8m 20s\n",
      "58:\tlearn: 1.3040490\ttotal: 12.1s\tremaining: 8m 19s\n",
      "59:\tlearn: 1.2988483\ttotal: 12.3s\tremaining: 8m 19s\n",
      "60:\tlearn: 1.2936878\ttotal: 12.5s\tremaining: 8m 19s\n",
      "61:\tlearn: 1.2888081\ttotal: 12.7s\tremaining: 8m 19s\n",
      "62:\tlearn: 1.2837749\ttotal: 12.9s\tremaining: 8m 19s\n",
      "63:\tlearn: 1.2788151\ttotal: 13.1s\tremaining: 8m 19s\n",
      "64:\tlearn: 1.2739255\ttotal: 13.3s\tremaining: 8m 18s\n",
      "65:\tlearn: 1.2691084\ttotal: 13.5s\tremaining: 8m 18s\n",
      "66:\tlearn: 1.2643394\ttotal: 13.7s\tremaining: 8m 18s\n",
      "67:\tlearn: 1.2595401\ttotal: 13.9s\tremaining: 8m 18s\n",
      "68:\tlearn: 1.2549027\ttotal: 14.1s\tremaining: 8m 18s\n",
      "69:\tlearn: 1.2504680\ttotal: 14.3s\tremaining: 8m 18s\n",
      "70:\tlearn: 1.2460417\ttotal: 14.6s\tremaining: 8m 17s\n",
      "71:\tlearn: 1.2415949\ttotal: 14.8s\tremaining: 8m 17s\n",
      "72:\tlearn: 1.2373129\ttotal: 15s\tremaining: 8m 17s\n",
      "73:\tlearn: 1.2329788\ttotal: 15.2s\tremaining: 8m 17s\n",
      "74:\tlearn: 1.2288519\ttotal: 15.4s\tremaining: 8m 17s\n",
      "75:\tlearn: 1.2247611\ttotal: 15.6s\tremaining: 8m 16s\n",
      "76:\tlearn: 1.2205775\ttotal: 15.8s\tremaining: 8m 16s\n",
      "77:\tlearn: 1.2165078\ttotal: 16s\tremaining: 8m 16s\n",
      "78:\tlearn: 1.2125298\ttotal: 16.2s\tremaining: 8m 16s\n",
      "79:\tlearn: 1.2086826\ttotal: 16.4s\tremaining: 8m 15s\n",
      "80:\tlearn: 1.2047449\ttotal: 16.6s\tremaining: 8m 15s\n",
      "81:\tlearn: 1.2008890\ttotal: 16.8s\tremaining: 8m 15s\n",
      "82:\tlearn: 1.1971165\ttotal: 17s\tremaining: 8m 15s\n",
      "83:\tlearn: 1.1934241\ttotal: 17.2s\tremaining: 8m 15s\n",
      "84:\tlearn: 1.1896587\ttotal: 17.4s\tremaining: 8m 14s\n",
      "85:\tlearn: 1.1860512\ttotal: 17.6s\tremaining: 8m 14s\n",
      "86:\tlearn: 1.1823580\ttotal: 17.8s\tremaining: 8m 14s\n",
      "87:\tlearn: 1.1787436\ttotal: 18s\tremaining: 8m 14s\n",
      "88:\tlearn: 1.1752394\ttotal: 18.2s\tremaining: 8m 14s\n",
      "89:\tlearn: 1.1718438\ttotal: 18.4s\tremaining: 8m 13s\n",
      "90:\tlearn: 1.1683214\ttotal: 18.6s\tremaining: 8m 13s\n",
      "91:\tlearn: 1.1649115\ttotal: 18.8s\tremaining: 8m 13s\n",
      "92:\tlearn: 1.1615432\ttotal: 19s\tremaining: 8m 12s\n",
      "93:\tlearn: 1.1582265\ttotal: 19.2s\tremaining: 8m 12s\n",
      "94:\tlearn: 1.1549245\ttotal: 19.5s\tremaining: 8m 12s\n",
      "95:\tlearn: 1.1516546\ttotal: 19.7s\tremaining: 8m 12s\n",
      "96:\tlearn: 1.1484584\ttotal: 19.9s\tremaining: 8m 12s\n",
      "97:\tlearn: 1.1452603\ttotal: 20.1s\tremaining: 8m 11s\n",
      "98:\tlearn: 1.1421766\ttotal: 20.3s\tremaining: 8m 11s\n",
      "99:\tlearn: 1.1391273\ttotal: 20.5s\tremaining: 8m 11s\n",
      "100:\tlearn: 1.1361603\ttotal: 20.7s\tremaining: 8m 11s\n",
      "101:\tlearn: 1.1331224\ttotal: 20.9s\tremaining: 8m 10s\n",
      "102:\tlearn: 1.1302659\ttotal: 21.1s\tremaining: 8m 10s\n",
      "103:\tlearn: 1.1273458\ttotal: 21.3s\tremaining: 8m 10s\n",
      "104:\tlearn: 1.1244924\ttotal: 21.5s\tremaining: 8m 10s\n",
      "105:\tlearn: 1.1216566\ttotal: 21.7s\tremaining: 8m 10s\n",
      "106:\tlearn: 1.1190247\ttotal: 21.9s\tremaining: 8m 9s\n",
      "107:\tlearn: 1.1162530\ttotal: 22.1s\tremaining: 8m 9s\n",
      "108:\tlearn: 1.1135581\ttotal: 22.3s\tremaining: 8m 9s\n",
      "109:\tlearn: 1.1108750\ttotal: 22.5s\tremaining: 8m 9s\n",
      "110:\tlearn: 1.1082401\ttotal: 22.7s\tremaining: 8m 9s\n",
      "111:\tlearn: 1.1056572\ttotal: 22.9s\tremaining: 8m 8s\n",
      "112:\tlearn: 1.1030367\ttotal: 23.1s\tremaining: 8m 8s\n",
      "113:\tlearn: 1.1005282\ttotal: 23.3s\tremaining: 8m 8s\n",
      "114:\tlearn: 1.0980543\ttotal: 23.5s\tremaining: 8m 8s\n",
      "115:\tlearn: 1.0955857\ttotal: 23.7s\tremaining: 8m 7s\n",
      "116:\tlearn: 1.0930517\ttotal: 23.9s\tremaining: 8m 7s\n",
      "117:\tlearn: 1.0906506\ttotal: 24.2s\tremaining: 8m 7s\n",
      "118:\tlearn: 1.0882633\ttotal: 24.4s\tremaining: 8m 7s\n",
      "119:\tlearn: 1.0858649\ttotal: 24.6s\tremaining: 8m 7s\n",
      "120:\tlearn: 1.0837283\ttotal: 24.8s\tremaining: 8m 6s\n",
      "121:\tlearn: 1.0814310\ttotal: 25s\tremaining: 8m 6s\n",
      "122:\tlearn: 1.0790880\ttotal: 25.3s\tremaining: 8m 8s\n",
      "123:\tlearn: 1.0767611\ttotal: 25.5s\tremaining: 8m 9s\n",
      "124:\tlearn: 1.0744768\ttotal: 25.8s\tremaining: 8m 9s\n",
      "125:\tlearn: 1.0720371\ttotal: 26s\tremaining: 8m 9s\n",
      "126:\tlearn: 1.0696340\ttotal: 26.2s\tremaining: 8m 8s\n",
      "127:\tlearn: 1.0673891\ttotal: 26.4s\tremaining: 8m 8s\n",
      "128:\tlearn: 1.0652910\ttotal: 26.6s\tremaining: 8m 8s\n",
      "129:\tlearn: 1.0632014\ttotal: 26.8s\tremaining: 8m 8s\n",
      "130:\tlearn: 1.0610702\ttotal: 27s\tremaining: 8m 8s\n",
      "131:\tlearn: 1.0589900\ttotal: 27.2s\tremaining: 8m 7s\n",
      "132:\tlearn: 1.0568224\ttotal: 27.4s\tremaining: 8m 7s\n",
      "133:\tlearn: 1.0546142\ttotal: 27.6s\tremaining: 8m 7s\n",
      "134:\tlearn: 1.0527348\ttotal: 27.8s\tremaining: 8m 6s\n",
      "135:\tlearn: 1.0507928\ttotal: 28s\tremaining: 8m 6s\n",
      "136:\tlearn: 1.0489599\ttotal: 28.2s\tremaining: 8m 6s\n",
      "137:\tlearn: 1.0470872\ttotal: 28.4s\tremaining: 8m 6s\n",
      "138:\tlearn: 1.0451328\ttotal: 28.6s\tremaining: 8m 5s\n",
      "139:\tlearn: 1.0431963\ttotal: 28.8s\tremaining: 8m 5s\n",
      "140:\tlearn: 1.0413260\ttotal: 29s\tremaining: 8m 5s\n",
      "141:\tlearn: 1.0394510\ttotal: 29.2s\tremaining: 8m 4s\n",
      "142:\tlearn: 1.0375838\ttotal: 29.4s\tremaining: 8m 4s\n",
      "143:\tlearn: 1.0356836\ttotal: 29.6s\tremaining: 8m 4s\n",
      "144:\tlearn: 1.0339720\ttotal: 29.8s\tremaining: 8m 4s\n",
      "145:\tlearn: 1.0319962\ttotal: 30s\tremaining: 8m 4s\n",
      "146:\tlearn: 1.0301610\ttotal: 30.2s\tremaining: 8m 3s\n",
      "147:\tlearn: 1.0285537\ttotal: 30.4s\tremaining: 8m 3s\n",
      "148:\tlearn: 1.0267709\ttotal: 30.6s\tremaining: 8m 3s\n",
      "149:\tlearn: 1.0252117\ttotal: 30.8s\tremaining: 8m 2s\n",
      "150:\tlearn: 1.0236264\ttotal: 31s\tremaining: 8m 2s\n",
      "151:\tlearn: 1.0219571\ttotal: 31.2s\tremaining: 8m 2s\n",
      "152:\tlearn: 1.0202354\ttotal: 31.4s\tremaining: 8m 2s\n",
      "153:\tlearn: 1.0186020\ttotal: 31.6s\tremaining: 8m 1s\n",
      "154:\tlearn: 1.0168291\ttotal: 31.8s\tremaining: 8m 1s\n",
      "155:\tlearn: 1.0152853\ttotal: 32s\tremaining: 8m 1s\n",
      "156:\tlearn: 1.0138238\ttotal: 32.2s\tremaining: 8m 1s\n",
      "157:\tlearn: 1.0121944\ttotal: 32.4s\tremaining: 8m\n",
      "158:\tlearn: 1.0107331\ttotal: 32.6s\tremaining: 8m\n",
      "159:\tlearn: 1.0092497\ttotal: 32.9s\tremaining: 8m\n",
      "160:\tlearn: 1.0076946\ttotal: 33s\tremaining: 8m\n",
      "161:\tlearn: 1.0062183\ttotal: 33.3s\tremaining: 7m 59s\n",
      "162:\tlearn: 1.0047477\ttotal: 33.4s\tremaining: 7m 59s\n",
      "163:\tlearn: 1.0032590\ttotal: 33.6s\tremaining: 7m 59s\n",
      "164:\tlearn: 1.0018059\ttotal: 33.9s\tremaining: 7m 59s\n",
      "165:\tlearn: 1.0003742\ttotal: 34.1s\tremaining: 7m 58s\n",
      "166:\tlearn: 0.9989244\ttotal: 34.3s\tremaining: 7m 58s\n",
      "167:\tlearn: 0.9976164\ttotal: 34.5s\tremaining: 7m 58s\n",
      "168:\tlearn: 0.9962162\ttotal: 34.7s\tremaining: 7m 58s\n",
      "169:\tlearn: 0.9948559\ttotal: 34.9s\tremaining: 7m 57s\n",
      "170:\tlearn: 0.9935200\ttotal: 35.1s\tremaining: 7m 57s\n",
      "171:\tlearn: 0.9920749\ttotal: 35.3s\tremaining: 7m 57s\n",
      "172:\tlearn: 0.9907966\ttotal: 35.5s\tremaining: 7m 56s\n",
      "173:\tlearn: 0.9895512\ttotal: 35.6s\tremaining: 7m 56s\n",
      "174:\tlearn: 0.9882054\ttotal: 35.9s\tremaining: 7m 56s\n",
      "175:\tlearn: 0.9868428\ttotal: 36.1s\tremaining: 7m 56s\n",
      "176:\tlearn: 0.9856208\ttotal: 36.3s\tremaining: 7m 55s\n",
      "177:\tlearn: 0.9845270\ttotal: 36.5s\tremaining: 7m 55s\n",
      "178:\tlearn: 0.9833189\ttotal: 36.7s\tremaining: 7m 55s\n",
      "179:\tlearn: 0.9819602\ttotal: 36.9s\tremaining: 7m 54s\n",
      "180:\tlearn: 0.9808358\ttotal: 37s\tremaining: 7m 54s\n",
      "181:\tlearn: 0.9796305\ttotal: 37.2s\tremaining: 7m 54s\n",
      "182:\tlearn: 0.9783823\ttotal: 37.5s\tremaining: 7m 54s\n",
      "183:\tlearn: 0.9771582\ttotal: 37.6s\tremaining: 7m 53s\n",
      "184:\tlearn: 0.9758992\ttotal: 37.9s\tremaining: 7m 53s\n",
      "185:\tlearn: 0.9747511\ttotal: 38.1s\tremaining: 7m 53s\n",
      "186:\tlearn: 0.9737091\ttotal: 38.3s\tremaining: 7m 53s\n",
      "187:\tlearn: 0.9725217\ttotal: 38.5s\tremaining: 7m 52s\n",
      "188:\tlearn: 0.9713192\ttotal: 38.7s\tremaining: 7m 52s\n",
      "189:\tlearn: 0.9701265\ttotal: 38.9s\tremaining: 7m 52s\n",
      "190:\tlearn: 0.9689769\ttotal: 39.1s\tremaining: 7m 52s\n",
      "191:\tlearn: 0.9679545\ttotal: 39.2s\tremaining: 7m 51s\n",
      "192:\tlearn: 0.9668093\ttotal: 39.5s\tremaining: 7m 51s\n",
      "193:\tlearn: 0.9658204\ttotal: 39.7s\tremaining: 7m 51s\n",
      "194:\tlearn: 0.9646771\ttotal: 39.9s\tremaining: 7m 51s\n",
      "195:\tlearn: 0.9634790\ttotal: 40.1s\tremaining: 7m 50s\n",
      "196:\tlearn: 0.9624780\ttotal: 40.3s\tremaining: 7m 50s\n",
      "197:\tlearn: 0.9613065\ttotal: 40.5s\tremaining: 7m 50s\n",
      "198:\tlearn: 0.9602595\ttotal: 40.7s\tremaining: 7m 50s\n",
      "199:\tlearn: 0.9593439\ttotal: 40.9s\tremaining: 7m 50s\n",
      "200:\tlearn: 0.9582067\ttotal: 41.1s\tremaining: 7m 49s\n",
      "201:\tlearn: 0.9571422\ttotal: 41.3s\tremaining: 7m 49s\n",
      "202:\tlearn: 0.9562073\ttotal: 41.5s\tremaining: 7m 49s\n",
      "203:\tlearn: 0.9552060\ttotal: 41.7s\tremaining: 7m 49s\n",
      "204:\tlearn: 0.9541795\ttotal: 41.9s\tremaining: 7m 48s\n",
      "205:\tlearn: 0.9531548\ttotal: 42.1s\tremaining: 7m 48s\n",
      "206:\tlearn: 0.9521930\ttotal: 42.3s\tremaining: 7m 48s\n",
      "207:\tlearn: 0.9511098\ttotal: 42.5s\tremaining: 7m 48s\n",
      "208:\tlearn: 0.9502535\ttotal: 42.7s\tremaining: 7m 47s\n",
      "209:\tlearn: 0.9492442\ttotal: 42.9s\tremaining: 7m 47s\n",
      "210:\tlearn: 0.9483047\ttotal: 43.1s\tremaining: 7m 47s\n",
      "211:\tlearn: 0.9473252\ttotal: 43.3s\tremaining: 7m 47s\n",
      "212:\tlearn: 0.9464198\ttotal: 43.5s\tremaining: 7m 46s\n",
      "213:\tlearn: 0.9454633\ttotal: 43.7s\tremaining: 7m 46s\n",
      "214:\tlearn: 0.9447033\ttotal: 43.9s\tremaining: 7m 46s\n",
      "215:\tlearn: 0.9438037\ttotal: 44.1s\tremaining: 7m 46s\n",
      "216:\tlearn: 0.9428973\ttotal: 44.3s\tremaining: 7m 45s\n",
      "217:\tlearn: 0.9419429\ttotal: 44.5s\tremaining: 7m 45s\n",
      "218:\tlearn: 0.9411513\ttotal: 44.7s\tremaining: 7m 45s\n",
      "219:\tlearn: 0.9402467\ttotal: 44.9s\tremaining: 7m 45s\n",
      "220:\tlearn: 0.9394488\ttotal: 45.1s\tremaining: 7m 44s\n",
      "221:\tlearn: 0.9385682\ttotal: 45.3s\tremaining: 7m 44s\n",
      "222:\tlearn: 0.9376435\ttotal: 45.5s\tremaining: 7m 44s\n",
      "223:\tlearn: 0.9368380\ttotal: 45.7s\tremaining: 7m 44s\n",
      "224:\tlearn: 0.9360003\ttotal: 45.9s\tremaining: 7m 43s\n",
      "225:\tlearn: 0.9353274\ttotal: 46.1s\tremaining: 7m 43s\n",
      "226:\tlearn: 0.9343695\ttotal: 46.3s\tremaining: 7m 43s\n",
      "227:\tlearn: 0.9335043\ttotal: 46.5s\tremaining: 7m 43s\n",
      "228:\tlearn: 0.9325440\ttotal: 46.7s\tremaining: 7m 43s\n",
      "229:\tlearn: 0.9317188\ttotal: 46.9s\tremaining: 7m 42s\n",
      "230:\tlearn: 0.9309655\ttotal: 47.1s\tremaining: 7m 42s\n",
      "231:\tlearn: 0.9302068\ttotal: 47.3s\tremaining: 7m 42s\n",
      "232:\tlearn: 0.9294849\ttotal: 47.5s\tremaining: 7m 41s\n",
      "233:\tlearn: 0.9285619\ttotal: 47.7s\tremaining: 7m 41s\n",
      "234:\tlearn: 0.9278450\ttotal: 47.9s\tremaining: 7m 41s\n",
      "235:\tlearn: 0.9272305\ttotal: 48.1s\tremaining: 7m 41s\n",
      "236:\tlearn: 0.9265552\ttotal: 48.3s\tremaining: 7m 41s\n",
      "237:\tlearn: 0.9258587\ttotal: 48.5s\tremaining: 7m 40s\n",
      "238:\tlearn: 0.9251629\ttotal: 48.7s\tremaining: 7m 40s\n",
      "239:\tlearn: 0.9245626\ttotal: 48.9s\tremaining: 7m 40s\n",
      "240:\tlearn: 0.9238632\ttotal: 49.1s\tremaining: 7m 39s\n",
      "241:\tlearn: 0.9231986\ttotal: 49.3s\tremaining: 7m 39s\n",
      "242:\tlearn: 0.9224387\ttotal: 49.5s\tremaining: 7m 39s\n",
      "243:\tlearn: 0.9217101\ttotal: 49.7s\tremaining: 7m 39s\n",
      "244:\tlearn: 0.9210169\ttotal: 49.9s\tremaining: 7m 39s\n",
      "245:\tlearn: 0.9202589\ttotal: 50.1s\tremaining: 7m 38s\n",
      "246:\tlearn: 0.9195174\ttotal: 50.3s\tremaining: 7m 38s\n",
      "247:\tlearn: 0.9187434\ttotal: 50.5s\tremaining: 7m 38s\n",
      "248:\tlearn: 0.9179798\ttotal: 50.7s\tremaining: 7m 38s\n",
      "249:\tlearn: 0.9171497\ttotal: 50.9s\tremaining: 7m 38s\n",
      "250:\tlearn: 0.9165366\ttotal: 51.1s\tremaining: 7m 38s\n",
      "251:\tlearn: 0.9158723\ttotal: 51.3s\tremaining: 7m 37s\n",
      "252:\tlearn: 0.9149766\ttotal: 51.5s\tremaining: 7m 37s\n",
      "253:\tlearn: 0.9141789\ttotal: 51.8s\tremaining: 7m 37s\n",
      "254:\tlearn: 0.9134735\ttotal: 51.9s\tremaining: 7m 37s\n",
      "255:\tlearn: 0.9128546\ttotal: 52.1s\tremaining: 7m 37s\n",
      "256:\tlearn: 0.9120318\ttotal: 52.4s\tremaining: 7m 37s\n",
      "257:\tlearn: 0.9114453\ttotal: 52.6s\tremaining: 7m 36s\n",
      "258:\tlearn: 0.9106679\ttotal: 52.8s\tremaining: 7m 36s\n",
      "259:\tlearn: 0.9100669\ttotal: 53s\tremaining: 7m 36s\n",
      "260:\tlearn: 0.9093341\ttotal: 53.2s\tremaining: 7m 36s\n",
      "261:\tlearn: 0.9085854\ttotal: 53.4s\tremaining: 7m 36s\n",
      "262:\tlearn: 0.9079280\ttotal: 53.6s\tremaining: 7m 36s\n",
      "263:\tlearn: 0.9073273\ttotal: 53.8s\tremaining: 7m 35s\n",
      "264:\tlearn: 0.9065488\ttotal: 54s\tremaining: 7m 35s\n",
      "265:\tlearn: 0.9058717\ttotal: 54.2s\tremaining: 7m 35s\n",
      "266:\tlearn: 0.9051171\ttotal: 54.4s\tremaining: 7m 34s\n",
      "267:\tlearn: 0.9042157\ttotal: 54.6s\tremaining: 7m 34s\n",
      "268:\tlearn: 0.9035711\ttotal: 54.8s\tremaining: 7m 34s\n",
      "269:\tlearn: 0.9028839\ttotal: 55s\tremaining: 7m 34s\n",
      "270:\tlearn: 0.9023741\ttotal: 55.2s\tremaining: 7m 34s\n",
      "271:\tlearn: 0.9015535\ttotal: 55.4s\tremaining: 7m 33s\n",
      "272:\tlearn: 0.9010501\ttotal: 55.6s\tremaining: 7m 33s\n",
      "273:\tlearn: 0.9002496\ttotal: 55.8s\tremaining: 7m 33s\n",
      "274:\tlearn: 0.8997510\ttotal: 56s\tremaining: 7m 33s\n",
      "275:\tlearn: 0.8991291\ttotal: 56.2s\tremaining: 7m 33s\n",
      "276:\tlearn: 0.8983491\ttotal: 56.4s\tremaining: 7m 32s\n",
      "277:\tlearn: 0.8978359\ttotal: 56.6s\tremaining: 7m 32s\n",
      "278:\tlearn: 0.8971092\ttotal: 56.8s\tremaining: 7m 32s\n",
      "279:\tlearn: 0.8965088\ttotal: 57.1s\tremaining: 7m 32s\n",
      "280:\tlearn: 0.8958908\ttotal: 57.2s\tremaining: 7m 32s\n",
      "281:\tlearn: 0.8951175\ttotal: 57.5s\tremaining: 7m 31s\n",
      "282:\tlearn: 0.8945306\ttotal: 57.7s\tremaining: 7m 31s\n",
      "283:\tlearn: 0.8939139\ttotal: 57.9s\tremaining: 7m 31s\n",
      "284:\tlearn: 0.8933924\ttotal: 58.1s\tremaining: 7m 31s\n",
      "285:\tlearn: 0.8928372\ttotal: 58.3s\tremaining: 7m 31s\n",
      "286:\tlearn: 0.8922652\ttotal: 58.5s\tremaining: 7m 30s\n",
      "287:\tlearn: 0.8917892\ttotal: 58.8s\tremaining: 7m 31s\n",
      "288:\tlearn: 0.8912737\ttotal: 59.1s\tremaining: 7m 32s\n",
      "289:\tlearn: 0.8907233\ttotal: 59.3s\tremaining: 7m 32s\n",
      "290:\tlearn: 0.8902308\ttotal: 59.5s\tremaining: 7m 31s\n",
      "291:\tlearn: 0.8896756\ttotal: 59.7s\tremaining: 7m 31s\n",
      "292:\tlearn: 0.8891915\ttotal: 59.9s\tremaining: 7m 31s\n",
      "293:\tlearn: 0.8885266\ttotal: 1m\tremaining: 7m 31s\n",
      "294:\tlearn: 0.8880838\ttotal: 1m\tremaining: 7m 31s\n",
      "295:\tlearn: 0.8875365\ttotal: 1m\tremaining: 7m 31s\n",
      "296:\tlearn: 0.8869724\ttotal: 1m\tremaining: 7m 31s\n",
      "297:\tlearn: 0.8865038\ttotal: 1m 1s\tremaining: 7m 31s\n",
      "298:\tlearn: 0.8860160\ttotal: 1m 1s\tremaining: 7m 30s\n",
      "299:\tlearn: 0.8854758\ttotal: 1m 1s\tremaining: 7m 30s\n",
      "300:\tlearn: 0.8849665\ttotal: 1m 1s\tremaining: 7m 30s\n",
      "301:\tlearn: 0.8844508\ttotal: 1m 1s\tremaining: 7m 30s\n",
      "302:\tlearn: 0.8838627\ttotal: 1m 2s\tremaining: 7m 30s\n",
      "303:\tlearn: 0.8831862\ttotal: 1m 2s\tremaining: 7m 30s\n",
      "304:\tlearn: 0.8826875\ttotal: 1m 2s\tremaining: 7m 30s\n",
      "305:\tlearn: 0.8821109\ttotal: 1m 2s\tremaining: 7m 30s\n",
      "306:\tlearn: 0.8816681\ttotal: 1m 2s\tremaining: 7m 30s\n",
      "307:\tlearn: 0.8810998\ttotal: 1m 3s\tremaining: 7m 29s\n",
      "308:\tlearn: 0.8805667\ttotal: 1m 3s\tremaining: 7m 29s\n",
      "309:\tlearn: 0.8799850\ttotal: 1m 3s\tremaining: 7m 29s\n",
      "310:\tlearn: 0.8794360\ttotal: 1m 3s\tremaining: 7m 29s\n",
      "311:\tlearn: 0.8788864\ttotal: 1m 4s\tremaining: 7m 29s\n",
      "312:\tlearn: 0.8782258\ttotal: 1m 4s\tremaining: 7m 29s\n",
      "313:\tlearn: 0.8776567\ttotal: 1m 4s\tremaining: 7m 28s\n",
      "314:\tlearn: 0.8772265\ttotal: 1m 4s\tremaining: 7m 28s\n",
      "315:\tlearn: 0.8767130\ttotal: 1m 4s\tremaining: 7m 28s\n",
      "316:\tlearn: 0.8761083\ttotal: 1m 5s\tremaining: 7m 28s\n",
      "317:\tlearn: 0.8757724\ttotal: 1m 5s\tremaining: 7m 27s\n",
      "318:\tlearn: 0.8752562\ttotal: 1m 5s\tremaining: 7m 27s\n",
      "319:\tlearn: 0.8748412\ttotal: 1m 5s\tremaining: 7m 27s\n",
      "320:\tlearn: 0.8742980\ttotal: 1m 5s\tremaining: 7m 27s\n",
      "321:\tlearn: 0.8738649\ttotal: 1m 6s\tremaining: 7m 27s\n",
      "322:\tlearn: 0.8733851\ttotal: 1m 6s\tremaining: 7m 26s\n",
      "323:\tlearn: 0.8729000\ttotal: 1m 6s\tremaining: 7m 26s\n",
      "324:\tlearn: 0.8723717\ttotal: 1m 6s\tremaining: 7m 26s\n",
      "325:\tlearn: 0.8718383\ttotal: 1m 6s\tremaining: 7m 26s\n",
      "326:\tlearn: 0.8713336\ttotal: 1m 7s\tremaining: 7m 25s\n",
      "327:\tlearn: 0.8708006\ttotal: 1m 7s\tremaining: 7m 25s\n",
      "328:\tlearn: 0.8700898\ttotal: 1m 7s\tremaining: 7m 25s\n",
      "329:\tlearn: 0.8697185\ttotal: 1m 7s\tremaining: 7m 25s\n",
      "330:\tlearn: 0.8693044\ttotal: 1m 7s\tremaining: 7m 25s\n",
      "331:\tlearn: 0.8688650\ttotal: 1m 8s\tremaining: 7m 24s\n",
      "332:\tlearn: 0.8683810\ttotal: 1m 8s\tremaining: 7m 24s\n",
      "333:\tlearn: 0.8679804\ttotal: 1m 8s\tremaining: 7m 24s\n",
      "334:\tlearn: 0.8673918\ttotal: 1m 8s\tremaining: 7m 24s\n",
      "335:\tlearn: 0.8670395\ttotal: 1m 8s\tremaining: 7m 23s\n",
      "336:\tlearn: 0.8666037\ttotal: 1m 9s\tremaining: 7m 23s\n",
      "337:\tlearn: 0.8661259\ttotal: 1m 9s\tremaining: 7m 23s\n",
      "338:\tlearn: 0.8655585\ttotal: 1m 9s\tremaining: 7m 23s\n",
      "339:\tlearn: 0.8651061\ttotal: 1m 9s\tremaining: 7m 22s\n",
      "340:\tlearn: 0.8646697\ttotal: 1m 9s\tremaining: 7m 22s\n",
      "341:\tlearn: 0.8641196\ttotal: 1m 10s\tremaining: 7m 22s\n",
      "342:\tlearn: 0.8636789\ttotal: 1m 10s\tremaining: 7m 22s\n",
      "343:\tlearn: 0.8632842\ttotal: 1m 10s\tremaining: 7m 22s\n",
      "344:\tlearn: 0.8627789\ttotal: 1m 10s\tremaining: 7m 21s\n",
      "345:\tlearn: 0.8622866\ttotal: 1m 10s\tremaining: 7m 21s\n",
      "346:\tlearn: 0.8618471\ttotal: 1m 11s\tremaining: 7m 21s\n",
      "347:\tlearn: 0.8614878\ttotal: 1m 11s\tremaining: 7m 21s\n",
      "348:\tlearn: 0.8610186\ttotal: 1m 11s\tremaining: 7m 20s\n",
      "349:\tlearn: 0.8604459\ttotal: 1m 11s\tremaining: 7m 20s\n",
      "350:\tlearn: 0.8599145\ttotal: 1m 11s\tremaining: 7m 20s\n",
      "351:\tlearn: 0.8594211\ttotal: 1m 12s\tremaining: 7m 20s\n",
      "352:\tlearn: 0.8590968\ttotal: 1m 12s\tremaining: 7m 20s\n",
      "353:\tlearn: 0.8587863\ttotal: 1m 12s\tremaining: 7m 19s\n",
      "354:\tlearn: 0.8583006\ttotal: 1m 12s\tremaining: 7m 19s\n",
      "355:\tlearn: 0.8578761\ttotal: 1m 12s\tremaining: 7m 19s\n",
      "356:\tlearn: 0.8575680\ttotal: 1m 13s\tremaining: 7m 19s\n",
      "357:\tlearn: 0.8569127\ttotal: 1m 13s\tremaining: 7m 19s\n",
      "358:\tlearn: 0.8565840\ttotal: 1m 13s\tremaining: 7m 18s\n",
      "359:\tlearn: 0.8560753\ttotal: 1m 13s\tremaining: 7m 18s\n",
      "360:\tlearn: 0.8556292\ttotal: 1m 13s\tremaining: 7m 18s\n",
      "361:\tlearn: 0.8551164\ttotal: 1m 14s\tremaining: 7m 18s\n",
      "362:\tlearn: 0.8546773\ttotal: 1m 14s\tremaining: 7m 17s\n",
      "363:\tlearn: 0.8541190\ttotal: 1m 14s\tremaining: 7m 17s\n",
      "364:\tlearn: 0.8537795\ttotal: 1m 14s\tremaining: 7m 17s\n",
      "365:\tlearn: 0.8533224\ttotal: 1m 15s\tremaining: 7m 17s\n",
      "366:\tlearn: 0.8528260\ttotal: 1m 15s\tremaining: 7m 17s\n",
      "367:\tlearn: 0.8523978\ttotal: 1m 15s\tremaining: 7m 17s\n",
      "368:\tlearn: 0.8519139\ttotal: 1m 15s\tremaining: 7m 16s\n",
      "369:\tlearn: 0.8515402\ttotal: 1m 15s\tremaining: 7m 16s\n",
      "370:\tlearn: 0.8511176\ttotal: 1m 16s\tremaining: 7m 16s\n",
      "371:\tlearn: 0.8506479\ttotal: 1m 16s\tremaining: 7m 16s\n",
      "372:\tlearn: 0.8502555\ttotal: 1m 16s\tremaining: 7m 16s\n",
      "373:\tlearn: 0.8498291\ttotal: 1m 16s\tremaining: 7m 15s\n",
      "374:\tlearn: 0.8493669\ttotal: 1m 16s\tremaining: 7m 15s\n",
      "375:\tlearn: 0.8487624\ttotal: 1m 17s\tremaining: 7m 15s\n",
      "376:\tlearn: 0.8483058\ttotal: 1m 17s\tremaining: 7m 15s\n",
      "377:\tlearn: 0.8477241\ttotal: 1m 17s\tremaining: 7m 15s\n",
      "378:\tlearn: 0.8473505\ttotal: 1m 17s\tremaining: 7m 14s\n",
      "379:\tlearn: 0.8469340\ttotal: 1m 17s\tremaining: 7m 14s\n",
      "380:\tlearn: 0.8465299\ttotal: 1m 18s\tremaining: 7m 14s\n",
      "381:\tlearn: 0.8460289\ttotal: 1m 18s\tremaining: 7m 14s\n",
      "382:\tlearn: 0.8456551\ttotal: 1m 18s\tremaining: 7m 14s\n",
      "383:\tlearn: 0.8452601\ttotal: 1m 18s\tremaining: 7m 14s\n",
      "384:\tlearn: 0.8448626\ttotal: 1m 18s\tremaining: 7m 13s\n",
      "385:\tlearn: 0.8445505\ttotal: 1m 19s\tremaining: 7m 13s\n",
      "386:\tlearn: 0.8442122\ttotal: 1m 19s\tremaining: 7m 13s\n",
      "387:\tlearn: 0.8436484\ttotal: 1m 19s\tremaining: 7m 13s\n",
      "388:\tlearn: 0.8432145\ttotal: 1m 19s\tremaining: 7m 13s\n",
      "389:\tlearn: 0.8426390\ttotal: 1m 20s\tremaining: 7m 13s\n",
      "390:\tlearn: 0.8422854\ttotal: 1m 20s\tremaining: 7m 12s\n",
      "391:\tlearn: 0.8418601\ttotal: 1m 20s\tremaining: 7m 12s\n",
      "392:\tlearn: 0.8414978\ttotal: 1m 20s\tremaining: 7m 12s\n",
      "393:\tlearn: 0.8411169\ttotal: 1m 20s\tremaining: 7m 12s\n",
      "394:\tlearn: 0.8408400\ttotal: 1m 21s\tremaining: 7m 12s\n",
      "395:\tlearn: 0.8405713\ttotal: 1m 21s\tremaining: 7m 11s\n",
      "396:\tlearn: 0.8402534\ttotal: 1m 21s\tremaining: 7m 11s\n",
      "397:\tlearn: 0.8397701\ttotal: 1m 21s\tremaining: 7m 11s\n",
      "398:\tlearn: 0.8392929\ttotal: 1m 21s\tremaining: 7m 11s\n",
      "399:\tlearn: 0.8387662\ttotal: 1m 22s\tremaining: 7m 11s\n",
      "400:\tlearn: 0.8382379\ttotal: 1m 22s\tremaining: 7m 10s\n",
      "401:\tlearn: 0.8378463\ttotal: 1m 22s\tremaining: 7m 10s\n",
      "402:\tlearn: 0.8373314\ttotal: 1m 22s\tremaining: 7m 10s\n",
      "403:\tlearn: 0.8368389\ttotal: 1m 22s\tremaining: 7m 10s\n",
      "404:\tlearn: 0.8364033\ttotal: 1m 23s\tremaining: 7m 10s\n",
      "405:\tlearn: 0.8360727\ttotal: 1m 23s\tremaining: 7m 10s\n",
      "406:\tlearn: 0.8357722\ttotal: 1m 23s\tremaining: 7m 9s\n",
      "407:\tlearn: 0.8354758\ttotal: 1m 23s\tremaining: 7m 9s\n",
      "408:\tlearn: 0.8352094\ttotal: 1m 24s\tremaining: 7m 9s\n",
      "409:\tlearn: 0.8346689\ttotal: 1m 24s\tremaining: 7m 9s\n",
      "410:\tlearn: 0.8343882\ttotal: 1m 24s\tremaining: 7m 9s\n",
      "411:\tlearn: 0.8339327\ttotal: 1m 24s\tremaining: 7m 9s\n",
      "412:\tlearn: 0.8334871\ttotal: 1m 24s\tremaining: 7m 8s\n",
      "413:\tlearn: 0.8329921\ttotal: 1m 25s\tremaining: 7m 8s\n",
      "414:\tlearn: 0.8327355\ttotal: 1m 25s\tremaining: 7m 8s\n",
      "415:\tlearn: 0.8324237\ttotal: 1m 25s\tremaining: 7m 8s\n",
      "416:\tlearn: 0.8319673\ttotal: 1m 25s\tremaining: 7m 8s\n",
      "417:\tlearn: 0.8316517\ttotal: 1m 25s\tremaining: 7m 7s\n",
      "418:\tlearn: 0.8314142\ttotal: 1m 26s\tremaining: 7m 7s\n",
      "419:\tlearn: 0.8311313\ttotal: 1m 26s\tremaining: 7m 7s\n",
      "420:\tlearn: 0.8307248\ttotal: 1m 26s\tremaining: 7m 7s\n",
      "421:\tlearn: 0.8304783\ttotal: 1m 26s\tremaining: 7m 7s\n",
      "422:\tlearn: 0.8302069\ttotal: 1m 26s\tremaining: 7m 6s\n",
      "423:\tlearn: 0.8299317\ttotal: 1m 27s\tremaining: 7m 6s\n",
      "424:\tlearn: 0.8295056\ttotal: 1m 27s\tremaining: 7m 6s\n",
      "425:\tlearn: 0.8290854\ttotal: 1m 27s\tremaining: 7m 6s\n",
      "426:\tlearn: 0.8288400\ttotal: 1m 27s\tremaining: 7m 6s\n",
      "427:\tlearn: 0.8283928\ttotal: 1m 28s\tremaining: 7m 6s\n",
      "428:\tlearn: 0.8280488\ttotal: 1m 28s\tremaining: 7m 6s\n",
      "429:\tlearn: 0.8277303\ttotal: 1m 28s\tremaining: 7m 5s\n",
      "430:\tlearn: 0.8274415\ttotal: 1m 28s\tremaining: 7m 5s\n",
      "431:\tlearn: 0.8271263\ttotal: 1m 28s\tremaining: 7m 5s\n",
      "432:\tlearn: 0.8268576\ttotal: 1m 29s\tremaining: 7m 5s\n",
      "433:\tlearn: 0.8266240\ttotal: 1m 29s\tremaining: 7m 4s\n",
      "434:\tlearn: 0.8263543\ttotal: 1m 29s\tremaining: 7m 4s\n",
      "435:\tlearn: 0.8259474\ttotal: 1m 29s\tremaining: 7m 4s\n",
      "436:\tlearn: 0.8254834\ttotal: 1m 29s\tremaining: 7m 4s\n",
      "437:\tlearn: 0.8251310\ttotal: 1m 30s\tremaining: 7m 4s\n",
      "438:\tlearn: 0.8248891\ttotal: 1m 30s\tremaining: 7m 4s\n",
      "439:\tlearn: 0.8246077\ttotal: 1m 30s\tremaining: 7m 3s\n",
      "440:\tlearn: 0.8242643\ttotal: 1m 30s\tremaining: 7m 3s\n",
      "441:\tlearn: 0.8236674\ttotal: 1m 30s\tremaining: 7m 3s\n",
      "442:\tlearn: 0.8234258\ttotal: 1m 31s\tremaining: 7m 3s\n",
      "443:\tlearn: 0.8230498\ttotal: 1m 31s\tremaining: 7m 3s\n",
      "444:\tlearn: 0.8227611\ttotal: 1m 31s\tremaining: 7m 2s\n",
      "445:\tlearn: 0.8222918\ttotal: 1m 31s\tremaining: 7m 2s\n",
      "446:\tlearn: 0.8220709\ttotal: 1m 31s\tremaining: 7m 2s\n",
      "447:\tlearn: 0.8217934\ttotal: 1m 32s\tremaining: 7m 2s\n",
      "448:\tlearn: 0.8215465\ttotal: 1m 32s\tremaining: 7m 1s\n",
      "449:\tlearn: 0.8212528\ttotal: 1m 32s\tremaining: 7m 1s\n",
      "450:\tlearn: 0.8207927\ttotal: 1m 32s\tremaining: 7m 1s\n",
      "451:\tlearn: 0.8205628\ttotal: 1m 33s\tremaining: 7m 1s\n",
      "452:\tlearn: 0.8201477\ttotal: 1m 33s\tremaining: 7m 1s\n",
      "453:\tlearn: 0.8197907\ttotal: 1m 33s\tremaining: 7m\n",
      "454:\tlearn: 0.8194318\ttotal: 1m 33s\tremaining: 7m\n",
      "455:\tlearn: 0.8190980\ttotal: 1m 33s\tremaining: 7m\n",
      "456:\tlearn: 0.8187918\ttotal: 1m 34s\tremaining: 7m\n",
      "457:\tlearn: 0.8184829\ttotal: 1m 34s\tremaining: 7m\n",
      "458:\tlearn: 0.8181750\ttotal: 1m 34s\tremaining: 6m 59s\n",
      "459:\tlearn: 0.8177989\ttotal: 1m 34s\tremaining: 6m 59s\n",
      "460:\tlearn: 0.8173942\ttotal: 1m 34s\tremaining: 6m 59s\n",
      "461:\tlearn: 0.8169775\ttotal: 1m 35s\tremaining: 6m 59s\n",
      "462:\tlearn: 0.8166634\ttotal: 1m 35s\tremaining: 6m 59s\n",
      "463:\tlearn: 0.8162197\ttotal: 1m 35s\tremaining: 6m 59s\n",
      "464:\tlearn: 0.8160108\ttotal: 1m 35s\tremaining: 6m 58s\n",
      "465:\tlearn: 0.8157453\ttotal: 1m 35s\tremaining: 6m 58s\n",
      "466:\tlearn: 0.8154202\ttotal: 1m 36s\tremaining: 6m 58s\n",
      "467:\tlearn: 0.8150206\ttotal: 1m 36s\tremaining: 6m 58s\n",
      "468:\tlearn: 0.8147553\ttotal: 1m 36s\tremaining: 6m 58s\n",
      "469:\tlearn: 0.8144602\ttotal: 1m 36s\tremaining: 6m 58s\n",
      "470:\tlearn: 0.8141956\ttotal: 1m 37s\tremaining: 6m 57s\n",
      "471:\tlearn: 0.8138736\ttotal: 1m 37s\tremaining: 6m 57s\n",
      "472:\tlearn: 0.8135654\ttotal: 1m 37s\tremaining: 6m 57s\n",
      "473:\tlearn: 0.8130856\ttotal: 1m 37s\tremaining: 6m 57s\n",
      "474:\tlearn: 0.8126925\ttotal: 1m 37s\tremaining: 6m 57s\n",
      "475:\tlearn: 0.8124531\ttotal: 1m 38s\tremaining: 6m 57s\n",
      "476:\tlearn: 0.8121066\ttotal: 1m 38s\tremaining: 6m 56s\n",
      "477:\tlearn: 0.8118501\ttotal: 1m 38s\tremaining: 6m 56s\n",
      "478:\tlearn: 0.8115188\ttotal: 1m 38s\tremaining: 6m 56s\n",
      "479:\tlearn: 0.8111953\ttotal: 1m 38s\tremaining: 6m 56s\n",
      "480:\tlearn: 0.8108906\ttotal: 1m 39s\tremaining: 6m 55s\n",
      "481:\tlearn: 0.8105077\ttotal: 1m 39s\tremaining: 6m 55s\n",
      "482:\tlearn: 0.8102054\ttotal: 1m 39s\tremaining: 6m 55s\n",
      "483:\tlearn: 0.8098996\ttotal: 1m 39s\tremaining: 6m 55s\n",
      "484:\tlearn: 0.8095215\ttotal: 1m 39s\tremaining: 6m 55s\n",
      "485:\tlearn: 0.8091847\ttotal: 1m 40s\tremaining: 6m 54s\n",
      "486:\tlearn: 0.8088952\ttotal: 1m 40s\tremaining: 6m 54s\n",
      "487:\tlearn: 0.8086434\ttotal: 1m 40s\tremaining: 6m 54s\n",
      "488:\tlearn: 0.8083579\ttotal: 1m 40s\tremaining: 6m 54s\n",
      "489:\tlearn: 0.8080926\ttotal: 1m 40s\tremaining: 6m 54s\n",
      "490:\tlearn: 0.8077187\ttotal: 1m 41s\tremaining: 6m 53s\n",
      "491:\tlearn: 0.8074178\ttotal: 1m 41s\tremaining: 6m 53s\n",
      "492:\tlearn: 0.8070816\ttotal: 1m 41s\tremaining: 6m 53s\n",
      "493:\tlearn: 0.8068139\ttotal: 1m 41s\tremaining: 6m 53s\n",
      "494:\tlearn: 0.8064876\ttotal: 1m 41s\tremaining: 6m 53s\n",
      "495:\tlearn: 0.8062194\ttotal: 1m 42s\tremaining: 6m 52s\n",
      "496:\tlearn: 0.8059236\ttotal: 1m 42s\tremaining: 6m 52s\n",
      "497:\tlearn: 0.8055897\ttotal: 1m 42s\tremaining: 6m 52s\n",
      "498:\tlearn: 0.8053574\ttotal: 1m 42s\tremaining: 6m 52s\n",
      "499:\tlearn: 0.8049949\ttotal: 1m 43s\tremaining: 6m 52s\n",
      "500:\tlearn: 0.8047830\ttotal: 1m 43s\tremaining: 6m 51s\n",
      "501:\tlearn: 0.8045839\ttotal: 1m 43s\tremaining: 6m 51s\n",
      "502:\tlearn: 0.8043363\ttotal: 1m 43s\tremaining: 6m 51s\n",
      "503:\tlearn: 0.8040084\ttotal: 1m 43s\tremaining: 6m 51s\n",
      "504:\tlearn: 0.8037534\ttotal: 1m 44s\tremaining: 6m 50s\n",
      "505:\tlearn: 0.8035486\ttotal: 1m 44s\tremaining: 6m 50s\n",
      "506:\tlearn: 0.8033469\ttotal: 1m 44s\tremaining: 6m 50s\n",
      "507:\tlearn: 0.8031145\ttotal: 1m 44s\tremaining: 6m 50s\n",
      "508:\tlearn: 0.8028865\ttotal: 1m 44s\tremaining: 6m 49s\n",
      "509:\tlearn: 0.8026143\ttotal: 1m 45s\tremaining: 6m 49s\n",
      "510:\tlearn: 0.8023701\ttotal: 1m 45s\tremaining: 6m 49s\n",
      "511:\tlearn: 0.8019934\ttotal: 1m 45s\tremaining: 6m 49s\n",
      "512:\tlearn: 0.8017591\ttotal: 1m 45s\tremaining: 6m 49s\n",
      "513:\tlearn: 0.8014459\ttotal: 1m 45s\tremaining: 6m 49s\n",
      "514:\tlearn: 0.8011268\ttotal: 1m 46s\tremaining: 6m 49s\n",
      "515:\tlearn: 0.8007187\ttotal: 1m 46s\tremaining: 6m 48s\n",
      "516:\tlearn: 0.8004187\ttotal: 1m 46s\tremaining: 6m 48s\n",
      "517:\tlearn: 0.8001295\ttotal: 1m 46s\tremaining: 6m 48s\n",
      "518:\tlearn: 0.7998739\ttotal: 1m 47s\tremaining: 6m 48s\n",
      "519:\tlearn: 0.7996004\ttotal: 1m 47s\tremaining: 6m 48s\n",
      "520:\tlearn: 0.7992999\ttotal: 1m 47s\tremaining: 6m 48s\n",
      "521:\tlearn: 0.7990224\ttotal: 1m 47s\tremaining: 6m 47s\n",
      "522:\tlearn: 0.7986461\ttotal: 1m 47s\tremaining: 6m 47s\n",
      "523:\tlearn: 0.7983011\ttotal: 1m 48s\tremaining: 6m 47s\n",
      "524:\tlearn: 0.7980802\ttotal: 1m 48s\tremaining: 6m 47s\n",
      "525:\tlearn: 0.7978507\ttotal: 1m 48s\tremaining: 6m 46s\n",
      "526:\tlearn: 0.7975855\ttotal: 1m 48s\tremaining: 6m 46s\n",
      "527:\tlearn: 0.7973100\ttotal: 1m 48s\tremaining: 6m 46s\n",
      "528:\tlearn: 0.7970613\ttotal: 1m 49s\tremaining: 6m 46s\n",
      "529:\tlearn: 0.7966598\ttotal: 1m 49s\tremaining: 6m 46s\n",
      "530:\tlearn: 0.7963563\ttotal: 1m 49s\tremaining: 6m 46s\n",
      "531:\tlearn: 0.7961064\ttotal: 1m 49s\tremaining: 6m 45s\n",
      "532:\tlearn: 0.7957706\ttotal: 1m 49s\tremaining: 6m 45s\n",
      "533:\tlearn: 0.7954766\ttotal: 1m 50s\tremaining: 6m 45s\n",
      "534:\tlearn: 0.7952294\ttotal: 1m 50s\tremaining: 6m 45s\n",
      "535:\tlearn: 0.7949169\ttotal: 1m 50s\tremaining: 6m 45s\n",
      "536:\tlearn: 0.7946483\ttotal: 1m 50s\tremaining: 6m 44s\n",
      "537:\tlearn: 0.7943717\ttotal: 1m 50s\tremaining: 6m 44s\n",
      "538:\tlearn: 0.7939710\ttotal: 1m 51s\tremaining: 6m 44s\n",
      "539:\tlearn: 0.7936641\ttotal: 1m 51s\tremaining: 6m 44s\n",
      "540:\tlearn: 0.7934221\ttotal: 1m 51s\tremaining: 6m 44s\n",
      "541:\tlearn: 0.7931338\ttotal: 1m 51s\tremaining: 6m 43s\n",
      "542:\tlearn: 0.7929137\ttotal: 1m 52s\tremaining: 6m 43s\n",
      "543:\tlearn: 0.7925695\ttotal: 1m 52s\tremaining: 6m 43s\n",
      "544:\tlearn: 0.7922726\ttotal: 1m 52s\tremaining: 6m 43s\n",
      "545:\tlearn: 0.7919968\ttotal: 1m 52s\tremaining: 6m 43s\n",
      "546:\tlearn: 0.7917491\ttotal: 1m 52s\tremaining: 6m 42s\n",
      "547:\tlearn: 0.7914742\ttotal: 1m 53s\tremaining: 6m 42s\n",
      "548:\tlearn: 0.7912682\ttotal: 1m 53s\tremaining: 6m 42s\n",
      "549:\tlearn: 0.7909881\ttotal: 1m 53s\tremaining: 6m 42s\n",
      "550:\tlearn: 0.7906910\ttotal: 1m 53s\tremaining: 6m 41s\n",
      "551:\tlearn: 0.7903531\ttotal: 1m 53s\tremaining: 6m 41s\n",
      "552:\tlearn: 0.7901189\ttotal: 1m 54s\tremaining: 6m 41s\n",
      "553:\tlearn: 0.7899007\ttotal: 1m 54s\tremaining: 6m 41s\n",
      "554:\tlearn: 0.7896100\ttotal: 1m 54s\tremaining: 6m 41s\n",
      "555:\tlearn: 0.7892774\ttotal: 1m 54s\tremaining: 6m 40s\n",
      "556:\tlearn: 0.7890493\ttotal: 1m 54s\tremaining: 6m 40s\n",
      "557:\tlearn: 0.7888365\ttotal: 1m 55s\tremaining: 6m 40s\n",
      "558:\tlearn: 0.7886451\ttotal: 1m 55s\tremaining: 6m 40s\n",
      "559:\tlearn: 0.7883716\ttotal: 1m 55s\tremaining: 6m 40s\n",
      "560:\tlearn: 0.7880604\ttotal: 1m 55s\tremaining: 6m 39s\n",
      "561:\tlearn: 0.7878532\ttotal: 1m 55s\tremaining: 6m 39s\n",
      "562:\tlearn: 0.7875722\ttotal: 1m 56s\tremaining: 6m 39s\n",
      "563:\tlearn: 0.7873726\ttotal: 1m 56s\tremaining: 6m 39s\n",
      "564:\tlearn: 0.7870504\ttotal: 1m 56s\tremaining: 6m 38s\n",
      "565:\tlearn: 0.7867622\ttotal: 1m 56s\tremaining: 6m 38s\n",
      "566:\tlearn: 0.7865014\ttotal: 1m 56s\tremaining: 6m 38s\n",
      "567:\tlearn: 0.7862725\ttotal: 1m 57s\tremaining: 6m 38s\n",
      "568:\tlearn: 0.7858688\ttotal: 1m 57s\tremaining: 6m 38s\n",
      "569:\tlearn: 0.7855868\ttotal: 1m 57s\tremaining: 6m 37s\n",
      "570:\tlearn: 0.7853270\ttotal: 1m 57s\tremaining: 6m 37s\n",
      "571:\tlearn: 0.7850546\ttotal: 1m 57s\tremaining: 6m 37s\n",
      "572:\tlearn: 0.7848319\ttotal: 1m 58s\tremaining: 6m 37s\n",
      "573:\tlearn: 0.7846444\ttotal: 1m 58s\tremaining: 6m 37s\n",
      "574:\tlearn: 0.7843717\ttotal: 1m 58s\tremaining: 6m 36s\n",
      "575:\tlearn: 0.7840592\ttotal: 1m 58s\tremaining: 6m 36s\n",
      "576:\tlearn: 0.7838044\ttotal: 1m 58s\tremaining: 6m 36s\n",
      "577:\tlearn: 0.7835550\ttotal: 1m 59s\tremaining: 6m 36s\n",
      "578:\tlearn: 0.7833712\ttotal: 1m 59s\tremaining: 6m 36s\n",
      "579:\tlearn: 0.7831270\ttotal: 1m 59s\tremaining: 6m 35s\n",
      "580:\tlearn: 0.7828750\ttotal: 1m 59s\tremaining: 6m 35s\n",
      "581:\tlearn: 0.7826294\ttotal: 1m 59s\tremaining: 6m 35s\n",
      "582:\tlearn: 0.7823805\ttotal: 2m\tremaining: 6m 35s\n",
      "583:\tlearn: 0.7821010\ttotal: 2m\tremaining: 6m 35s\n",
      "584:\tlearn: 0.7819366\ttotal: 2m\tremaining: 6m 34s\n",
      "585:\tlearn: 0.7817704\ttotal: 2m\tremaining: 6m 34s\n",
      "586:\tlearn: 0.7815191\ttotal: 2m 1s\tremaining: 6m 34s\n",
      "587:\tlearn: 0.7811607\ttotal: 2m 1s\tremaining: 6m 34s\n",
      "588:\tlearn: 0.7809138\ttotal: 2m 1s\tremaining: 6m 33s\n",
      "589:\tlearn: 0.7806986\ttotal: 2m 1s\tremaining: 6m 33s\n",
      "590:\tlearn: 0.7803777\ttotal: 2m 1s\tremaining: 6m 33s\n",
      "591:\tlearn: 0.7801810\ttotal: 2m 2s\tremaining: 6m 33s\n",
      "592:\tlearn: 0.7799225\ttotal: 2m 2s\tremaining: 6m 33s\n",
      "593:\tlearn: 0.7796961\ttotal: 2m 2s\tremaining: 6m 33s\n",
      "594:\tlearn: 0.7794341\ttotal: 2m 2s\tremaining: 6m 32s\n",
      "595:\tlearn: 0.7791451\ttotal: 2m 2s\tremaining: 6m 32s\n",
      "596:\tlearn: 0.7789411\ttotal: 2m 3s\tremaining: 6m 32s\n",
      "597:\tlearn: 0.7787284\ttotal: 2m 3s\tremaining: 6m 32s\n",
      "598:\tlearn: 0.7784504\ttotal: 2m 3s\tremaining: 6m 31s\n",
      "599:\tlearn: 0.7782624\ttotal: 2m 3s\tremaining: 6m 31s\n",
      "600:\tlearn: 0.7779362\ttotal: 2m 3s\tremaining: 6m 31s\n",
      "601:\tlearn: 0.7777252\ttotal: 2m 4s\tremaining: 6m 31s\n",
      "602:\tlearn: 0.7774872\ttotal: 2m 4s\tremaining: 6m 31s\n",
      "603:\tlearn: 0.7772337\ttotal: 2m 4s\tremaining: 6m 30s\n",
      "604:\tlearn: 0.7770685\ttotal: 2m 4s\tremaining: 6m 30s\n",
      "605:\tlearn: 0.7768942\ttotal: 2m 4s\tremaining: 6m 30s\n",
      "606:\tlearn: 0.7766114\ttotal: 2m 5s\tremaining: 6m 30s\n",
      "607:\tlearn: 0.7763827\ttotal: 2m 5s\tremaining: 6m 30s\n",
      "608:\tlearn: 0.7761456\ttotal: 2m 5s\tremaining: 6m 29s\n",
      "609:\tlearn: 0.7759069\ttotal: 2m 5s\tremaining: 6m 29s\n",
      "610:\tlearn: 0.7757261\ttotal: 2m 5s\tremaining: 6m 29s\n",
      "611:\tlearn: 0.7754799\ttotal: 2m 6s\tremaining: 6m 29s\n",
      "612:\tlearn: 0.7753095\ttotal: 2m 6s\tremaining: 6m 28s\n",
      "613:\tlearn: 0.7750366\ttotal: 2m 6s\tremaining: 6m 28s\n",
      "614:\tlearn: 0.7747354\ttotal: 2m 6s\tremaining: 6m 28s\n",
      "615:\tlearn: 0.7743731\ttotal: 2m 7s\tremaining: 6m 28s\n",
      "616:\tlearn: 0.7740860\ttotal: 2m 7s\tremaining: 6m 28s\n",
      "617:\tlearn: 0.7738982\ttotal: 2m 7s\tremaining: 6m 28s\n",
      "618:\tlearn: 0.7736400\ttotal: 2m 7s\tremaining: 6m 27s\n",
      "619:\tlearn: 0.7733610\ttotal: 2m 7s\tremaining: 6m 27s\n",
      "620:\tlearn: 0.7731587\ttotal: 2m 8s\tremaining: 6m 27s\n",
      "621:\tlearn: 0.7729496\ttotal: 2m 8s\tremaining: 6m 27s\n",
      "622:\tlearn: 0.7727969\ttotal: 2m 8s\tremaining: 6m 27s\n",
      "623:\tlearn: 0.7725171\ttotal: 2m 8s\tremaining: 6m 27s\n",
      "624:\tlearn: 0.7723323\ttotal: 2m 8s\tremaining: 6m 26s\n",
      "625:\tlearn: 0.7720854\ttotal: 2m 9s\tremaining: 6m 26s\n",
      "626:\tlearn: 0.7719117\ttotal: 2m 9s\tremaining: 6m 26s\n",
      "627:\tlearn: 0.7716397\ttotal: 2m 9s\tremaining: 6m 26s\n",
      "628:\tlearn: 0.7713335\ttotal: 2m 9s\tremaining: 6m 25s\n",
      "629:\tlearn: 0.7711562\ttotal: 2m 9s\tremaining: 6m 25s\n",
      "630:\tlearn: 0.7709615\ttotal: 2m 10s\tremaining: 6m 25s\n",
      "631:\tlearn: 0.7707242\ttotal: 2m 10s\tremaining: 6m 25s\n",
      "632:\tlearn: 0.7704494\ttotal: 2m 10s\tremaining: 6m 25s\n",
      "633:\tlearn: 0.7702042\ttotal: 2m 10s\tremaining: 6m 24s\n",
      "634:\tlearn: 0.7700782\ttotal: 2m 10s\tremaining: 6m 24s\n",
      "635:\tlearn: 0.7698768\ttotal: 2m 11s\tremaining: 6m 24s\n",
      "636:\tlearn: 0.7697096\ttotal: 2m 11s\tremaining: 6m 24s\n",
      "637:\tlearn: 0.7695185\ttotal: 2m 11s\tremaining: 6m 24s\n",
      "638:\tlearn: 0.7692504\ttotal: 2m 11s\tremaining: 6m 23s\n",
      "639:\tlearn: 0.7690506\ttotal: 2m 12s\tremaining: 6m 23s\n",
      "640:\tlearn: 0.7688305\ttotal: 2m 12s\tremaining: 6m 23s\n",
      "641:\tlearn: 0.7685799\ttotal: 2m 12s\tremaining: 6m 23s\n",
      "642:\tlearn: 0.7683127\ttotal: 2m 12s\tremaining: 6m 23s\n",
      "643:\tlearn: 0.7681880\ttotal: 2m 12s\tremaining: 6m 22s\n",
      "644:\tlearn: 0.7680303\ttotal: 2m 13s\tremaining: 6m 22s\n",
      "645:\tlearn: 0.7678207\ttotal: 2m 13s\tremaining: 6m 22s\n",
      "646:\tlearn: 0.7675644\ttotal: 2m 13s\tremaining: 6m 22s\n",
      "647:\tlearn: 0.7673310\ttotal: 2m 13s\tremaining: 6m 22s\n",
      "648:\tlearn: 0.7670638\ttotal: 2m 13s\tremaining: 6m 21s\n",
      "649:\tlearn: 0.7668089\ttotal: 2m 14s\tremaining: 6m 21s\n",
      "650:\tlearn: 0.7666390\ttotal: 2m 14s\tremaining: 6m 21s\n",
      "651:\tlearn: 0.7664426\ttotal: 2m 14s\tremaining: 6m 21s\n",
      "652:\tlearn: 0.7662994\ttotal: 2m 14s\tremaining: 6m 21s\n",
      "653:\tlearn: 0.7661100\ttotal: 2m 14s\tremaining: 6m 20s\n",
      "654:\tlearn: 0.7658091\ttotal: 2m 15s\tremaining: 6m 20s\n",
      "655:\tlearn: 0.7655346\ttotal: 2m 15s\tremaining: 6m 20s\n",
      "656:\tlearn: 0.7653056\ttotal: 2m 15s\tremaining: 6m 20s\n",
      "657:\tlearn: 0.7651026\ttotal: 2m 15s\tremaining: 6m 20s\n",
      "658:\tlearn: 0.7649125\ttotal: 2m 15s\tremaining: 6m 19s\n",
      "659:\tlearn: 0.7647248\ttotal: 2m 16s\tremaining: 6m 19s\n",
      "660:\tlearn: 0.7645492\ttotal: 2m 16s\tremaining: 6m 19s\n",
      "661:\tlearn: 0.7643743\ttotal: 2m 16s\tremaining: 6m 19s\n",
      "662:\tlearn: 0.7641415\ttotal: 2m 16s\tremaining: 6m 18s\n",
      "663:\tlearn: 0.7639658\ttotal: 2m 16s\tremaining: 6m 18s\n",
      "664:\tlearn: 0.7637718\ttotal: 2m 17s\tremaining: 6m 18s\n",
      "665:\tlearn: 0.7635412\ttotal: 2m 17s\tremaining: 6m 18s\n",
      "666:\tlearn: 0.7633373\ttotal: 2m 17s\tremaining: 6m 18s\n",
      "667:\tlearn: 0.7631486\ttotal: 2m 17s\tremaining: 6m 18s\n",
      "668:\tlearn: 0.7628673\ttotal: 2m 18s\tremaining: 6m 17s\n",
      "669:\tlearn: 0.7626814\ttotal: 2m 18s\tremaining: 6m 17s\n",
      "670:\tlearn: 0.7625598\ttotal: 2m 18s\tremaining: 6m 17s\n",
      "671:\tlearn: 0.7623472\ttotal: 2m 18s\tremaining: 6m 17s\n",
      "672:\tlearn: 0.7621536\ttotal: 2m 18s\tremaining: 6m 16s\n",
      "673:\tlearn: 0.7619577\ttotal: 2m 19s\tremaining: 6m 16s\n",
      "674:\tlearn: 0.7617434\ttotal: 2m 19s\tremaining: 6m 16s\n",
      "675:\tlearn: 0.7614076\ttotal: 2m 19s\tremaining: 6m 16s\n",
      "676:\tlearn: 0.7612635\ttotal: 2m 19s\tremaining: 6m 16s\n",
      "677:\tlearn: 0.7610516\ttotal: 2m 19s\tremaining: 6m 15s\n",
      "678:\tlearn: 0.7608493\ttotal: 2m 20s\tremaining: 6m 15s\n",
      "679:\tlearn: 0.7606385\ttotal: 2m 20s\tremaining: 6m 15s\n",
      "680:\tlearn: 0.7605010\ttotal: 2m 20s\tremaining: 6m 15s\n",
      "681:\tlearn: 0.7602855\ttotal: 2m 20s\tremaining: 6m 14s\n",
      "682:\tlearn: 0.7600669\ttotal: 2m 20s\tremaining: 6m 14s\n",
      "683:\tlearn: 0.7597848\ttotal: 2m 21s\tremaining: 6m 14s\n",
      "684:\tlearn: 0.7595624\ttotal: 2m 21s\tremaining: 6m 14s\n",
      "685:\tlearn: 0.7592842\ttotal: 2m 21s\tremaining: 6m 14s\n",
      "686:\tlearn: 0.7590881\ttotal: 2m 21s\tremaining: 6m 13s\n",
      "687:\tlearn: 0.7588807\ttotal: 2m 21s\tremaining: 6m 13s\n",
      "688:\tlearn: 0.7586557\ttotal: 2m 22s\tremaining: 6m 13s\n",
      "689:\tlearn: 0.7584580\ttotal: 2m 22s\tremaining: 6m 13s\n",
      "690:\tlearn: 0.7581038\ttotal: 2m 22s\tremaining: 6m 13s\n",
      "691:\tlearn: 0.7578977\ttotal: 2m 22s\tremaining: 6m 12s\n",
      "692:\tlearn: 0.7575992\ttotal: 2m 22s\tremaining: 6m 12s\n",
      "693:\tlearn: 0.7574280\ttotal: 2m 23s\tremaining: 6m 12s\n",
      "694:\tlearn: 0.7571364\ttotal: 2m 23s\tremaining: 6m 12s\n",
      "695:\tlearn: 0.7569115\ttotal: 2m 23s\tremaining: 6m 12s\n",
      "696:\tlearn: 0.7567067\ttotal: 2m 23s\tremaining: 6m 11s\n",
      "697:\tlearn: 0.7564857\ttotal: 2m 23s\tremaining: 6m 11s\n",
      "698:\tlearn: 0.7562667\ttotal: 2m 24s\tremaining: 6m 11s\n",
      "699:\tlearn: 0.7559234\ttotal: 2m 24s\tremaining: 6m 11s\n",
      "700:\tlearn: 0.7555877\ttotal: 2m 24s\tremaining: 6m 11s\n",
      "701:\tlearn: 0.7553719\ttotal: 2m 24s\tremaining: 6m 10s\n",
      "702:\tlearn: 0.7550545\ttotal: 2m 24s\tremaining: 6m 10s\n",
      "703:\tlearn: 0.7548227\ttotal: 2m 25s\tremaining: 6m 10s\n",
      "704:\tlearn: 0.7545292\ttotal: 2m 25s\tremaining: 6m 10s\n",
      "705:\tlearn: 0.7542881\ttotal: 2m 25s\tremaining: 6m 9s\n",
      "706:\tlearn: 0.7540448\ttotal: 2m 25s\tremaining: 6m 9s\n",
      "707:\tlearn: 0.7538228\ttotal: 2m 25s\tremaining: 6m 9s\n",
      "708:\tlearn: 0.7536975\ttotal: 2m 26s\tremaining: 6m 9s\n",
      "709:\tlearn: 0.7534429\ttotal: 2m 26s\tremaining: 6m 9s\n",
      "710:\tlearn: 0.7533190\ttotal: 2m 26s\tremaining: 6m 8s\n",
      "711:\tlearn: 0.7531196\ttotal: 2m 26s\tremaining: 6m 8s\n",
      "712:\tlearn: 0.7529214\ttotal: 2m 27s\tremaining: 6m 8s\n",
      "713:\tlearn: 0.7526885\ttotal: 2m 27s\tremaining: 6m 8s\n",
      "714:\tlearn: 0.7525007\ttotal: 2m 27s\tremaining: 6m 7s\n",
      "715:\tlearn: 0.7523081\ttotal: 2m 27s\tremaining: 6m 7s\n",
      "716:\tlearn: 0.7520900\ttotal: 2m 27s\tremaining: 6m 7s\n",
      "717:\tlearn: 0.7518633\ttotal: 2m 28s\tremaining: 6m 7s\n",
      "718:\tlearn: 0.7516796\ttotal: 2m 28s\tremaining: 6m 7s\n",
      "719:\tlearn: 0.7514941\ttotal: 2m 28s\tremaining: 6m 6s\n",
      "720:\tlearn: 0.7512392\ttotal: 2m 28s\tremaining: 6m 6s\n",
      "721:\tlearn: 0.7510184\ttotal: 2m 28s\tremaining: 6m 6s\n",
      "722:\tlearn: 0.7508468\ttotal: 2m 29s\tremaining: 6m 6s\n",
      "723:\tlearn: 0.7505932\ttotal: 2m 29s\tremaining: 6m 6s\n",
      "724:\tlearn: 0.7503983\ttotal: 2m 29s\tremaining: 6m 5s\n",
      "725:\tlearn: 0.7501869\ttotal: 2m 29s\tremaining: 6m 5s\n",
      "726:\tlearn: 0.7499509\ttotal: 2m 29s\tremaining: 6m 5s\n",
      "727:\tlearn: 0.7497628\ttotal: 2m 30s\tremaining: 6m 5s\n",
      "728:\tlearn: 0.7494993\ttotal: 2m 30s\tremaining: 6m 5s\n",
      "729:\tlearn: 0.7493455\ttotal: 2m 30s\tremaining: 6m 4s\n",
      "730:\tlearn: 0.7491481\ttotal: 2m 30s\tremaining: 6m 4s\n",
      "731:\tlearn: 0.7489991\ttotal: 2m 30s\tremaining: 6m 4s\n",
      "732:\tlearn: 0.7487512\ttotal: 2m 31s\tremaining: 6m 4s\n",
      "733:\tlearn: 0.7485063\ttotal: 2m 31s\tremaining: 6m 4s\n",
      "734:\tlearn: 0.7482334\ttotal: 2m 31s\tremaining: 6m 3s\n",
      "735:\tlearn: 0.7480700\ttotal: 2m 31s\tremaining: 6m 3s\n",
      "736:\tlearn: 0.7478326\ttotal: 2m 31s\tremaining: 6m 3s\n",
      "737:\tlearn: 0.7475973\ttotal: 2m 32s\tremaining: 6m 3s\n",
      "738:\tlearn: 0.7474057\ttotal: 2m 32s\tremaining: 6m 3s\n",
      "739:\tlearn: 0.7471939\ttotal: 2m 32s\tremaining: 6m 2s\n",
      "740:\tlearn: 0.7469958\ttotal: 2m 32s\tremaining: 6m 2s\n",
      "741:\tlearn: 0.7468072\ttotal: 2m 32s\tremaining: 6m 2s\n",
      "742:\tlearn: 0.7465777\ttotal: 2m 33s\tremaining: 6m 2s\n",
      "743:\tlearn: 0.7464166\ttotal: 2m 33s\tremaining: 6m 2s\n",
      "744:\tlearn: 0.7462013\ttotal: 2m 33s\tremaining: 6m 1s\n",
      "745:\tlearn: 0.7460672\ttotal: 2m 33s\tremaining: 6m 1s\n",
      "746:\tlearn: 0.7458214\ttotal: 2m 34s\tremaining: 6m 1s\n",
      "747:\tlearn: 0.7456248\ttotal: 2m 34s\tremaining: 6m 1s\n",
      "748:\tlearn: 0.7453705\ttotal: 2m 34s\tremaining: 6m 1s\n",
      "749:\tlearn: 0.7451004\ttotal: 2m 34s\tremaining: 6m\n",
      "750:\tlearn: 0.7449193\ttotal: 2m 34s\tremaining: 6m\n",
      "751:\tlearn: 0.7446788\ttotal: 2m 35s\tremaining: 6m\n",
      "752:\tlearn: 0.7444333\ttotal: 2m 35s\tremaining: 6m\n",
      "753:\tlearn: 0.7442708\ttotal: 2m 35s\tremaining: 5m 59s\n",
      "754:\tlearn: 0.7440696\ttotal: 2m 35s\tremaining: 5m 59s\n",
      "755:\tlearn: 0.7438831\ttotal: 2m 35s\tremaining: 5m 59s\n",
      "756:\tlearn: 0.7436488\ttotal: 2m 36s\tremaining: 5m 59s\n",
      "757:\tlearn: 0.7434630\ttotal: 2m 36s\tremaining: 5m 59s\n",
      "758:\tlearn: 0.7432323\ttotal: 2m 36s\tremaining: 5m 58s\n",
      "759:\tlearn: 0.7429373\ttotal: 2m 36s\tremaining: 5m 58s\n",
      "760:\tlearn: 0.7427125\ttotal: 2m 36s\tremaining: 5m 58s\n",
      "761:\tlearn: 0.7424827\ttotal: 2m 37s\tremaining: 5m 58s\n",
      "762:\tlearn: 0.7423571\ttotal: 2m 37s\tremaining: 5m 58s\n",
      "763:\tlearn: 0.7421449\ttotal: 2m 37s\tremaining: 5m 57s\n",
      "764:\tlearn: 0.7419231\ttotal: 2m 37s\tremaining: 5m 57s\n",
      "765:\tlearn: 0.7417446\ttotal: 2m 37s\tremaining: 5m 57s\n",
      "766:\tlearn: 0.7415493\ttotal: 2m 38s\tremaining: 5m 57s\n",
      "767:\tlearn: 0.7413000\ttotal: 2m 38s\tremaining: 5m 57s\n",
      "768:\tlearn: 0.7410121\ttotal: 2m 38s\tremaining: 5m 56s\n",
      "769:\tlearn: 0.7408578\ttotal: 2m 38s\tremaining: 5m 56s\n",
      "770:\tlearn: 0.7406253\ttotal: 2m 38s\tremaining: 5m 56s\n",
      "771:\tlearn: 0.7403355\ttotal: 2m 39s\tremaining: 5m 56s\n",
      "772:\tlearn: 0.7401148\ttotal: 2m 39s\tremaining: 5m 56s\n",
      "773:\tlearn: 0.7398766\ttotal: 2m 39s\tremaining: 5m 55s\n",
      "774:\tlearn: 0.7397067\ttotal: 2m 39s\tremaining: 5m 55s\n",
      "775:\tlearn: 0.7394852\ttotal: 2m 40s\tremaining: 5m 55s\n",
      "776:\tlearn: 0.7393103\ttotal: 2m 40s\tremaining: 5m 55s\n",
      "777:\tlearn: 0.7390097\ttotal: 2m 40s\tremaining: 5m 55s\n",
      "778:\tlearn: 0.7388322\ttotal: 2m 40s\tremaining: 5m 54s\n",
      "779:\tlearn: 0.7385923\ttotal: 2m 40s\tremaining: 5m 54s\n",
      "780:\tlearn: 0.7383791\ttotal: 2m 41s\tremaining: 5m 54s\n",
      "781:\tlearn: 0.7381123\ttotal: 2m 41s\tremaining: 5m 54s\n",
      "782:\tlearn: 0.7379085\ttotal: 2m 41s\tremaining: 5m 54s\n",
      "783:\tlearn: 0.7377717\ttotal: 2m 41s\tremaining: 5m 53s\n",
      "784:\tlearn: 0.7375951\ttotal: 2m 41s\tremaining: 5m 53s\n",
      "785:\tlearn: 0.7372884\ttotal: 2m 42s\tremaining: 5m 53s\n",
      "786:\tlearn: 0.7370313\ttotal: 2m 42s\tremaining: 5m 53s\n",
      "787:\tlearn: 0.7367568\ttotal: 2m 42s\tremaining: 5m 53s\n",
      "788:\tlearn: 0.7365713\ttotal: 2m 42s\tremaining: 5m 52s\n",
      "789:\tlearn: 0.7363581\ttotal: 2m 42s\tremaining: 5m 52s\n",
      "790:\tlearn: 0.7361018\ttotal: 2m 43s\tremaining: 5m 52s\n",
      "791:\tlearn: 0.7359347\ttotal: 2m 43s\tremaining: 5m 52s\n",
      "792:\tlearn: 0.7357508\ttotal: 2m 43s\tremaining: 5m 52s\n",
      "793:\tlearn: 0.7355118\ttotal: 2m 43s\tremaining: 5m 51s\n",
      "794:\tlearn: 0.7352355\ttotal: 2m 43s\tremaining: 5m 51s\n",
      "795:\tlearn: 0.7349672\ttotal: 2m 44s\tremaining: 5m 51s\n",
      "796:\tlearn: 0.7346681\ttotal: 2m 44s\tremaining: 5m 51s\n",
      "797:\tlearn: 0.7344001\ttotal: 2m 44s\tremaining: 5m 51s\n",
      "798:\tlearn: 0.7341478\ttotal: 2m 44s\tremaining: 5m 50s\n",
      "799:\tlearn: 0.7339027\ttotal: 2m 45s\tremaining: 5m 50s\n",
      "800:\tlearn: 0.7336264\ttotal: 2m 45s\tremaining: 5m 50s\n",
      "801:\tlearn: 0.7334433\ttotal: 2m 45s\tremaining: 5m 50s\n",
      "802:\tlearn: 0.7332478\ttotal: 2m 45s\tremaining: 5m 49s\n",
      "803:\tlearn: 0.7329584\ttotal: 2m 45s\tremaining: 5m 49s\n",
      "804:\tlearn: 0.7328226\ttotal: 2m 46s\tremaining: 5m 49s\n",
      "805:\tlearn: 0.7326356\ttotal: 2m 46s\tremaining: 5m 49s\n",
      "806:\tlearn: 0.7324129\ttotal: 2m 46s\tremaining: 5m 49s\n",
      "807:\tlearn: 0.7322365\ttotal: 2m 46s\tremaining: 5m 48s\n",
      "808:\tlearn: 0.7321049\ttotal: 2m 46s\tremaining: 5m 48s\n",
      "809:\tlearn: 0.7318283\ttotal: 2m 47s\tremaining: 5m 48s\n",
      "810:\tlearn: 0.7315424\ttotal: 2m 47s\tremaining: 5m 48s\n",
      "811:\tlearn: 0.7313416\ttotal: 2m 47s\tremaining: 5m 48s\n",
      "812:\tlearn: 0.7311809\ttotal: 2m 47s\tremaining: 5m 47s\n",
      "813:\tlearn: 0.7310079\ttotal: 2m 47s\tremaining: 5m 47s\n",
      "814:\tlearn: 0.7308047\ttotal: 2m 48s\tremaining: 5m 47s\n",
      "815:\tlearn: 0.7304787\ttotal: 2m 48s\tremaining: 5m 47s\n",
      "816:\tlearn: 0.7302005\ttotal: 2m 48s\tremaining: 5m 47s\n",
      "817:\tlearn: 0.7300119\ttotal: 2m 48s\tremaining: 5m 46s\n",
      "818:\tlearn: 0.7298422\ttotal: 2m 48s\tremaining: 5m 46s\n",
      "819:\tlearn: 0.7295638\ttotal: 2m 49s\tremaining: 5m 46s\n",
      "820:\tlearn: 0.7293512\ttotal: 2m 49s\tremaining: 5m 46s\n",
      "821:\tlearn: 0.7291781\ttotal: 2m 49s\tremaining: 5m 46s\n",
      "822:\tlearn: 0.7290267\ttotal: 2m 49s\tremaining: 5m 45s\n",
      "823:\tlearn: 0.7288122\ttotal: 2m 49s\tremaining: 5m 45s\n",
      "824:\tlearn: 0.7285739\ttotal: 2m 50s\tremaining: 5m 45s\n",
      "825:\tlearn: 0.7283235\ttotal: 2m 50s\tremaining: 5m 45s\n",
      "826:\tlearn: 0.7281553\ttotal: 2m 50s\tremaining: 5m 45s\n",
      "827:\tlearn: 0.7280003\ttotal: 2m 50s\tremaining: 5m 44s\n",
      "828:\tlearn: 0.7278412\ttotal: 2m 50s\tremaining: 5m 44s\n",
      "829:\tlearn: 0.7276090\ttotal: 2m 51s\tremaining: 5m 44s\n",
      "830:\tlearn: 0.7273486\ttotal: 2m 51s\tremaining: 5m 44s\n",
      "831:\tlearn: 0.7271959\ttotal: 2m 51s\tremaining: 5m 44s\n",
      "832:\tlearn: 0.7269765\ttotal: 2m 51s\tremaining: 5m 43s\n",
      "833:\tlearn: 0.7267763\ttotal: 2m 52s\tremaining: 5m 43s\n",
      "834:\tlearn: 0.7265591\ttotal: 2m 52s\tremaining: 5m 43s\n",
      "835:\tlearn: 0.7264398\ttotal: 2m 52s\tremaining: 5m 43s\n",
      "836:\tlearn: 0.7261997\ttotal: 2m 52s\tremaining: 5m 43s\n",
      "837:\tlearn: 0.7259235\ttotal: 2m 52s\tremaining: 5m 42s\n",
      "838:\tlearn: 0.7257387\ttotal: 2m 53s\tremaining: 5m 42s\n",
      "839:\tlearn: 0.7254805\ttotal: 2m 53s\tremaining: 5m 42s\n",
      "840:\tlearn: 0.7253147\ttotal: 2m 53s\tremaining: 5m 42s\n",
      "841:\tlearn: 0.7251426\ttotal: 2m 53s\tremaining: 5m 42s\n",
      "842:\tlearn: 0.7248895\ttotal: 2m 53s\tremaining: 5m 41s\n",
      "843:\tlearn: 0.7246543\ttotal: 2m 54s\tremaining: 5m 41s\n",
      "844:\tlearn: 0.7244767\ttotal: 2m 54s\tremaining: 5m 41s\n",
      "845:\tlearn: 0.7242861\ttotal: 2m 54s\tremaining: 5m 41s\n",
      "846:\tlearn: 0.7240993\ttotal: 2m 54s\tremaining: 5m 41s\n",
      "847:\tlearn: 0.7239405\ttotal: 2m 54s\tremaining: 5m 40s\n",
      "848:\tlearn: 0.7237575\ttotal: 2m 55s\tremaining: 5m 40s\n",
      "849:\tlearn: 0.7236329\ttotal: 2m 55s\tremaining: 5m 40s\n",
      "850:\tlearn: 0.7234070\ttotal: 2m 55s\tremaining: 5m 40s\n",
      "851:\tlearn: 0.7232023\ttotal: 2m 55s\tremaining: 5m 39s\n",
      "852:\tlearn: 0.7230197\ttotal: 2m 55s\tremaining: 5m 39s\n",
      "853:\tlearn: 0.7227995\ttotal: 2m 56s\tremaining: 5m 39s\n",
      "854:\tlearn: 0.7225606\ttotal: 2m 56s\tremaining: 5m 39s\n",
      "855:\tlearn: 0.7222871\ttotal: 2m 56s\tremaining: 5m 39s\n",
      "856:\tlearn: 0.7221103\ttotal: 2m 56s\tremaining: 5m 38s\n",
      "857:\tlearn: 0.7219364\ttotal: 2m 56s\tremaining: 5m 38s\n",
      "858:\tlearn: 0.7216961\ttotal: 2m 57s\tremaining: 5m 38s\n",
      "859:\tlearn: 0.7214787\ttotal: 2m 57s\tremaining: 5m 38s\n",
      "860:\tlearn: 0.7212279\ttotal: 2m 57s\tremaining: 5m 38s\n",
      "861:\tlearn: 0.7210766\ttotal: 2m 57s\tremaining: 5m 37s\n",
      "862:\tlearn: 0.7209710\ttotal: 2m 58s\tremaining: 5m 37s\n",
      "863:\tlearn: 0.7207212\ttotal: 2m 58s\tremaining: 5m 37s\n",
      "864:\tlearn: 0.7204631\ttotal: 2m 58s\tremaining: 5m 37s\n",
      "865:\tlearn: 0.7202913\ttotal: 2m 58s\tremaining: 5m 37s\n",
      "866:\tlearn: 0.7200373\ttotal: 2m 58s\tremaining: 5m 36s\n",
      "867:\tlearn: 0.7199034\ttotal: 2m 59s\tremaining: 5m 36s\n",
      "868:\tlearn: 0.7197046\ttotal: 2m 59s\tremaining: 5m 36s\n",
      "869:\tlearn: 0.7195679\ttotal: 2m 59s\tremaining: 5m 36s\n",
      "870:\tlearn: 0.7193689\ttotal: 2m 59s\tremaining: 5m 35s\n",
      "871:\tlearn: 0.7192062\ttotal: 2m 59s\tremaining: 5m 35s\n",
      "872:\tlearn: 0.7190042\ttotal: 3m\tremaining: 5m 35s\n",
      "873:\tlearn: 0.7187456\ttotal: 3m\tremaining: 5m 35s\n",
      "874:\tlearn: 0.7184986\ttotal: 3m\tremaining: 5m 35s\n",
      "875:\tlearn: 0.7183446\ttotal: 3m\tremaining: 5m 34s\n",
      "876:\tlearn: 0.7181963\ttotal: 3m\tremaining: 5m 34s\n",
      "877:\tlearn: 0.7179924\ttotal: 3m 1s\tremaining: 5m 34s\n",
      "878:\tlearn: 0.7177145\ttotal: 3m 1s\tremaining: 5m 34s\n",
      "879:\tlearn: 0.7175327\ttotal: 3m 1s\tremaining: 5m 34s\n",
      "880:\tlearn: 0.7174161\ttotal: 3m 1s\tremaining: 5m 34s\n",
      "881:\tlearn: 0.7171987\ttotal: 3m 1s\tremaining: 5m 33s\n",
      "882:\tlearn: 0.7169235\ttotal: 3m 2s\tremaining: 5m 33s\n",
      "883:\tlearn: 0.7167458\ttotal: 3m 2s\tremaining: 5m 33s\n",
      "884:\tlearn: 0.7165010\ttotal: 3m 2s\tremaining: 5m 33s\n",
      "885:\tlearn: 0.7163160\ttotal: 3m 2s\tremaining: 5m 33s\n",
      "886:\tlearn: 0.7161244\ttotal: 3m 3s\tremaining: 5m 32s\n",
      "887:\tlearn: 0.7159199\ttotal: 3m 3s\tremaining: 5m 32s\n",
      "888:\tlearn: 0.7157184\ttotal: 3m 3s\tremaining: 5m 32s\n",
      "889:\tlearn: 0.7155076\ttotal: 3m 3s\tremaining: 5m 32s\n",
      "890:\tlearn: 0.7152690\ttotal: 3m 3s\tremaining: 5m 32s\n",
      "891:\tlearn: 0.7150973\ttotal: 3m 4s\tremaining: 5m 31s\n",
      "892:\tlearn: 0.7148627\ttotal: 3m 4s\tremaining: 5m 31s\n",
      "893:\tlearn: 0.7146519\ttotal: 3m 4s\tremaining: 5m 31s\n",
      "894:\tlearn: 0.7144872\ttotal: 3m 4s\tremaining: 5m 31s\n",
      "895:\tlearn: 0.7142800\ttotal: 3m 4s\tremaining: 5m 31s\n",
      "896:\tlearn: 0.7140277\ttotal: 3m 5s\tremaining: 5m 30s\n",
      "897:\tlearn: 0.7138636\ttotal: 3m 5s\tremaining: 5m 30s\n",
      "898:\tlearn: 0.7136456\ttotal: 3m 5s\tremaining: 5m 30s\n",
      "899:\tlearn: 0.7134632\ttotal: 3m 5s\tremaining: 5m 30s\n",
      "900:\tlearn: 0.7132833\ttotal: 3m 5s\tremaining: 5m 30s\n",
      "901:\tlearn: 0.7130613\ttotal: 3m 6s\tremaining: 5m 29s\n",
      "902:\tlearn: 0.7128718\ttotal: 3m 6s\tremaining: 5m 29s\n",
      "903:\tlearn: 0.7127567\ttotal: 3m 6s\tremaining: 5m 29s\n",
      "904:\tlearn: 0.7126047\ttotal: 3m 6s\tremaining: 5m 29s\n",
      "905:\tlearn: 0.7124295\ttotal: 3m 7s\tremaining: 5m 29s\n",
      "906:\tlearn: 0.7122574\ttotal: 3m 7s\tremaining: 5m 28s\n",
      "907:\tlearn: 0.7120710\ttotal: 3m 7s\tremaining: 5m 28s\n",
      "908:\tlearn: 0.7118583\ttotal: 3m 7s\tremaining: 5m 28s\n",
      "909:\tlearn: 0.7116300\ttotal: 3m 7s\tremaining: 5m 28s\n",
      "910:\tlearn: 0.7114828\ttotal: 3m 8s\tremaining: 5m 28s\n",
      "911:\tlearn: 0.7113377\ttotal: 3m 8s\tremaining: 5m 27s\n",
      "912:\tlearn: 0.7112040\ttotal: 3m 8s\tremaining: 5m 27s\n",
      "913:\tlearn: 0.7110130\ttotal: 3m 8s\tremaining: 5m 27s\n",
      "914:\tlearn: 0.7107524\ttotal: 3m 8s\tremaining: 5m 27s\n",
      "915:\tlearn: 0.7105997\ttotal: 3m 9s\tremaining: 5m 27s\n",
      "916:\tlearn: 0.7104645\ttotal: 3m 9s\tremaining: 5m 26s\n",
      "917:\tlearn: 0.7103277\ttotal: 3m 9s\tremaining: 5m 26s\n",
      "918:\tlearn: 0.7101147\ttotal: 3m 9s\tremaining: 5m 26s\n",
      "919:\tlearn: 0.7098663\ttotal: 3m 9s\tremaining: 5m 26s\n",
      "920:\tlearn: 0.7096779\ttotal: 3m 10s\tremaining: 5m 26s\n",
      "921:\tlearn: 0.7094487\ttotal: 3m 10s\tremaining: 5m 25s\n",
      "922:\tlearn: 0.7093197\ttotal: 3m 10s\tremaining: 5m 25s\n",
      "923:\tlearn: 0.7091742\ttotal: 3m 10s\tremaining: 5m 25s\n",
      "924:\tlearn: 0.7090289\ttotal: 3m 10s\tremaining: 5m 25s\n",
      "925:\tlearn: 0.7088681\ttotal: 3m 11s\tremaining: 5m 24s\n",
      "926:\tlearn: 0.7087642\ttotal: 3m 11s\tremaining: 5m 24s\n",
      "927:\tlearn: 0.7084937\ttotal: 3m 11s\tremaining: 5m 24s\n",
      "928:\tlearn: 0.7083585\ttotal: 3m 11s\tremaining: 5m 24s\n",
      "929:\tlearn: 0.7082015\ttotal: 3m 11s\tremaining: 5m 24s\n",
      "930:\tlearn: 0.7080705\ttotal: 3m 12s\tremaining: 5m 23s\n",
      "931:\tlearn: 0.7078906\ttotal: 3m 12s\tremaining: 5m 23s\n",
      "932:\tlearn: 0.7077502\ttotal: 3m 12s\tremaining: 5m 23s\n",
      "933:\tlearn: 0.7075559\ttotal: 3m 12s\tremaining: 5m 23s\n",
      "934:\tlearn: 0.7073018\ttotal: 3m 12s\tremaining: 5m 23s\n",
      "935:\tlearn: 0.7070686\ttotal: 3m 13s\tremaining: 5m 22s\n",
      "936:\tlearn: 0.7068658\ttotal: 3m 13s\tremaining: 5m 22s\n",
      "937:\tlearn: 0.7066870\ttotal: 3m 13s\tremaining: 5m 22s\n",
      "938:\tlearn: 0.7064315\ttotal: 3m 13s\tremaining: 5m 22s\n",
      "939:\tlearn: 0.7062377\ttotal: 3m 14s\tremaining: 5m 22s\n",
      "940:\tlearn: 0.7060681\ttotal: 3m 14s\tremaining: 5m 21s\n",
      "941:\tlearn: 0.7059527\ttotal: 3m 14s\tremaining: 5m 21s\n",
      "942:\tlearn: 0.7057530\ttotal: 3m 14s\tremaining: 5m 21s\n",
      "943:\tlearn: 0.7055630\ttotal: 3m 14s\tremaining: 5m 21s\n",
      "944:\tlearn: 0.7054179\ttotal: 3m 15s\tremaining: 5m 21s\n",
      "945:\tlearn: 0.7051835\ttotal: 3m 15s\tremaining: 5m 20s\n",
      "946:\tlearn: 0.7050327\ttotal: 3m 15s\tremaining: 5m 20s\n",
      "947:\tlearn: 0.7048137\ttotal: 3m 15s\tremaining: 5m 20s\n",
      "948:\tlearn: 0.7045810\ttotal: 3m 15s\tremaining: 5m 20s\n",
      "949:\tlearn: 0.7043639\ttotal: 3m 16s\tremaining: 5m 20s\n",
      "950:\tlearn: 0.7041300\ttotal: 3m 16s\tremaining: 5m 19s\n",
      "951:\tlearn: 0.7038489\ttotal: 3m 16s\tremaining: 5m 19s\n",
      "952:\tlearn: 0.7036157\ttotal: 3m 16s\tremaining: 5m 19s\n",
      "953:\tlearn: 0.7033880\ttotal: 3m 16s\tremaining: 5m 19s\n",
      "954:\tlearn: 0.7032514\ttotal: 3m 17s\tremaining: 5m 18s\n",
      "955:\tlearn: 0.7030751\ttotal: 3m 17s\tremaining: 5m 18s\n",
      "956:\tlearn: 0.7028976\ttotal: 3m 17s\tremaining: 5m 18s\n",
      "957:\tlearn: 0.7027549\ttotal: 3m 17s\tremaining: 5m 18s\n",
      "958:\tlearn: 0.7025020\ttotal: 3m 17s\tremaining: 5m 18s\n",
      "959:\tlearn: 0.7022508\ttotal: 3m 18s\tremaining: 5m 17s\n",
      "960:\tlearn: 0.7021000\ttotal: 3m 18s\tremaining: 5m 17s\n",
      "961:\tlearn: 0.7018792\ttotal: 3m 18s\tremaining: 5m 17s\n",
      "962:\tlearn: 0.7016887\ttotal: 3m 18s\tremaining: 5m 17s\n",
      "963:\tlearn: 0.7015215\ttotal: 3m 19s\tremaining: 5m 17s\n",
      "964:\tlearn: 0.7013458\ttotal: 3m 19s\tremaining: 5m 16s\n",
      "965:\tlearn: 0.7011441\ttotal: 3m 19s\tremaining: 5m 16s\n",
      "966:\tlearn: 0.7010204\ttotal: 3m 19s\tremaining: 5m 16s\n",
      "967:\tlearn: 0.7009192\ttotal: 3m 19s\tremaining: 5m 16s\n",
      "968:\tlearn: 0.7006759\ttotal: 3m 20s\tremaining: 5m 16s\n",
      "969:\tlearn: 0.7005343\ttotal: 3m 20s\tremaining: 5m 15s\n",
      "970:\tlearn: 0.7004156\ttotal: 3m 20s\tremaining: 5m 15s\n",
      "971:\tlearn: 0.7002487\ttotal: 3m 20s\tremaining: 5m 15s\n",
      "972:\tlearn: 0.7000939\ttotal: 3m 20s\tremaining: 5m 15s\n",
      "973:\tlearn: 0.6998711\ttotal: 3m 21s\tremaining: 5m 15s\n",
      "974:\tlearn: 0.6997178\ttotal: 3m 21s\tremaining: 5m 14s\n",
      "975:\tlearn: 0.6995550\ttotal: 3m 21s\tremaining: 5m 14s\n",
      "976:\tlearn: 0.6994174\ttotal: 3m 21s\tremaining: 5m 14s\n",
      "977:\tlearn: 0.6991392\ttotal: 3m 21s\tremaining: 5m 14s\n",
      "978:\tlearn: 0.6989689\ttotal: 3m 22s\tremaining: 5m 14s\n",
      "979:\tlearn: 0.6988232\ttotal: 3m 22s\tremaining: 5m 13s\n",
      "980:\tlearn: 0.6986951\ttotal: 3m 22s\tremaining: 5m 13s\n",
      "981:\tlearn: 0.6984678\ttotal: 3m 22s\tremaining: 5m 13s\n",
      "982:\tlearn: 0.6983456\ttotal: 3m 22s\tremaining: 5m 13s\n",
      "983:\tlearn: 0.6981997\ttotal: 3m 23s\tremaining: 5m 12s\n",
      "984:\tlearn: 0.6980030\ttotal: 3m 23s\tremaining: 5m 12s\n",
      "985:\tlearn: 0.6977631\ttotal: 3m 23s\tremaining: 5m 12s\n",
      "986:\tlearn: 0.6975427\ttotal: 3m 23s\tremaining: 5m 12s\n",
      "987:\tlearn: 0.6973387\ttotal: 3m 23s\tremaining: 5m 12s\n",
      "988:\tlearn: 0.6971326\ttotal: 3m 24s\tremaining: 5m 11s\n",
      "989:\tlearn: 0.6970249\ttotal: 3m 24s\tremaining: 5m 11s\n",
      "990:\tlearn: 0.6968430\ttotal: 3m 24s\tremaining: 5m 11s\n",
      "991:\tlearn: 0.6966413\ttotal: 3m 24s\tremaining: 5m 11s\n",
      "992:\tlearn: 0.6964545\ttotal: 3m 24s\tremaining: 5m 11s\n",
      "993:\tlearn: 0.6963144\ttotal: 3m 25s\tremaining: 5m 10s\n",
      "994:\tlearn: 0.6961959\ttotal: 3m 25s\tremaining: 5m 10s\n",
      "995:\tlearn: 0.6960187\ttotal: 3m 25s\tremaining: 5m 10s\n",
      "996:\tlearn: 0.6958329\ttotal: 3m 25s\tremaining: 5m 10s\n",
      "997:\tlearn: 0.6956730\ttotal: 3m 26s\tremaining: 5m 10s\n",
      "998:\tlearn: 0.6954741\ttotal: 3m 26s\tremaining: 5m 9s\n",
      "999:\tlearn: 0.6953497\ttotal: 3m 26s\tremaining: 5m 9s\n",
      "1000:\tlearn: 0.6952244\ttotal: 3m 26s\tremaining: 5m 9s\n",
      "1001:\tlearn: 0.6950208\ttotal: 3m 26s\tremaining: 5m 9s\n",
      "1002:\tlearn: 0.6948110\ttotal: 3m 27s\tremaining: 5m 9s\n",
      "1003:\tlearn: 0.6946466\ttotal: 3m 27s\tremaining: 5m 8s\n",
      "1004:\tlearn: 0.6944534\ttotal: 3m 27s\tremaining: 5m 8s\n",
      "1005:\tlearn: 0.6942100\ttotal: 3m 27s\tremaining: 5m 8s\n",
      "1006:\tlearn: 0.6939913\ttotal: 3m 27s\tremaining: 5m 8s\n",
      "1007:\tlearn: 0.6937254\ttotal: 3m 28s\tremaining: 5m 7s\n",
      "1008:\tlearn: 0.6935393\ttotal: 3m 28s\tremaining: 5m 7s\n",
      "1009:\tlearn: 0.6933632\ttotal: 3m 28s\tremaining: 5m 7s\n",
      "1010:\tlearn: 0.6932027\ttotal: 3m 28s\tremaining: 5m 7s\n",
      "1011:\tlearn: 0.6930239\ttotal: 3m 28s\tremaining: 5m 7s\n",
      "1012:\tlearn: 0.6928227\ttotal: 3m 29s\tremaining: 5m 6s\n",
      "1013:\tlearn: 0.6927173\ttotal: 3m 29s\tremaining: 5m 6s\n",
      "1014:\tlearn: 0.6925488\ttotal: 3m 29s\tremaining: 5m 6s\n",
      "1015:\tlearn: 0.6924178\ttotal: 3m 29s\tremaining: 5m 6s\n",
      "1016:\tlearn: 0.6922683\ttotal: 3m 29s\tremaining: 5m 6s\n",
      "1017:\tlearn: 0.6920794\ttotal: 3m 30s\tremaining: 5m 5s\n",
      "1018:\tlearn: 0.6918838\ttotal: 3m 30s\tremaining: 5m 5s\n",
      "1019:\tlearn: 0.6916991\ttotal: 3m 30s\tremaining: 5m 5s\n",
      "1020:\tlearn: 0.6915264\ttotal: 3m 30s\tremaining: 5m 5s\n",
      "1021:\tlearn: 0.6913449\ttotal: 3m 30s\tremaining: 5m 5s\n",
      "1022:\tlearn: 0.6912287\ttotal: 3m 31s\tremaining: 5m 4s\n",
      "1023:\tlearn: 0.6911047\ttotal: 3m 31s\tremaining: 5m 4s\n",
      "1024:\tlearn: 0.6909662\ttotal: 3m 31s\tremaining: 5m 4s\n",
      "1025:\tlearn: 0.6908157\ttotal: 3m 31s\tremaining: 5m 4s\n",
      "1026:\tlearn: 0.6906345\ttotal: 3m 32s\tremaining: 5m 4s\n",
      "1027:\tlearn: 0.6904998\ttotal: 3m 32s\tremaining: 5m 4s\n",
      "1028:\tlearn: 0.6903495\ttotal: 3m 32s\tremaining: 5m 3s\n",
      "1029:\tlearn: 0.6901648\ttotal: 3m 32s\tremaining: 5m 3s\n",
      "1030:\tlearn: 0.6899826\ttotal: 3m 33s\tremaining: 5m 3s\n",
      "1031:\tlearn: 0.6898429\ttotal: 3m 33s\tremaining: 5m 3s\n",
      "1032:\tlearn: 0.6896595\ttotal: 3m 33s\tremaining: 5m 3s\n",
      "1033:\tlearn: 0.6894411\ttotal: 3m 33s\tremaining: 5m 2s\n",
      "1034:\tlearn: 0.6893035\ttotal: 3m 33s\tremaining: 5m 2s\n",
      "1035:\tlearn: 0.6890892\ttotal: 3m 34s\tremaining: 5m 2s\n",
      "1036:\tlearn: 0.6889075\ttotal: 3m 34s\tremaining: 5m 2s\n",
      "1037:\tlearn: 0.6887724\ttotal: 3m 34s\tremaining: 5m 2s\n",
      "1038:\tlearn: 0.6885289\ttotal: 3m 34s\tremaining: 5m 1s\n",
      "1039:\tlearn: 0.6883162\ttotal: 3m 34s\tremaining: 5m 1s\n",
      "1040:\tlearn: 0.6881844\ttotal: 3m 35s\tremaining: 5m 1s\n",
      "1041:\tlearn: 0.6880728\ttotal: 3m 35s\tremaining: 5m 1s\n",
      "1042:\tlearn: 0.6878803\ttotal: 3m 35s\tremaining: 5m 1s\n",
      "1043:\tlearn: 0.6876812\ttotal: 3m 35s\tremaining: 5m\n",
      "1044:\tlearn: 0.6875517\ttotal: 3m 35s\tremaining: 5m\n",
      "1045:\tlearn: 0.6874272\ttotal: 3m 36s\tremaining: 5m\n",
      "1046:\tlearn: 0.6871818\ttotal: 3m 36s\tremaining: 5m\n",
      "1047:\tlearn: 0.6869977\ttotal: 3m 36s\tremaining: 5m\n",
      "1048:\tlearn: 0.6867921\ttotal: 3m 36s\tremaining: 4m 59s\n",
      "1049:\tlearn: 0.6866214\ttotal: 3m 36s\tremaining: 4m 59s\n",
      "1050:\tlearn: 0.6864050\ttotal: 3m 37s\tremaining: 4m 59s\n",
      "1051:\tlearn: 0.6862501\ttotal: 3m 37s\tremaining: 4m 59s\n",
      "1052:\tlearn: 0.6861120\ttotal: 3m 37s\tremaining: 4m 59s\n",
      "1053:\tlearn: 0.6859472\ttotal: 3m 37s\tremaining: 4m 58s\n",
      "1054:\tlearn: 0.6857484\ttotal: 3m 38s\tremaining: 4m 58s\n",
      "1055:\tlearn: 0.6855522\ttotal: 3m 38s\tremaining: 4m 58s\n",
      "1056:\tlearn: 0.6853895\ttotal: 3m 38s\tremaining: 4m 58s\n",
      "1057:\tlearn: 0.6852548\ttotal: 3m 38s\tremaining: 4m 58s\n",
      "1058:\tlearn: 0.6850829\ttotal: 3m 38s\tremaining: 4m 57s\n",
      "1059:\tlearn: 0.6849115\ttotal: 3m 39s\tremaining: 4m 57s\n",
      "1060:\tlearn: 0.6846990\ttotal: 3m 39s\tremaining: 4m 57s\n",
      "1061:\tlearn: 0.6844654\ttotal: 3m 39s\tremaining: 4m 57s\n",
      "1062:\tlearn: 0.6843074\ttotal: 3m 39s\tremaining: 4m 56s\n",
      "1063:\tlearn: 0.6841732\ttotal: 3m 39s\tremaining: 4m 56s\n",
      "1064:\tlearn: 0.6839475\ttotal: 3m 40s\tremaining: 4m 56s\n",
      "1065:\tlearn: 0.6838030\ttotal: 3m 40s\tremaining: 4m 56s\n",
      "1066:\tlearn: 0.6836402\ttotal: 3m 40s\tremaining: 4m 56s\n",
      "1067:\tlearn: 0.6834611\ttotal: 3m 40s\tremaining: 4m 55s\n",
      "1068:\tlearn: 0.6832634\ttotal: 3m 40s\tremaining: 4m 55s\n",
      "1069:\tlearn: 0.6831209\ttotal: 3m 41s\tremaining: 4m 55s\n",
      "1070:\tlearn: 0.6829410\ttotal: 3m 41s\tremaining: 4m 55s\n",
      "1071:\tlearn: 0.6827082\ttotal: 3m 41s\tremaining: 4m 55s\n",
      "1072:\tlearn: 0.6825193\ttotal: 3m 41s\tremaining: 4m 54s\n",
      "1073:\tlearn: 0.6823642\ttotal: 3m 41s\tremaining: 4m 54s\n",
      "1074:\tlearn: 0.6821701\ttotal: 3m 42s\tremaining: 4m 54s\n",
      "1075:\tlearn: 0.6820110\ttotal: 3m 42s\tremaining: 4m 54s\n",
      "1076:\tlearn: 0.6818026\ttotal: 3m 42s\tremaining: 4m 54s\n",
      "1077:\tlearn: 0.6816427\ttotal: 3m 42s\tremaining: 4m 53s\n",
      "1078:\tlearn: 0.6815181\ttotal: 3m 43s\tremaining: 4m 53s\n",
      "1079:\tlearn: 0.6813732\ttotal: 3m 43s\tremaining: 4m 53s\n",
      "1080:\tlearn: 0.6812208\ttotal: 3m 43s\tremaining: 4m 53s\n",
      "1081:\tlearn: 0.6809982\ttotal: 3m 43s\tremaining: 4m 53s\n",
      "1082:\tlearn: 0.6808438\ttotal: 3m 43s\tremaining: 4m 52s\n",
      "1083:\tlearn: 0.6806976\ttotal: 3m 44s\tremaining: 4m 52s\n",
      "1084:\tlearn: 0.6805665\ttotal: 3m 44s\tremaining: 4m 52s\n",
      "1085:\tlearn: 0.6804203\ttotal: 3m 44s\tremaining: 4m 52s\n",
      "1086:\tlearn: 0.6802760\ttotal: 3m 44s\tremaining: 4m 52s\n",
      "1087:\tlearn: 0.6800922\ttotal: 3m 44s\tremaining: 4m 51s\n",
      "1088:\tlearn: 0.6799250\ttotal: 3m 45s\tremaining: 4m 51s\n",
      "1089:\tlearn: 0.6796546\ttotal: 3m 45s\tremaining: 4m 51s\n",
      "1090:\tlearn: 0.6795312\ttotal: 3m 45s\tremaining: 4m 51s\n",
      "1091:\tlearn: 0.6793950\ttotal: 3m 45s\tremaining: 4m 51s\n",
      "1092:\tlearn: 0.6792192\ttotal: 3m 45s\tremaining: 4m 50s\n",
      "1093:\tlearn: 0.6790095\ttotal: 3m 46s\tremaining: 4m 50s\n",
      "1094:\tlearn: 0.6788599\ttotal: 3m 46s\tremaining: 4m 50s\n",
      "1095:\tlearn: 0.6787345\ttotal: 3m 46s\tremaining: 4m 50s\n",
      "1096:\tlearn: 0.6785623\ttotal: 3m 46s\tremaining: 4m 50s\n",
      "1097:\tlearn: 0.6784060\ttotal: 3m 46s\tremaining: 4m 49s\n",
      "1098:\tlearn: 0.6782865\ttotal: 3m 47s\tremaining: 4m 49s\n",
      "1099:\tlearn: 0.6781155\ttotal: 3m 47s\tremaining: 4m 49s\n",
      "1100:\tlearn: 0.6779695\ttotal: 3m 47s\tremaining: 4m 49s\n",
      "1101:\tlearn: 0.6778184\ttotal: 3m 47s\tremaining: 4m 48s\n",
      "1102:\tlearn: 0.6776112\ttotal: 3m 48s\tremaining: 4m 48s\n",
      "1103:\tlearn: 0.6774455\ttotal: 3m 48s\tremaining: 4m 48s\n",
      "1104:\tlearn: 0.6773117\ttotal: 3m 48s\tremaining: 4m 48s\n",
      "1105:\tlearn: 0.6771668\ttotal: 3m 48s\tremaining: 4m 48s\n",
      "1106:\tlearn: 0.6770452\ttotal: 3m 48s\tremaining: 4m 48s\n",
      "1107:\tlearn: 0.6768830\ttotal: 3m 49s\tremaining: 4m 47s\n",
      "1108:\tlearn: 0.6767071\ttotal: 3m 49s\tremaining: 4m 47s\n",
      "1109:\tlearn: 0.6764960\ttotal: 3m 49s\tremaining: 4m 47s\n",
      "1110:\tlearn: 0.6763701\ttotal: 3m 49s\tremaining: 4m 47s\n",
      "1111:\tlearn: 0.6762645\ttotal: 3m 49s\tremaining: 4m 46s\n",
      "1112:\tlearn: 0.6761006\ttotal: 3m 50s\tremaining: 4m 46s\n",
      "1113:\tlearn: 0.6759163\ttotal: 3m 50s\tremaining: 4m 46s\n",
      "1114:\tlearn: 0.6757565\ttotal: 3m 50s\tremaining: 4m 46s\n",
      "1115:\tlearn: 0.6755454\ttotal: 3m 50s\tremaining: 4m 46s\n",
      "1116:\tlearn: 0.6753255\ttotal: 3m 50s\tremaining: 4m 45s\n",
      "1117:\tlearn: 0.6751387\ttotal: 3m 51s\tremaining: 4m 45s\n",
      "1118:\tlearn: 0.6749530\ttotal: 3m 51s\tremaining: 4m 45s\n",
      "1119:\tlearn: 0.6747755\ttotal: 3m 51s\tremaining: 4m 45s\n",
      "1120:\tlearn: 0.6745548\ttotal: 3m 51s\tremaining: 4m 45s\n",
      "1121:\tlearn: 0.6742998\ttotal: 3m 52s\tremaining: 4m 44s\n",
      "1122:\tlearn: 0.6741083\ttotal: 3m 52s\tremaining: 4m 44s\n",
      "1123:\tlearn: 0.6739906\ttotal: 3m 52s\tremaining: 4m 44s\n",
      "1124:\tlearn: 0.6738507\ttotal: 3m 52s\tremaining: 4m 44s\n",
      "1125:\tlearn: 0.6736651\ttotal: 3m 52s\tremaining: 4m 44s\n",
      "1126:\tlearn: 0.6735359\ttotal: 3m 53s\tremaining: 4m 43s\n",
      "1127:\tlearn: 0.6733962\ttotal: 3m 53s\tremaining: 4m 43s\n",
      "1128:\tlearn: 0.6732691\ttotal: 3m 53s\tremaining: 4m 43s\n",
      "1129:\tlearn: 0.6730905\ttotal: 3m 53s\tremaining: 4m 43s\n",
      "1130:\tlearn: 0.6728830\ttotal: 3m 53s\tremaining: 4m 43s\n",
      "1131:\tlearn: 0.6727446\ttotal: 3m 54s\tremaining: 4m 43s\n",
      "1132:\tlearn: 0.6726022\ttotal: 3m 54s\tremaining: 4m 42s\n",
      "1133:\tlearn: 0.6724860\ttotal: 3m 54s\tremaining: 4m 42s\n",
      "1134:\tlearn: 0.6723583\ttotal: 3m 54s\tremaining: 4m 42s\n",
      "1135:\tlearn: 0.6721781\ttotal: 3m 54s\tremaining: 4m 42s\n",
      "1136:\tlearn: 0.6720231\ttotal: 3m 55s\tremaining: 4m 41s\n",
      "1137:\tlearn: 0.6717800\ttotal: 3m 55s\tremaining: 4m 41s\n",
      "1138:\tlearn: 0.6715696\ttotal: 3m 55s\tremaining: 4m 41s\n",
      "1139:\tlearn: 0.6714309\ttotal: 3m 55s\tremaining: 4m 41s\n",
      "1140:\tlearn: 0.6712724\ttotal: 3m 56s\tremaining: 4m 41s\n",
      "1141:\tlearn: 0.6711323\ttotal: 3m 56s\tremaining: 4m 40s\n",
      "1142:\tlearn: 0.6709977\ttotal: 3m 56s\tremaining: 4m 40s\n",
      "1143:\tlearn: 0.6708836\ttotal: 3m 56s\tremaining: 4m 40s\n",
      "1144:\tlearn: 0.6706351\ttotal: 3m 56s\tremaining: 4m 40s\n",
      "1145:\tlearn: 0.6704342\ttotal: 3m 57s\tremaining: 4m 40s\n",
      "1146:\tlearn: 0.6702960\ttotal: 3m 57s\tremaining: 4m 39s\n",
      "1147:\tlearn: 0.6701572\ttotal: 3m 57s\tremaining: 4m 39s\n",
      "1148:\tlearn: 0.6699988\ttotal: 3m 57s\tremaining: 4m 39s\n",
      "1149:\tlearn: 0.6697443\ttotal: 3m 57s\tremaining: 4m 39s\n",
      "1150:\tlearn: 0.6695531\ttotal: 3m 58s\tremaining: 4m 39s\n",
      "1151:\tlearn: 0.6693809\ttotal: 3m 58s\tremaining: 4m 38s\n",
      "1152:\tlearn: 0.6692205\ttotal: 3m 58s\tremaining: 4m 38s\n",
      "1153:\tlearn: 0.6690691\ttotal: 3m 58s\tremaining: 4m 38s\n",
      "1154:\tlearn: 0.6689682\ttotal: 3m 58s\tremaining: 4m 38s\n",
      "1155:\tlearn: 0.6688326\ttotal: 3m 59s\tremaining: 4m 38s\n",
      "1156:\tlearn: 0.6686727\ttotal: 3m 59s\tremaining: 4m 37s\n",
      "1157:\tlearn: 0.6684538\ttotal: 3m 59s\tremaining: 4m 37s\n",
      "1158:\tlearn: 0.6683519\ttotal: 3m 59s\tremaining: 4m 37s\n",
      "1159:\tlearn: 0.6682516\ttotal: 3m 59s\tremaining: 4m 37s\n",
      "1160:\tlearn: 0.6680985\ttotal: 4m\tremaining: 4m 36s\n",
      "1161:\tlearn: 0.6679644\ttotal: 4m\tremaining: 4m 36s\n",
      "1162:\tlearn: 0.6677702\ttotal: 4m\tremaining: 4m 36s\n",
      "1163:\tlearn: 0.6676206\ttotal: 4m\tremaining: 4m 36s\n",
      "1164:\tlearn: 0.6674707\ttotal: 4m\tremaining: 4m 36s\n",
      "1165:\tlearn: 0.6673355\ttotal: 4m 1s\tremaining: 4m 35s\n",
      "1166:\tlearn: 0.6672169\ttotal: 4m 1s\tremaining: 4m 35s\n",
      "1167:\tlearn: 0.6670825\ttotal: 4m 1s\tremaining: 4m 35s\n",
      "1168:\tlearn: 0.6668600\ttotal: 4m 1s\tremaining: 4m 35s\n",
      "1169:\tlearn: 0.6667515\ttotal: 4m 2s\tremaining: 4m 35s\n",
      "1170:\tlearn: 0.6666574\ttotal: 4m 2s\tremaining: 4m 34s\n",
      "1171:\tlearn: 0.6665543\ttotal: 4m 2s\tremaining: 4m 34s\n",
      "1172:\tlearn: 0.6664436\ttotal: 4m 2s\tremaining: 4m 34s\n",
      "1173:\tlearn: 0.6663070\ttotal: 4m 2s\tremaining: 4m 34s\n",
      "1174:\tlearn: 0.6661455\ttotal: 4m 3s\tremaining: 4m 34s\n",
      "1175:\tlearn: 0.6659789\ttotal: 4m 3s\tremaining: 4m 33s\n",
      "1176:\tlearn: 0.6658478\ttotal: 4m 3s\tremaining: 4m 33s\n",
      "1177:\tlearn: 0.6656291\ttotal: 4m 3s\tremaining: 4m 33s\n",
      "1178:\tlearn: 0.6654599\ttotal: 4m 3s\tremaining: 4m 33s\n",
      "1179:\tlearn: 0.6653286\ttotal: 4m 4s\tremaining: 4m 33s\n",
      "1180:\tlearn: 0.6651991\ttotal: 4m 4s\tremaining: 4m 32s\n",
      "1181:\tlearn: 0.6650336\ttotal: 4m 4s\tremaining: 4m 32s\n",
      "1182:\tlearn: 0.6649122\ttotal: 4m 4s\tremaining: 4m 32s\n",
      "1183:\tlearn: 0.6647083\ttotal: 4m 4s\tremaining: 4m 32s\n",
      "1184:\tlearn: 0.6645351\ttotal: 4m 5s\tremaining: 4m 32s\n",
      "1185:\tlearn: 0.6643897\ttotal: 4m 5s\tremaining: 4m 31s\n",
      "1186:\tlearn: 0.6642713\ttotal: 4m 5s\tremaining: 4m 31s\n",
      "1187:\tlearn: 0.6641371\ttotal: 4m 5s\tremaining: 4m 31s\n",
      "1188:\tlearn: 0.6640226\ttotal: 4m 6s\tremaining: 4m 31s\n",
      "1189:\tlearn: 0.6638964\ttotal: 4m 6s\tremaining: 4m 31s\n",
      "1190:\tlearn: 0.6637147\ttotal: 4m 6s\tremaining: 4m 30s\n",
      "1191:\tlearn: 0.6636348\ttotal: 4m 6s\tremaining: 4m 30s\n",
      "1192:\tlearn: 0.6635145\ttotal: 4m 6s\tremaining: 4m 30s\n",
      "1193:\tlearn: 0.6633239\ttotal: 4m 7s\tremaining: 4m 30s\n",
      "1194:\tlearn: 0.6631927\ttotal: 4m 7s\tremaining: 4m 30s\n",
      "1195:\tlearn: 0.6631064\ttotal: 4m 7s\tremaining: 4m 29s\n",
      "1196:\tlearn: 0.6629720\ttotal: 4m 7s\tremaining: 4m 29s\n",
      "1197:\tlearn: 0.6627667\ttotal: 4m 7s\tremaining: 4m 29s\n",
      "1198:\tlearn: 0.6626068\ttotal: 4m 8s\tremaining: 4m 29s\n",
      "1199:\tlearn: 0.6625025\ttotal: 4m 8s\tremaining: 4m 29s\n",
      "1200:\tlearn: 0.6623858\ttotal: 4m 8s\tremaining: 4m 28s\n",
      "1201:\tlearn: 0.6622520\ttotal: 4m 8s\tremaining: 4m 28s\n",
      "1202:\tlearn: 0.6621280\ttotal: 4m 9s\tremaining: 4m 28s\n",
      "1203:\tlearn: 0.6619403\ttotal: 4m 9s\tremaining: 4m 28s\n",
      "1204:\tlearn: 0.6618195\ttotal: 4m 9s\tremaining: 4m 28s\n",
      "1205:\tlearn: 0.6617259\ttotal: 4m 9s\tremaining: 4m 27s\n",
      "1206:\tlearn: 0.6615950\ttotal: 4m 9s\tremaining: 4m 27s\n",
      "1207:\tlearn: 0.6614161\ttotal: 4m 10s\tremaining: 4m 27s\n",
      "1208:\tlearn: 0.6612425\ttotal: 4m 10s\tremaining: 4m 27s\n",
      "1209:\tlearn: 0.6611328\ttotal: 4m 10s\tremaining: 4m 27s\n",
      "1210:\tlearn: 0.6609832\ttotal: 4m 10s\tremaining: 4m 26s\n",
      "1211:\tlearn: 0.6608040\ttotal: 4m 10s\tremaining: 4m 26s\n",
      "1212:\tlearn: 0.6606437\ttotal: 4m 11s\tremaining: 4m 26s\n",
      "1213:\tlearn: 0.6605447\ttotal: 4m 11s\tremaining: 4m 26s\n",
      "1214:\tlearn: 0.6603859\ttotal: 4m 11s\tremaining: 4m 26s\n",
      "1215:\tlearn: 0.6602715\ttotal: 4m 11s\tremaining: 4m 25s\n",
      "1216:\tlearn: 0.6601821\ttotal: 4m 11s\tremaining: 4m 25s\n",
      "1217:\tlearn: 0.6600593\ttotal: 4m 12s\tremaining: 4m 25s\n",
      "1218:\tlearn: 0.6598902\ttotal: 4m 12s\tremaining: 4m 25s\n",
      "1219:\tlearn: 0.6596899\ttotal: 4m 12s\tremaining: 4m 25s\n",
      "1220:\tlearn: 0.6595026\ttotal: 4m 12s\tremaining: 4m 24s\n",
      "1221:\tlearn: 0.6593546\ttotal: 4m 13s\tremaining: 4m 24s\n",
      "1222:\tlearn: 0.6592149\ttotal: 4m 13s\tremaining: 4m 24s\n",
      "1223:\tlearn: 0.6590362\ttotal: 4m 13s\tremaining: 4m 24s\n",
      "1224:\tlearn: 0.6589098\ttotal: 4m 13s\tremaining: 4m 24s\n",
      "1225:\tlearn: 0.6587893\ttotal: 4m 13s\tremaining: 4m 23s\n",
      "1226:\tlearn: 0.6586425\ttotal: 4m 14s\tremaining: 4m 23s\n",
      "1227:\tlearn: 0.6585227\ttotal: 4m 14s\tremaining: 4m 23s\n",
      "1228:\tlearn: 0.6583789\ttotal: 4m 14s\tremaining: 4m 23s\n",
      "1229:\tlearn: 0.6582640\ttotal: 4m 14s\tremaining: 4m 23s\n",
      "1230:\tlearn: 0.6580803\ttotal: 4m 14s\tremaining: 4m 22s\n",
      "1231:\tlearn: 0.6579363\ttotal: 4m 15s\tremaining: 4m 22s\n",
      "1232:\tlearn: 0.6577954\ttotal: 4m 15s\tremaining: 4m 22s\n",
      "1233:\tlearn: 0.6576574\ttotal: 4m 15s\tremaining: 4m 22s\n",
      "1234:\tlearn: 0.6575241\ttotal: 4m 15s\tremaining: 4m 21s\n",
      "1235:\tlearn: 0.6574137\ttotal: 4m 15s\tremaining: 4m 21s\n",
      "1236:\tlearn: 0.6572873\ttotal: 4m 16s\tremaining: 4m 21s\n",
      "1237:\tlearn: 0.6571317\ttotal: 4m 16s\tremaining: 4m 21s\n",
      "1238:\tlearn: 0.6569444\ttotal: 4m 16s\tremaining: 4m 21s\n",
      "1239:\tlearn: 0.6568458\ttotal: 4m 16s\tremaining: 4m 20s\n",
      "1240:\tlearn: 0.6567015\ttotal: 4m 17s\tremaining: 4m 20s\n",
      "1241:\tlearn: 0.6565850\ttotal: 4m 17s\tremaining: 4m 20s\n",
      "1242:\tlearn: 0.6564094\ttotal: 4m 17s\tremaining: 4m 20s\n",
      "1243:\tlearn: 0.6562826\ttotal: 4m 17s\tremaining: 4m 20s\n",
      "1244:\tlearn: 0.6560937\ttotal: 4m 17s\tremaining: 4m 19s\n",
      "1245:\tlearn: 0.6559650\ttotal: 4m 18s\tremaining: 4m 19s\n",
      "1246:\tlearn: 0.6558215\ttotal: 4m 18s\tremaining: 4m 19s\n",
      "1247:\tlearn: 0.6557423\ttotal: 4m 18s\tremaining: 4m 19s\n",
      "1248:\tlearn: 0.6556391\ttotal: 4m 18s\tremaining: 4m 19s\n",
      "1249:\tlearn: 0.6555078\ttotal: 4m 18s\tremaining: 4m 18s\n",
      "1250:\tlearn: 0.6554494\ttotal: 4m 19s\tremaining: 4m 18s\n",
      "1251:\tlearn: 0.6553235\ttotal: 4m 19s\tremaining: 4m 18s\n",
      "1252:\tlearn: 0.6551382\ttotal: 4m 19s\tremaining: 4m 18s\n",
      "1253:\tlearn: 0.6550359\ttotal: 4m 19s\tremaining: 4m 18s\n",
      "1254:\tlearn: 0.6549162\ttotal: 4m 19s\tremaining: 4m 17s\n",
      "1255:\tlearn: 0.6548298\ttotal: 4m 20s\tremaining: 4m 17s\n",
      "1256:\tlearn: 0.6547459\ttotal: 4m 20s\tremaining: 4m 17s\n",
      "1257:\tlearn: 0.6546293\ttotal: 4m 20s\tremaining: 4m 17s\n",
      "1258:\tlearn: 0.6544945\ttotal: 4m 20s\tremaining: 4m 17s\n",
      "1259:\tlearn: 0.6543684\ttotal: 4m 20s\tremaining: 4m 16s\n",
      "1260:\tlearn: 0.6541831\ttotal: 4m 21s\tremaining: 4m 16s\n",
      "1261:\tlearn: 0.6539812\ttotal: 4m 21s\tremaining: 4m 16s\n",
      "1262:\tlearn: 0.6538928\ttotal: 4m 21s\tremaining: 4m 16s\n",
      "1263:\tlearn: 0.6537708\ttotal: 4m 21s\tremaining: 4m 16s\n",
      "1264:\tlearn: 0.6536250\ttotal: 4m 22s\tremaining: 4m 15s\n",
      "1265:\tlearn: 0.6535029\ttotal: 4m 22s\tremaining: 4m 15s\n",
      "1266:\tlearn: 0.6533813\ttotal: 4m 22s\tremaining: 4m 15s\n",
      "1267:\tlearn: 0.6532546\ttotal: 4m 22s\tremaining: 4m 15s\n",
      "1268:\tlearn: 0.6531539\ttotal: 4m 22s\tremaining: 4m 14s\n",
      "1269:\tlearn: 0.6530701\ttotal: 4m 23s\tremaining: 4m 14s\n",
      "1270:\tlearn: 0.6529767\ttotal: 4m 23s\tremaining: 4m 14s\n",
      "1271:\tlearn: 0.6527839\ttotal: 4m 23s\tremaining: 4m 14s\n",
      "1272:\tlearn: 0.6526161\ttotal: 4m 23s\tremaining: 4m 14s\n",
      "1273:\tlearn: 0.6524987\ttotal: 4m 23s\tremaining: 4m 13s\n",
      "1274:\tlearn: 0.6523394\ttotal: 4m 24s\tremaining: 4m 13s\n",
      "1275:\tlearn: 0.6522119\ttotal: 4m 24s\tremaining: 4m 13s\n",
      "1276:\tlearn: 0.6520231\ttotal: 4m 24s\tremaining: 4m 13s\n",
      "1277:\tlearn: 0.6518973\ttotal: 4m 24s\tremaining: 4m 13s\n",
      "1278:\tlearn: 0.6517723\ttotal: 4m 24s\tremaining: 4m 12s\n",
      "1279:\tlearn: 0.6516042\ttotal: 4m 25s\tremaining: 4m 12s\n",
      "1280:\tlearn: 0.6515008\ttotal: 4m 25s\tremaining: 4m 12s\n",
      "1281:\tlearn: 0.6513742\ttotal: 4m 25s\tremaining: 4m 12s\n",
      "1282:\tlearn: 0.6512406\ttotal: 4m 25s\tremaining: 4m 12s\n",
      "1283:\tlearn: 0.6510999\ttotal: 4m 26s\tremaining: 4m 11s\n",
      "1284:\tlearn: 0.6510016\ttotal: 4m 26s\tremaining: 4m 11s\n",
      "1285:\tlearn: 0.6508822\ttotal: 4m 26s\tremaining: 4m 11s\n",
      "1286:\tlearn: 0.6507548\ttotal: 4m 26s\tremaining: 4m 11s\n",
      "1287:\tlearn: 0.6505771\ttotal: 4m 26s\tremaining: 4m 11s\n",
      "1288:\tlearn: 0.6504802\ttotal: 4m 27s\tremaining: 4m 10s\n",
      "1289:\tlearn: 0.6503282\ttotal: 4m 27s\tremaining: 4m 10s\n",
      "1290:\tlearn: 0.6501564\ttotal: 4m 27s\tremaining: 4m 10s\n",
      "1291:\tlearn: 0.6499899\ttotal: 4m 27s\tremaining: 4m 10s\n",
      "1292:\tlearn: 0.6498580\ttotal: 4m 27s\tremaining: 4m 10s\n",
      "1293:\tlearn: 0.6497035\ttotal: 4m 28s\tremaining: 4m 9s\n",
      "1294:\tlearn: 0.6495524\ttotal: 4m 28s\tremaining: 4m 9s\n",
      "1295:\tlearn: 0.6493890\ttotal: 4m 28s\tremaining: 4m 9s\n",
      "1296:\tlearn: 0.6491930\ttotal: 4m 28s\tremaining: 4m 9s\n",
      "1297:\tlearn: 0.6490467\ttotal: 4m 28s\tremaining: 4m 9s\n",
      "1298:\tlearn: 0.6489253\ttotal: 4m 29s\tremaining: 4m 8s\n",
      "1299:\tlearn: 0.6487499\ttotal: 4m 29s\tremaining: 4m 8s\n",
      "1300:\tlearn: 0.6486522\ttotal: 4m 29s\tremaining: 4m 8s\n",
      "1301:\tlearn: 0.6485179\ttotal: 4m 29s\tremaining: 4m 8s\n",
      "1302:\tlearn: 0.6483684\ttotal: 4m 29s\tremaining: 4m 7s\n",
      "1303:\tlearn: 0.6482829\ttotal: 4m 30s\tremaining: 4m 7s\n",
      "1304:\tlearn: 0.6481720\ttotal: 4m 30s\tremaining: 4m 7s\n",
      "1305:\tlearn: 0.6480919\ttotal: 4m 30s\tremaining: 4m 7s\n",
      "1306:\tlearn: 0.6479310\ttotal: 4m 30s\tremaining: 4m 7s\n",
      "1307:\tlearn: 0.6477817\ttotal: 4m 31s\tremaining: 4m 6s\n",
      "1308:\tlearn: 0.6476642\ttotal: 4m 31s\tremaining: 4m 6s\n",
      "1309:\tlearn: 0.6475898\ttotal: 4m 31s\tremaining: 4m 6s\n",
      "1310:\tlearn: 0.6474355\ttotal: 4m 31s\tremaining: 4m 6s\n",
      "1311:\tlearn: 0.6472953\ttotal: 4m 31s\tremaining: 4m 6s\n",
      "1312:\tlearn: 0.6471710\ttotal: 4m 32s\tremaining: 4m 6s\n",
      "1313:\tlearn: 0.6470207\ttotal: 4m 32s\tremaining: 4m 5s\n",
      "1314:\tlearn: 0.6468624\ttotal: 4m 32s\tremaining: 4m 5s\n",
      "1315:\tlearn: 0.6467607\ttotal: 4m 32s\tremaining: 4m 5s\n",
      "1316:\tlearn: 0.6466661\ttotal: 4m 32s\tremaining: 4m 5s\n",
      "1317:\tlearn: 0.6465429\ttotal: 4m 33s\tremaining: 4m 5s\n",
      "1318:\tlearn: 0.6464080\ttotal: 4m 33s\tremaining: 4m 4s\n",
      "1319:\tlearn: 0.6462985\ttotal: 4m 33s\tremaining: 4m 4s\n",
      "1320:\tlearn: 0.6461338\ttotal: 4m 33s\tremaining: 4m 4s\n",
      "1321:\tlearn: 0.6460175\ttotal: 4m 34s\tremaining: 4m 4s\n",
      "1322:\tlearn: 0.6458574\ttotal: 4m 34s\tremaining: 4m 3s\n",
      "1323:\tlearn: 0.6457397\ttotal: 4m 34s\tremaining: 4m 3s\n",
      "1324:\tlearn: 0.6456331\ttotal: 4m 34s\tremaining: 4m 3s\n",
      "1325:\tlearn: 0.6455191\ttotal: 4m 34s\tremaining: 4m 3s\n",
      "1326:\tlearn: 0.6454127\ttotal: 4m 35s\tremaining: 4m 3s\n",
      "1327:\tlearn: 0.6452665\ttotal: 4m 35s\tremaining: 4m 2s\n",
      "1328:\tlearn: 0.6451576\ttotal: 4m 35s\tremaining: 4m 2s\n",
      "1329:\tlearn: 0.6450421\ttotal: 4m 35s\tremaining: 4m 2s\n",
      "1330:\tlearn: 0.6449108\ttotal: 4m 35s\tremaining: 4m 2s\n",
      "1331:\tlearn: 0.6447337\ttotal: 4m 36s\tremaining: 4m 2s\n",
      "1332:\tlearn: 0.6446381\ttotal: 4m 36s\tremaining: 4m 1s\n",
      "1333:\tlearn: 0.6445461\ttotal: 4m 36s\tremaining: 4m 1s\n",
      "1334:\tlearn: 0.6444680\ttotal: 4m 36s\tremaining: 4m 1s\n",
      "1335:\tlearn: 0.6443784\ttotal: 4m 36s\tremaining: 4m 1s\n",
      "1336:\tlearn: 0.6442650\ttotal: 4m 37s\tremaining: 4m 1s\n",
      "1337:\tlearn: 0.6441351\ttotal: 4m 37s\tremaining: 4m\n",
      "1338:\tlearn: 0.6440257\ttotal: 4m 37s\tremaining: 4m\n",
      "1339:\tlearn: 0.6438762\ttotal: 4m 37s\tremaining: 4m\n",
      "1340:\tlearn: 0.6437027\ttotal: 4m 37s\tremaining: 4m\n",
      "1341:\tlearn: 0.6436214\ttotal: 4m 38s\tremaining: 3m 59s\n",
      "1342:\tlearn: 0.6435155\ttotal: 4m 38s\tremaining: 3m 59s\n",
      "1343:\tlearn: 0.6434030\ttotal: 4m 38s\tremaining: 3m 59s\n",
      "1344:\tlearn: 0.6433204\ttotal: 4m 38s\tremaining: 3m 59s\n",
      "1345:\tlearn: 0.6432476\ttotal: 4m 38s\tremaining: 3m 59s\n",
      "1346:\tlearn: 0.6431228\ttotal: 4m 39s\tremaining: 3m 58s\n",
      "1347:\tlearn: 0.6430419\ttotal: 4m 39s\tremaining: 3m 58s\n",
      "1348:\tlearn: 0.6429415\ttotal: 4m 39s\tremaining: 3m 58s\n",
      "1349:\tlearn: 0.6428413\ttotal: 4m 39s\tremaining: 3m 58s\n",
      "1350:\tlearn: 0.6427079\ttotal: 4m 39s\tremaining: 3m 58s\n",
      "1351:\tlearn: 0.6425762\ttotal: 4m 40s\tremaining: 3m 57s\n",
      "1352:\tlearn: 0.6424556\ttotal: 4m 40s\tremaining: 3m 57s\n",
      "1353:\tlearn: 0.6423033\ttotal: 4m 40s\tremaining: 3m 57s\n",
      "1354:\tlearn: 0.6421771\ttotal: 4m 40s\tremaining: 3m 57s\n",
      "1355:\tlearn: 0.6420770\ttotal: 4m 41s\tremaining: 3m 57s\n",
      "1356:\tlearn: 0.6419323\ttotal: 4m 41s\tremaining: 3m 56s\n",
      "1357:\tlearn: 0.6417800\ttotal: 4m 41s\tremaining: 3m 56s\n",
      "1358:\tlearn: 0.6416505\ttotal: 4m 41s\tremaining: 3m 56s\n",
      "1359:\tlearn: 0.6415462\ttotal: 4m 41s\tremaining: 3m 56s\n",
      "1360:\tlearn: 0.6414526\ttotal: 4m 42s\tremaining: 3m 56s\n",
      "1361:\tlearn: 0.6412990\ttotal: 4m 42s\tremaining: 3m 55s\n",
      "1362:\tlearn: 0.6411547\ttotal: 4m 42s\tremaining: 3m 55s\n",
      "1363:\tlearn: 0.6410566\ttotal: 4m 42s\tremaining: 3m 55s\n",
      "1364:\tlearn: 0.6409853\ttotal: 4m 42s\tremaining: 3m 55s\n",
      "1365:\tlearn: 0.6408584\ttotal: 4m 43s\tremaining: 3m 55s\n",
      "1366:\tlearn: 0.6407281\ttotal: 4m 43s\tremaining: 3m 54s\n",
      "1367:\tlearn: 0.6405628\ttotal: 4m 43s\tremaining: 3m 54s\n",
      "1368:\tlearn: 0.6404706\ttotal: 4m 43s\tremaining: 3m 54s\n",
      "1369:\tlearn: 0.6403663\ttotal: 4m 43s\tremaining: 3m 54s\n",
      "1370:\tlearn: 0.6402475\ttotal: 4m 44s\tremaining: 3m 54s\n",
      "1371:\tlearn: 0.6401275\ttotal: 4m 44s\tremaining: 3m 53s\n",
      "1372:\tlearn: 0.6399963\ttotal: 4m 44s\tremaining: 3m 53s\n",
      "1373:\tlearn: 0.6398325\ttotal: 4m 44s\tremaining: 3m 53s\n",
      "1374:\tlearn: 0.6396965\ttotal: 4m 45s\tremaining: 3m 53s\n",
      "1375:\tlearn: 0.6395493\ttotal: 4m 45s\tremaining: 3m 53s\n",
      "1376:\tlearn: 0.6394044\ttotal: 4m 45s\tremaining: 3m 52s\n",
      "1377:\tlearn: 0.6392557\ttotal: 4m 45s\tremaining: 3m 52s\n",
      "1378:\tlearn: 0.6390913\ttotal: 4m 45s\tremaining: 3m 52s\n",
      "1379:\tlearn: 0.6389571\ttotal: 4m 46s\tremaining: 3m 52s\n",
      "1380:\tlearn: 0.6388095\ttotal: 4m 46s\tremaining: 3m 51s\n",
      "1381:\tlearn: 0.6387306\ttotal: 4m 46s\tremaining: 3m 51s\n",
      "1382:\tlearn: 0.6386282\ttotal: 4m 46s\tremaining: 3m 51s\n",
      "1383:\tlearn: 0.6385132\ttotal: 4m 46s\tremaining: 3m 51s\n",
      "1384:\tlearn: 0.6383653\ttotal: 4m 47s\tremaining: 3m 51s\n",
      "1385:\tlearn: 0.6382792\ttotal: 4m 47s\tremaining: 3m 50s\n",
      "1386:\tlearn: 0.6381568\ttotal: 4m 47s\tremaining: 3m 50s\n",
      "1387:\tlearn: 0.6380317\ttotal: 4m 47s\tremaining: 3m 50s\n",
      "1388:\tlearn: 0.6379311\ttotal: 4m 47s\tremaining: 3m 50s\n",
      "1389:\tlearn: 0.6378534\ttotal: 4m 48s\tremaining: 3m 50s\n",
      "1390:\tlearn: 0.6377683\ttotal: 4m 48s\tremaining: 3m 49s\n",
      "1391:\tlearn: 0.6376177\ttotal: 4m 48s\tremaining: 3m 49s\n",
      "1392:\tlearn: 0.6375391\ttotal: 4m 48s\tremaining: 3m 49s\n",
      "1393:\tlearn: 0.6374706\ttotal: 4m 48s\tremaining: 3m 49s\n",
      "1394:\tlearn: 0.6373630\ttotal: 4m 49s\tremaining: 3m 49s\n",
      "1395:\tlearn: 0.6372427\ttotal: 4m 49s\tremaining: 3m 48s\n",
      "1396:\tlearn: 0.6371271\ttotal: 4m 49s\tremaining: 3m 48s\n",
      "1397:\tlearn: 0.6370308\ttotal: 4m 49s\tremaining: 3m 48s\n",
      "1398:\tlearn: 0.6368568\ttotal: 4m 49s\tremaining: 3m 48s\n",
      "1399:\tlearn: 0.6367115\ttotal: 4m 50s\tremaining: 3m 48s\n",
      "1400:\tlearn: 0.6365415\ttotal: 4m 50s\tremaining: 3m 47s\n",
      "1401:\tlearn: 0.6364376\ttotal: 4m 50s\tremaining: 3m 47s\n",
      "1402:\tlearn: 0.6363068\ttotal: 4m 50s\tremaining: 3m 47s\n",
      "1403:\tlearn: 0.6361840\ttotal: 4m 51s\tremaining: 3m 47s\n",
      "1404:\tlearn: 0.6360601\ttotal: 4m 51s\tremaining: 3m 47s\n",
      "1405:\tlearn: 0.6359439\ttotal: 4m 51s\tremaining: 3m 46s\n",
      "1406:\tlearn: 0.6358026\ttotal: 4m 51s\tremaining: 3m 46s\n",
      "1407:\tlearn: 0.6356714\ttotal: 4m 51s\tremaining: 3m 46s\n",
      "1408:\tlearn: 0.6355138\ttotal: 4m 52s\tremaining: 3m 46s\n",
      "1409:\tlearn: 0.6354033\ttotal: 4m 52s\tremaining: 3m 45s\n",
      "1410:\tlearn: 0.6353049\ttotal: 4m 52s\tremaining: 3m 45s\n",
      "1411:\tlearn: 0.6352047\ttotal: 4m 52s\tremaining: 3m 45s\n",
      "1412:\tlearn: 0.6351286\ttotal: 4m 52s\tremaining: 3m 45s\n",
      "1413:\tlearn: 0.6350469\ttotal: 4m 53s\tremaining: 3m 45s\n",
      "1414:\tlearn: 0.6349525\ttotal: 4m 53s\tremaining: 3m 44s\n",
      "1415:\tlearn: 0.6348177\ttotal: 4m 53s\tremaining: 3m 44s\n",
      "1416:\tlearn: 0.6347020\ttotal: 4m 53s\tremaining: 3m 44s\n",
      "1417:\tlearn: 0.6345600\ttotal: 4m 53s\tremaining: 3m 44s\n",
      "1418:\tlearn: 0.6344244\ttotal: 4m 54s\tremaining: 3m 44s\n",
      "1419:\tlearn: 0.6343401\ttotal: 4m 54s\tremaining: 3m 43s\n",
      "1420:\tlearn: 0.6342125\ttotal: 4m 54s\tremaining: 3m 43s\n",
      "1421:\tlearn: 0.6341209\ttotal: 4m 54s\tremaining: 3m 43s\n",
      "1422:\tlearn: 0.6340268\ttotal: 4m 54s\tremaining: 3m 43s\n",
      "1423:\tlearn: 0.6339593\ttotal: 4m 55s\tremaining: 3m 43s\n",
      "1424:\tlearn: 0.6338525\ttotal: 4m 55s\tremaining: 3m 42s\n",
      "1425:\tlearn: 0.6337183\ttotal: 4m 55s\tremaining: 3m 42s\n",
      "1426:\tlearn: 0.6335859\ttotal: 4m 55s\tremaining: 3m 42s\n",
      "1427:\tlearn: 0.6334119\ttotal: 4m 55s\tremaining: 3m 42s\n",
      "1428:\tlearn: 0.6332948\ttotal: 4m 56s\tremaining: 3m 42s\n",
      "1429:\tlearn: 0.6332075\ttotal: 4m 56s\tremaining: 3m 41s\n",
      "1430:\tlearn: 0.6331055\ttotal: 4m 56s\tremaining: 3m 41s\n",
      "1431:\tlearn: 0.6330232\ttotal: 4m 56s\tremaining: 3m 41s\n",
      "1432:\tlearn: 0.6329157\ttotal: 4m 57s\tremaining: 3m 41s\n",
      "1433:\tlearn: 0.6327898\ttotal: 4m 57s\tremaining: 3m 40s\n",
      "1434:\tlearn: 0.6326446\ttotal: 4m 57s\tremaining: 3m 40s\n",
      "1435:\tlearn: 0.6325346\ttotal: 4m 57s\tremaining: 3m 40s\n",
      "1436:\tlearn: 0.6324610\ttotal: 4m 57s\tremaining: 3m 40s\n",
      "1437:\tlearn: 0.6323480\ttotal: 4m 58s\tremaining: 3m 40s\n",
      "1438:\tlearn: 0.6322428\ttotal: 4m 58s\tremaining: 3m 39s\n",
      "1439:\tlearn: 0.6321479\ttotal: 4m 58s\tremaining: 3m 39s\n",
      "1440:\tlearn: 0.6320475\ttotal: 4m 58s\tremaining: 3m 39s\n",
      "1441:\tlearn: 0.6319362\ttotal: 4m 58s\tremaining: 3m 39s\n",
      "1442:\tlearn: 0.6318347\ttotal: 4m 59s\tremaining: 3m 39s\n",
      "1443:\tlearn: 0.6316880\ttotal: 4m 59s\tremaining: 3m 38s\n",
      "1444:\tlearn: 0.6316078\ttotal: 4m 59s\tremaining: 3m 38s\n",
      "1445:\tlearn: 0.6315257\ttotal: 4m 59s\tremaining: 3m 38s\n",
      "1446:\tlearn: 0.6313902\ttotal: 5m\tremaining: 3m 38s\n",
      "1447:\tlearn: 0.6312741\ttotal: 5m\tremaining: 3m 38s\n",
      "1448:\tlearn: 0.6311639\ttotal: 5m\tremaining: 3m 37s\n",
      "1449:\tlearn: 0.6310822\ttotal: 5m\tremaining: 3m 37s\n",
      "1450:\tlearn: 0.6309350\ttotal: 5m\tremaining: 3m 37s\n",
      "1451:\tlearn: 0.6308198\ttotal: 5m 1s\tremaining: 3m 37s\n",
      "1452:\tlearn: 0.6307392\ttotal: 5m 1s\tremaining: 3m 37s\n",
      "1453:\tlearn: 0.6306258\ttotal: 5m 1s\tremaining: 3m 36s\n",
      "1454:\tlearn: 0.6304907\ttotal: 5m 1s\tremaining: 3m 36s\n",
      "1455:\tlearn: 0.6303925\ttotal: 5m 1s\tremaining: 3m 36s\n",
      "1456:\tlearn: 0.6302865\ttotal: 5m 2s\tremaining: 3m 36s\n",
      "1457:\tlearn: 0.6301517\ttotal: 5m 2s\tremaining: 3m 36s\n",
      "1458:\tlearn: 0.6300437\ttotal: 5m 2s\tremaining: 3m 35s\n",
      "1459:\tlearn: 0.6299138\ttotal: 5m 2s\tremaining: 3m 35s\n",
      "1460:\tlearn: 0.6297970\ttotal: 5m 2s\tremaining: 3m 35s\n",
      "1461:\tlearn: 0.6296821\ttotal: 5m 3s\tremaining: 3m 35s\n",
      "1462:\tlearn: 0.6295875\ttotal: 5m 3s\tremaining: 3m 35s\n",
      "1463:\tlearn: 0.6294904\ttotal: 5m 3s\tremaining: 3m 34s\n",
      "1464:\tlearn: 0.6293600\ttotal: 5m 3s\tremaining: 3m 34s\n",
      "1465:\tlearn: 0.6292531\ttotal: 5m 4s\tremaining: 3m 34s\n",
      "1466:\tlearn: 0.6291048\ttotal: 5m 4s\tremaining: 3m 34s\n",
      "1467:\tlearn: 0.6290243\ttotal: 5m 4s\tremaining: 3m 34s\n",
      "1468:\tlearn: 0.6289085\ttotal: 5m 4s\tremaining: 3m 33s\n",
      "1469:\tlearn: 0.6287369\ttotal: 5m 4s\tremaining: 3m 33s\n",
      "1470:\tlearn: 0.6286382\ttotal: 5m 5s\tremaining: 3m 33s\n",
      "1471:\tlearn: 0.6285268\ttotal: 5m 5s\tremaining: 3m 33s\n",
      "1472:\tlearn: 0.6283971\ttotal: 5m 5s\tremaining: 3m 33s\n",
      "1473:\tlearn: 0.6282896\ttotal: 5m 5s\tremaining: 3m 32s\n",
      "1474:\tlearn: 0.6281883\ttotal: 5m 5s\tremaining: 3m 32s\n",
      "1475:\tlearn: 0.6280984\ttotal: 5m 6s\tremaining: 3m 32s\n",
      "1476:\tlearn: 0.6280049\ttotal: 5m 6s\tremaining: 3m 32s\n",
      "1477:\tlearn: 0.6278633\ttotal: 5m 6s\tremaining: 3m 31s\n",
      "1478:\tlearn: 0.6277375\ttotal: 5m 6s\tremaining: 3m 31s\n",
      "1479:\tlearn: 0.6276536\ttotal: 5m 6s\tremaining: 3m 31s\n",
      "1480:\tlearn: 0.6275054\ttotal: 5m 7s\tremaining: 3m 31s\n",
      "1481:\tlearn: 0.6274137\ttotal: 5m 7s\tremaining: 3m 31s\n",
      "1482:\tlearn: 0.6273148\ttotal: 5m 7s\tremaining: 3m 30s\n",
      "1483:\tlearn: 0.6271746\ttotal: 5m 7s\tremaining: 3m 30s\n",
      "1484:\tlearn: 0.6270353\ttotal: 5m 7s\tremaining: 3m 30s\n",
      "1485:\tlearn: 0.6269299\ttotal: 5m 8s\tremaining: 3m 30s\n",
      "1486:\tlearn: 0.6268417\ttotal: 5m 8s\tremaining: 3m 30s\n",
      "1487:\tlearn: 0.6267289\ttotal: 5m 8s\tremaining: 3m 29s\n",
      "1488:\tlearn: 0.6265787\ttotal: 5m 8s\tremaining: 3m 29s\n",
      "1489:\tlearn: 0.6264301\ttotal: 5m 9s\tremaining: 3m 29s\n",
      "1490:\tlearn: 0.6263346\ttotal: 5m 9s\tremaining: 3m 29s\n",
      "1491:\tlearn: 0.6262060\ttotal: 5m 9s\tremaining: 3m 29s\n",
      "1492:\tlearn: 0.6260556\ttotal: 5m 9s\tremaining: 3m 28s\n",
      "1493:\tlearn: 0.6259446\ttotal: 5m 9s\tremaining: 3m 28s\n",
      "1494:\tlearn: 0.6258125\ttotal: 5m 10s\tremaining: 3m 28s\n",
      "1495:\tlearn: 0.6256706\ttotal: 5m 10s\tremaining: 3m 28s\n",
      "1496:\tlearn: 0.6255376\ttotal: 5m 10s\tremaining: 3m 28s\n",
      "1497:\tlearn: 0.6254380\ttotal: 5m 10s\tremaining: 3m 27s\n",
      "1498:\tlearn: 0.6253080\ttotal: 5m 10s\tremaining: 3m 27s\n",
      "1499:\tlearn: 0.6252325\ttotal: 5m 11s\tremaining: 3m 27s\n",
      "1500:\tlearn: 0.6251220\ttotal: 5m 11s\tremaining: 3m 27s\n",
      "1501:\tlearn: 0.6249875\ttotal: 5m 11s\tremaining: 3m 26s\n",
      "1502:\tlearn: 0.6248518\ttotal: 5m 11s\tremaining: 3m 26s\n",
      "1503:\tlearn: 0.6247319\ttotal: 5m 11s\tremaining: 3m 26s\n",
      "1504:\tlearn: 0.6245941\ttotal: 5m 12s\tremaining: 3m 26s\n",
      "1505:\tlearn: 0.6245008\ttotal: 5m 12s\tremaining: 3m 26s\n",
      "1506:\tlearn: 0.6243508\ttotal: 5m 12s\tremaining: 3m 25s\n",
      "1507:\tlearn: 0.6242952\ttotal: 5m 12s\tremaining: 3m 25s\n",
      "1508:\tlearn: 0.6241741\ttotal: 5m 12s\tremaining: 3m 25s\n",
      "1509:\tlearn: 0.6240471\ttotal: 5m 13s\tremaining: 3m 25s\n",
      "1510:\tlearn: 0.6239214\ttotal: 5m 13s\tremaining: 3m 25s\n",
      "1511:\tlearn: 0.6238353\ttotal: 5m 13s\tremaining: 3m 24s\n",
      "1512:\tlearn: 0.6236973\ttotal: 5m 13s\tremaining: 3m 24s\n",
      "1513:\tlearn: 0.6235830\ttotal: 5m 14s\tremaining: 3m 24s\n",
      "1514:\tlearn: 0.6234530\ttotal: 5m 14s\tremaining: 3m 24s\n",
      "1515:\tlearn: 0.6233015\ttotal: 5m 14s\tremaining: 3m 24s\n",
      "1516:\tlearn: 0.6231939\ttotal: 5m 14s\tremaining: 3m 23s\n",
      "1517:\tlearn: 0.6230751\ttotal: 5m 14s\tremaining: 3m 23s\n",
      "1518:\tlearn: 0.6230105\ttotal: 5m 15s\tremaining: 3m 23s\n",
      "1519:\tlearn: 0.6229023\ttotal: 5m 15s\tremaining: 3m 23s\n",
      "1520:\tlearn: 0.6227800\ttotal: 5m 15s\tremaining: 3m 23s\n",
      "1521:\tlearn: 0.6227082\ttotal: 5m 15s\tremaining: 3m 22s\n",
      "1522:\tlearn: 0.6226165\ttotal: 5m 15s\tremaining: 3m 22s\n",
      "1523:\tlearn: 0.6224910\ttotal: 5m 16s\tremaining: 3m 22s\n",
      "1524:\tlearn: 0.6223769\ttotal: 5m 16s\tremaining: 3m 22s\n",
      "1525:\tlearn: 0.6222449\ttotal: 5m 16s\tremaining: 3m 22s\n",
      "1526:\tlearn: 0.6221518\ttotal: 5m 16s\tremaining: 3m 21s\n",
      "1527:\tlearn: 0.6220281\ttotal: 5m 16s\tremaining: 3m 21s\n",
      "1528:\tlearn: 0.6219121\ttotal: 5m 17s\tremaining: 3m 21s\n",
      "1529:\tlearn: 0.6217744\ttotal: 5m 17s\tremaining: 3m 21s\n",
      "1530:\tlearn: 0.6216508\ttotal: 5m 17s\tremaining: 3m 21s\n",
      "1531:\tlearn: 0.6215156\ttotal: 5m 17s\tremaining: 3m 20s\n",
      "1532:\tlearn: 0.6213897\ttotal: 5m 18s\tremaining: 3m 20s\n",
      "1533:\tlearn: 0.6212886\ttotal: 5m 18s\tremaining: 3m 20s\n",
      "1534:\tlearn: 0.6211910\ttotal: 5m 18s\tremaining: 3m 20s\n",
      "1535:\tlearn: 0.6210564\ttotal: 5m 18s\tremaining: 3m 20s\n",
      "1536:\tlearn: 0.6209391\ttotal: 5m 18s\tremaining: 3m 19s\n",
      "1537:\tlearn: 0.6208276\ttotal: 5m 19s\tremaining: 3m 19s\n",
      "1538:\tlearn: 0.6206997\ttotal: 5m 19s\tremaining: 3m 19s\n",
      "1539:\tlearn: 0.6206309\ttotal: 5m 19s\tremaining: 3m 19s\n",
      "1540:\tlearn: 0.6205130\ttotal: 5m 19s\tremaining: 3m 18s\n",
      "1541:\tlearn: 0.6203665\ttotal: 5m 19s\tremaining: 3m 18s\n",
      "1542:\tlearn: 0.6202673\ttotal: 5m 20s\tremaining: 3m 18s\n",
      "1543:\tlearn: 0.6201413\ttotal: 5m 20s\tremaining: 3m 18s\n",
      "1544:\tlearn: 0.6200268\ttotal: 5m 20s\tremaining: 3m 18s\n",
      "1545:\tlearn: 0.6199200\ttotal: 5m 20s\tremaining: 3m 17s\n",
      "1546:\tlearn: 0.6198585\ttotal: 5m 20s\tremaining: 3m 17s\n",
      "1547:\tlearn: 0.6197305\ttotal: 5m 21s\tremaining: 3m 17s\n",
      "1548:\tlearn: 0.6196193\ttotal: 5m 21s\tremaining: 3m 17s\n",
      "1549:\tlearn: 0.6195257\ttotal: 5m 21s\tremaining: 3m 17s\n",
      "1550:\tlearn: 0.6194116\ttotal: 5m 21s\tremaining: 3m 16s\n",
      "1551:\tlearn: 0.6193181\ttotal: 5m 22s\tremaining: 3m 16s\n",
      "1552:\tlearn: 0.6192286\ttotal: 5m 22s\tremaining: 3m 16s\n",
      "1553:\tlearn: 0.6191468\ttotal: 5m 22s\tremaining: 3m 16s\n",
      "1554:\tlearn: 0.6190268\ttotal: 5m 22s\tremaining: 3m 16s\n",
      "1555:\tlearn: 0.6189229\ttotal: 5m 22s\tremaining: 3m 15s\n",
      "1556:\tlearn: 0.6188165\ttotal: 5m 23s\tremaining: 3m 15s\n",
      "1557:\tlearn: 0.6187041\ttotal: 5m 23s\tremaining: 3m 15s\n",
      "1558:\tlearn: 0.6186296\ttotal: 5m 23s\tremaining: 3m 15s\n",
      "1559:\tlearn: 0.6185012\ttotal: 5m 23s\tremaining: 3m 15s\n",
      "1560:\tlearn: 0.6183830\ttotal: 5m 23s\tremaining: 3m 14s\n",
      "1561:\tlearn: 0.6182912\ttotal: 5m 24s\tremaining: 3m 14s\n",
      "1562:\tlearn: 0.6181780\ttotal: 5m 24s\tremaining: 3m 14s\n",
      "1563:\tlearn: 0.6180850\ttotal: 5m 24s\tremaining: 3m 14s\n",
      "1564:\tlearn: 0.6180137\ttotal: 5m 24s\tremaining: 3m 14s\n",
      "1565:\tlearn: 0.6178924\ttotal: 5m 25s\tremaining: 3m 13s\n",
      "1566:\tlearn: 0.6178155\ttotal: 5m 25s\tremaining: 3m 13s\n",
      "1567:\tlearn: 0.6177147\ttotal: 5m 25s\tremaining: 3m 13s\n",
      "1568:\tlearn: 0.6175850\ttotal: 5m 25s\tremaining: 3m 13s\n",
      "1569:\tlearn: 0.6175024\ttotal: 5m 25s\tremaining: 3m 13s\n",
      "1570:\tlearn: 0.6174091\ttotal: 5m 26s\tremaining: 3m 12s\n",
      "1571:\tlearn: 0.6173274\ttotal: 5m 26s\tremaining: 3m 12s\n",
      "1572:\tlearn: 0.6172162\ttotal: 5m 26s\tremaining: 3m 12s\n",
      "1573:\tlearn: 0.6170763\ttotal: 5m 26s\tremaining: 3m 12s\n",
      "1574:\tlearn: 0.6169618\ttotal: 5m 26s\tremaining: 3m 11s\n",
      "1575:\tlearn: 0.6168421\ttotal: 5m 27s\tremaining: 3m 11s\n",
      "1576:\tlearn: 0.6167728\ttotal: 5m 27s\tremaining: 3m 11s\n",
      "1577:\tlearn: 0.6166425\ttotal: 5m 27s\tremaining: 3m 11s\n",
      "1578:\tlearn: 0.6165375\ttotal: 5m 27s\tremaining: 3m 11s\n",
      "1579:\tlearn: 0.6164276\ttotal: 5m 27s\tremaining: 3m 10s\n",
      "1580:\tlearn: 0.6163154\ttotal: 5m 28s\tremaining: 3m 10s\n",
      "1581:\tlearn: 0.6162324\ttotal: 5m 28s\tremaining: 3m 10s\n",
      "1582:\tlearn: 0.6161263\ttotal: 5m 28s\tremaining: 3m 10s\n",
      "1583:\tlearn: 0.6160106\ttotal: 5m 28s\tremaining: 3m 10s\n",
      "1584:\tlearn: 0.6159286\ttotal: 5m 29s\tremaining: 3m 9s\n",
      "1585:\tlearn: 0.6157982\ttotal: 5m 29s\tremaining: 3m 9s\n",
      "1586:\tlearn: 0.6156590\ttotal: 5m 29s\tremaining: 3m 9s\n",
      "1587:\tlearn: 0.6155613\ttotal: 5m 29s\tremaining: 3m 9s\n",
      "1588:\tlearn: 0.6154528\ttotal: 5m 29s\tremaining: 3m 9s\n",
      "1589:\tlearn: 0.6153745\ttotal: 5m 30s\tremaining: 3m 8s\n",
      "1590:\tlearn: 0.6152878\ttotal: 5m 30s\tremaining: 3m 8s\n",
      "1591:\tlearn: 0.6152111\ttotal: 5m 30s\tremaining: 3m 8s\n",
      "1592:\tlearn: 0.6150968\ttotal: 5m 30s\tremaining: 3m 8s\n",
      "1593:\tlearn: 0.6150013\ttotal: 5m 30s\tremaining: 3m 8s\n",
      "1594:\tlearn: 0.6149322\ttotal: 5m 31s\tremaining: 3m 7s\n",
      "1595:\tlearn: 0.6148445\ttotal: 5m 31s\tremaining: 3m 7s\n",
      "1596:\tlearn: 0.6147397\ttotal: 5m 31s\tremaining: 3m 7s\n",
      "1597:\tlearn: 0.6146731\ttotal: 5m 31s\tremaining: 3m 7s\n",
      "1598:\tlearn: 0.6145426\ttotal: 5m 31s\tremaining: 3m 7s\n",
      "1599:\tlearn: 0.6144199\ttotal: 5m 32s\tremaining: 3m 6s\n",
      "1600:\tlearn: 0.6143350\ttotal: 5m 32s\tremaining: 3m 6s\n",
      "1601:\tlearn: 0.6142195\ttotal: 5m 32s\tremaining: 3m 6s\n",
      "1602:\tlearn: 0.6140729\ttotal: 5m 32s\tremaining: 3m 6s\n",
      "1603:\tlearn: 0.6139769\ttotal: 5m 33s\tremaining: 3m 6s\n",
      "1604:\tlearn: 0.6138551\ttotal: 5m 33s\tremaining: 3m 5s\n",
      "1605:\tlearn: 0.6137427\ttotal: 5m 33s\tremaining: 3m 5s\n",
      "1606:\tlearn: 0.6135999\ttotal: 5m 33s\tremaining: 3m 5s\n",
      "1607:\tlearn: 0.6135010\ttotal: 5m 33s\tremaining: 3m 5s\n",
      "1608:\tlearn: 0.6133925\ttotal: 5m 34s\tremaining: 3m 5s\n",
      "1609:\tlearn: 0.6133106\ttotal: 5m 34s\tremaining: 3m 4s\n",
      "1610:\tlearn: 0.6131925\ttotal: 5m 34s\tremaining: 3m 4s\n",
      "1611:\tlearn: 0.6130661\ttotal: 5m 34s\tremaining: 3m 4s\n",
      "1612:\tlearn: 0.6129488\ttotal: 5m 34s\tremaining: 3m 4s\n",
      "1613:\tlearn: 0.6128166\ttotal: 5m 35s\tremaining: 3m 3s\n",
      "1614:\tlearn: 0.6127018\ttotal: 5m 35s\tremaining: 3m 3s\n",
      "1615:\tlearn: 0.6125896\ttotal: 5m 35s\tremaining: 3m 3s\n",
      "1616:\tlearn: 0.6124686\ttotal: 5m 35s\tremaining: 3m 3s\n",
      "1617:\tlearn: 0.6124119\ttotal: 5m 35s\tremaining: 3m 3s\n",
      "1618:\tlearn: 0.6122994\ttotal: 5m 36s\tremaining: 3m 2s\n",
      "1619:\tlearn: 0.6121783\ttotal: 5m 36s\tremaining: 3m 2s\n",
      "1620:\tlearn: 0.6120974\ttotal: 5m 36s\tremaining: 3m 2s\n",
      "1621:\tlearn: 0.6119789\ttotal: 5m 36s\tremaining: 3m 2s\n",
      "1622:\tlearn: 0.6118429\ttotal: 5m 37s\tremaining: 3m 2s\n",
      "1623:\tlearn: 0.6117760\ttotal: 5m 37s\tremaining: 3m 1s\n",
      "1624:\tlearn: 0.6116678\ttotal: 5m 37s\tremaining: 3m 1s\n",
      "1625:\tlearn: 0.6115892\ttotal: 5m 37s\tremaining: 3m 1s\n",
      "1626:\tlearn: 0.6114652\ttotal: 5m 37s\tremaining: 3m 1s\n",
      "1627:\tlearn: 0.6113554\ttotal: 5m 38s\tremaining: 3m 1s\n",
      "1628:\tlearn: 0.6112412\ttotal: 5m 38s\tremaining: 3m\n",
      "1629:\tlearn: 0.6111253\ttotal: 5m 38s\tremaining: 3m\n",
      "1630:\tlearn: 0.6109994\ttotal: 5m 38s\tremaining: 3m\n",
      "1631:\tlearn: 0.6109241\ttotal: 5m 38s\tremaining: 3m\n",
      "1632:\tlearn: 0.6108010\ttotal: 5m 39s\tremaining: 3m\n",
      "1633:\tlearn: 0.6106909\ttotal: 5m 39s\tremaining: 2m 59s\n",
      "1634:\tlearn: 0.6106018\ttotal: 5m 39s\tremaining: 2m 59s\n",
      "1635:\tlearn: 0.6105006\ttotal: 5m 39s\tremaining: 2m 59s\n",
      "1636:\tlearn: 0.6103918\ttotal: 5m 40s\tremaining: 2m 59s\n",
      "1637:\tlearn: 0.6103185\ttotal: 5m 40s\tremaining: 2m 59s\n",
      "1638:\tlearn: 0.6101974\ttotal: 5m 40s\tremaining: 2m 58s\n",
      "1639:\tlearn: 0.6101235\ttotal: 5m 40s\tremaining: 2m 58s\n",
      "1640:\tlearn: 0.6100091\ttotal: 5m 40s\tremaining: 2m 58s\n",
      "1641:\tlearn: 0.6099145\ttotal: 5m 41s\tremaining: 2m 58s\n",
      "1642:\tlearn: 0.6098215\ttotal: 5m 41s\tremaining: 2m 58s\n",
      "1643:\tlearn: 0.6097225\ttotal: 5m 41s\tremaining: 2m 57s\n",
      "1644:\tlearn: 0.6096122\ttotal: 5m 41s\tremaining: 2m 57s\n",
      "1645:\tlearn: 0.6095099\ttotal: 5m 41s\tremaining: 2m 57s\n",
      "1646:\tlearn: 0.6094339\ttotal: 5m 42s\tremaining: 2m 57s\n",
      "1647:\tlearn: 0.6093280\ttotal: 5m 42s\tremaining: 2m 56s\n",
      "1648:\tlearn: 0.6092383\ttotal: 5m 42s\tremaining: 2m 56s\n",
      "1649:\tlearn: 0.6091587\ttotal: 5m 42s\tremaining: 2m 56s\n",
      "1650:\tlearn: 0.6090754\ttotal: 5m 42s\tremaining: 2m 56s\n",
      "1651:\tlearn: 0.6089752\ttotal: 5m 43s\tremaining: 2m 56s\n",
      "1652:\tlearn: 0.6088554\ttotal: 5m 43s\tremaining: 2m 55s\n",
      "1653:\tlearn: 0.6087371\ttotal: 5m 43s\tremaining: 2m 55s\n",
      "1654:\tlearn: 0.6086553\ttotal: 5m 43s\tremaining: 2m 55s\n",
      "1655:\tlearn: 0.6085484\ttotal: 5m 44s\tremaining: 2m 55s\n",
      "1656:\tlearn: 0.6084382\ttotal: 5m 44s\tremaining: 2m 55s\n",
      "1657:\tlearn: 0.6083398\ttotal: 5m 44s\tremaining: 2m 54s\n",
      "1658:\tlearn: 0.6082423\ttotal: 5m 44s\tremaining: 2m 54s\n",
      "1659:\tlearn: 0.6081618\ttotal: 5m 44s\tremaining: 2m 54s\n",
      "1660:\tlearn: 0.6080386\ttotal: 5m 45s\tremaining: 2m 54s\n",
      "1661:\tlearn: 0.6079293\ttotal: 5m 45s\tremaining: 2m 54s\n",
      "1662:\tlearn: 0.6078257\ttotal: 5m 45s\tremaining: 2m 53s\n",
      "1663:\tlearn: 0.6077320\ttotal: 5m 45s\tremaining: 2m 53s\n",
      "1664:\tlearn: 0.6076154\ttotal: 5m 45s\tremaining: 2m 53s\n",
      "1665:\tlearn: 0.6075078\ttotal: 5m 46s\tremaining: 2m 53s\n",
      "1666:\tlearn: 0.6073934\ttotal: 5m 46s\tremaining: 2m 53s\n",
      "1667:\tlearn: 0.6072952\ttotal: 5m 46s\tremaining: 2m 52s\n",
      "1668:\tlearn: 0.6071897\ttotal: 5m 46s\tremaining: 2m 52s\n",
      "1669:\tlearn: 0.6071081\ttotal: 5m 47s\tremaining: 2m 52s\n",
      "1670:\tlearn: 0.6070001\ttotal: 5m 47s\tremaining: 2m 52s\n",
      "1671:\tlearn: 0.6069049\ttotal: 5m 47s\tremaining: 2m 52s\n",
      "1672:\tlearn: 0.6068116\ttotal: 5m 47s\tremaining: 2m 51s\n",
      "1673:\tlearn: 0.6067183\ttotal: 5m 47s\tremaining: 2m 51s\n",
      "1674:\tlearn: 0.6066170\ttotal: 5m 48s\tremaining: 2m 51s\n",
      "1675:\tlearn: 0.6065172\ttotal: 5m 48s\tremaining: 2m 51s\n",
      "1676:\tlearn: 0.6063873\ttotal: 5m 48s\tremaining: 2m 51s\n",
      "1677:\tlearn: 0.6063141\ttotal: 5m 48s\tremaining: 2m 50s\n",
      "1678:\tlearn: 0.6062477\ttotal: 5m 48s\tremaining: 2m 50s\n",
      "1679:\tlearn: 0.6061760\ttotal: 5m 49s\tremaining: 2m 50s\n",
      "1680:\tlearn: 0.6060506\ttotal: 5m 49s\tremaining: 2m 50s\n",
      "1681:\tlearn: 0.6059503\ttotal: 5m 49s\tremaining: 2m 50s\n",
      "1682:\tlearn: 0.6058525\ttotal: 5m 49s\tremaining: 2m 49s\n",
      "1683:\tlearn: 0.6057558\ttotal: 5m 49s\tremaining: 2m 49s\n",
      "1684:\tlearn: 0.6056770\ttotal: 5m 50s\tremaining: 2m 49s\n",
      "1685:\tlearn: 0.6055835\ttotal: 5m 50s\tremaining: 2m 49s\n",
      "1686:\tlearn: 0.6054751\ttotal: 5m 50s\tremaining: 2m 48s\n",
      "1687:\tlearn: 0.6053777\ttotal: 5m 50s\tremaining: 2m 48s\n",
      "1688:\tlearn: 0.6052426\ttotal: 5m 51s\tremaining: 2m 48s\n",
      "1689:\tlearn: 0.6051436\ttotal: 5m 51s\tremaining: 2m 48s\n",
      "1690:\tlearn: 0.6050437\ttotal: 5m 51s\tremaining: 2m 48s\n",
      "1691:\tlearn: 0.6049581\ttotal: 5m 51s\tremaining: 2m 47s\n",
      "1692:\tlearn: 0.6048671\ttotal: 5m 51s\tremaining: 2m 47s\n",
      "1693:\tlearn: 0.6047926\ttotal: 5m 52s\tremaining: 2m 47s\n",
      "1694:\tlearn: 0.6047040\ttotal: 5m 52s\tremaining: 2m 47s\n",
      "1695:\tlearn: 0.6046412\ttotal: 5m 52s\tremaining: 2m 47s\n",
      "1696:\tlearn: 0.6045591\ttotal: 5m 52s\tremaining: 2m 46s\n",
      "1697:\tlearn: 0.6044768\ttotal: 5m 52s\tremaining: 2m 46s\n",
      "1698:\tlearn: 0.6043992\ttotal: 5m 53s\tremaining: 2m 46s\n",
      "1699:\tlearn: 0.6042854\ttotal: 5m 53s\tremaining: 2m 46s\n",
      "1700:\tlearn: 0.6042024\ttotal: 5m 53s\tremaining: 2m 46s\n",
      "1701:\tlearn: 0.6041042\ttotal: 5m 53s\tremaining: 2m 45s\n",
      "1702:\tlearn: 0.6040113\ttotal: 5m 53s\tremaining: 2m 45s\n",
      "1703:\tlearn: 0.6039215\ttotal: 5m 54s\tremaining: 2m 45s\n",
      "1704:\tlearn: 0.6038318\ttotal: 5m 54s\tremaining: 2m 45s\n",
      "1705:\tlearn: 0.6037557\ttotal: 5m 54s\tremaining: 2m 45s\n",
      "1706:\tlearn: 0.6036729\ttotal: 5m 54s\tremaining: 2m 44s\n",
      "1707:\tlearn: 0.6035743\ttotal: 5m 54s\tremaining: 2m 44s\n",
      "1708:\tlearn: 0.6034948\ttotal: 5m 55s\tremaining: 2m 44s\n",
      "1709:\tlearn: 0.6034035\ttotal: 5m 55s\tremaining: 2m 44s\n",
      "1710:\tlearn: 0.6033342\ttotal: 5m 55s\tremaining: 2m 43s\n",
      "1711:\tlearn: 0.6032407\ttotal: 5m 55s\tremaining: 2m 43s\n",
      "1712:\tlearn: 0.6031427\ttotal: 5m 56s\tremaining: 2m 43s\n",
      "1713:\tlearn: 0.6030743\ttotal: 5m 56s\tremaining: 2m 43s\n",
      "1714:\tlearn: 0.6029762\ttotal: 5m 56s\tremaining: 2m 43s\n",
      "1715:\tlearn: 0.6028965\ttotal: 5m 56s\tremaining: 2m 42s\n",
      "1716:\tlearn: 0.6028279\ttotal: 5m 56s\tremaining: 2m 42s\n",
      "1717:\tlearn: 0.6027124\ttotal: 5m 57s\tremaining: 2m 42s\n",
      "1718:\tlearn: 0.6026257\ttotal: 5m 57s\tremaining: 2m 42s\n",
      "1719:\tlearn: 0.6025145\ttotal: 5m 57s\tremaining: 2m 42s\n",
      "1720:\tlearn: 0.6024362\ttotal: 5m 57s\tremaining: 2m 41s\n",
      "1721:\tlearn: 0.6023612\ttotal: 5m 57s\tremaining: 2m 41s\n",
      "1722:\tlearn: 0.6022775\ttotal: 5m 58s\tremaining: 2m 41s\n",
      "1723:\tlearn: 0.6022094\ttotal: 5m 58s\tremaining: 2m 41s\n",
      "1724:\tlearn: 0.6021412\ttotal: 5m 58s\tremaining: 2m 41s\n",
      "1725:\tlearn: 0.6020365\ttotal: 5m 58s\tremaining: 2m 40s\n",
      "1726:\tlearn: 0.6019503\ttotal: 5m 59s\tremaining: 2m 40s\n",
      "1727:\tlearn: 0.6018877\ttotal: 5m 59s\tremaining: 2m 40s\n",
      "1728:\tlearn: 0.6017823\ttotal: 5m 59s\tremaining: 2m 40s\n",
      "1729:\tlearn: 0.6016764\ttotal: 5m 59s\tremaining: 2m 40s\n",
      "1730:\tlearn: 0.6016052\ttotal: 5m 59s\tremaining: 2m 39s\n",
      "1731:\tlearn: 0.6014964\ttotal: 6m\tremaining: 2m 39s\n",
      "1732:\tlearn: 0.6014157\ttotal: 6m\tremaining: 2m 39s\n",
      "1733:\tlearn: 0.6013244\ttotal: 6m\tremaining: 2m 39s\n",
      "1734:\tlearn: 0.6012414\ttotal: 6m\tremaining: 2m 39s\n",
      "1735:\tlearn: 0.6011896\ttotal: 6m\tremaining: 2m 38s\n",
      "1736:\tlearn: 0.6010902\ttotal: 6m 1s\tremaining: 2m 38s\n",
      "1737:\tlearn: 0.6010060\ttotal: 6m 1s\tremaining: 2m 38s\n",
      "1738:\tlearn: 0.6008964\ttotal: 6m 1s\tremaining: 2m 38s\n",
      "1739:\tlearn: 0.6008145\ttotal: 6m 1s\tremaining: 2m 38s\n",
      "1740:\tlearn: 0.6007364\ttotal: 6m 2s\tremaining: 2m 37s\n",
      "1741:\tlearn: 0.6006336\ttotal: 6m 2s\tremaining: 2m 37s\n",
      "1742:\tlearn: 0.6005437\ttotal: 6m 2s\tremaining: 2m 37s\n",
      "1743:\tlearn: 0.6004887\ttotal: 6m 2s\tremaining: 2m 37s\n",
      "1744:\tlearn: 0.6003995\ttotal: 6m 2s\tremaining: 2m 37s\n",
      "1745:\tlearn: 0.6003139\ttotal: 6m 3s\tremaining: 2m 36s\n",
      "1746:\tlearn: 0.6002148\ttotal: 6m 3s\tremaining: 2m 36s\n",
      "1747:\tlearn: 0.6001341\ttotal: 6m 3s\tremaining: 2m 36s\n",
      "1748:\tlearn: 0.6000222\ttotal: 6m 3s\tremaining: 2m 36s\n",
      "1749:\tlearn: 0.5999458\ttotal: 6m 3s\tremaining: 2m 35s\n",
      "1750:\tlearn: 0.5998282\ttotal: 6m 4s\tremaining: 2m 35s\n",
      "1751:\tlearn: 0.5997253\ttotal: 6m 4s\tremaining: 2m 35s\n",
      "1752:\tlearn: 0.5996470\ttotal: 6m 4s\tremaining: 2m 35s\n",
      "1753:\tlearn: 0.5995302\ttotal: 6m 4s\tremaining: 2m 35s\n",
      "1754:\tlearn: 0.5994422\ttotal: 6m 5s\tremaining: 2m 34s\n",
      "1755:\tlearn: 0.5993355\ttotal: 6m 5s\tremaining: 2m 34s\n",
      "1756:\tlearn: 0.5992184\ttotal: 6m 5s\tremaining: 2m 34s\n",
      "1757:\tlearn: 0.5991768\ttotal: 6m 5s\tremaining: 2m 34s\n",
      "1758:\tlearn: 0.5990934\ttotal: 6m 5s\tremaining: 2m 34s\n",
      "1759:\tlearn: 0.5989964\ttotal: 6m 6s\tremaining: 2m 33s\n",
      "1760:\tlearn: 0.5989077\ttotal: 6m 6s\tremaining: 2m 33s\n",
      "1761:\tlearn: 0.5988133\ttotal: 6m 6s\tremaining: 2m 33s\n",
      "1762:\tlearn: 0.5987109\ttotal: 6m 6s\tremaining: 2m 33s\n",
      "1763:\tlearn: 0.5986372\ttotal: 6m 6s\tremaining: 2m 33s\n",
      "1764:\tlearn: 0.5985766\ttotal: 6m 7s\tremaining: 2m 32s\n",
      "1765:\tlearn: 0.5984771\ttotal: 6m 7s\tremaining: 2m 32s\n",
      "1766:\tlearn: 0.5983964\ttotal: 6m 7s\tremaining: 2m 32s\n",
      "1767:\tlearn: 0.5983059\ttotal: 6m 7s\tremaining: 2m 32s\n",
      "1768:\tlearn: 0.5982241\ttotal: 6m 7s\tremaining: 2m 32s\n",
      "1769:\tlearn: 0.5981327\ttotal: 6m 8s\tremaining: 2m 31s\n",
      "1770:\tlearn: 0.5980164\ttotal: 6m 8s\tremaining: 2m 31s\n",
      "1771:\tlearn: 0.5979249\ttotal: 6m 8s\tremaining: 2m 31s\n",
      "1772:\tlearn: 0.5978371\ttotal: 6m 8s\tremaining: 2m 31s\n",
      "1773:\tlearn: 0.5977715\ttotal: 6m 8s\tremaining: 2m 31s\n",
      "1774:\tlearn: 0.5976707\ttotal: 6m 9s\tremaining: 2m 30s\n",
      "1775:\tlearn: 0.5975944\ttotal: 6m 9s\tremaining: 2m 30s\n",
      "1776:\tlearn: 0.5975220\ttotal: 6m 9s\tremaining: 2m 30s\n",
      "1777:\tlearn: 0.5974487\ttotal: 6m 9s\tremaining: 2m 30s\n",
      "1778:\tlearn: 0.5973772\ttotal: 6m 10s\tremaining: 2m 29s\n",
      "1779:\tlearn: 0.5972960\ttotal: 6m 10s\tremaining: 2m 29s\n",
      "1780:\tlearn: 0.5972393\ttotal: 6m 10s\tremaining: 2m 29s\n",
      "1781:\tlearn: 0.5971624\ttotal: 6m 10s\tremaining: 2m 29s\n",
      "1782:\tlearn: 0.5970823\ttotal: 6m 10s\tremaining: 2m 29s\n",
      "1783:\tlearn: 0.5970109\ttotal: 6m 11s\tremaining: 2m 28s\n",
      "1784:\tlearn: 0.5969255\ttotal: 6m 11s\tremaining: 2m 28s\n",
      "1785:\tlearn: 0.5968351\ttotal: 6m 11s\tremaining: 2m 28s\n",
      "1786:\tlearn: 0.5967466\ttotal: 6m 11s\tremaining: 2m 28s\n",
      "1787:\tlearn: 0.5966798\ttotal: 6m 11s\tremaining: 2m 28s\n",
      "1788:\tlearn: 0.5965946\ttotal: 6m 12s\tremaining: 2m 27s\n",
      "1789:\tlearn: 0.5965415\ttotal: 6m 12s\tremaining: 2m 27s\n",
      "1790:\tlearn: 0.5964379\ttotal: 6m 12s\tremaining: 2m 27s\n",
      "1791:\tlearn: 0.5963678\ttotal: 6m 12s\tremaining: 2m 27s\n",
      "1792:\tlearn: 0.5962993\ttotal: 6m 12s\tremaining: 2m 27s\n",
      "1793:\tlearn: 0.5961744\ttotal: 6m 13s\tremaining: 2m 26s\n",
      "1794:\tlearn: 0.5960885\ttotal: 6m 13s\tremaining: 2m 26s\n",
      "1795:\tlearn: 0.5960029\ttotal: 6m 13s\tremaining: 2m 26s\n",
      "1796:\tlearn: 0.5959180\ttotal: 6m 13s\tremaining: 2m 26s\n",
      "1797:\tlearn: 0.5958317\ttotal: 6m 13s\tremaining: 2m 26s\n",
      "1798:\tlearn: 0.5957398\ttotal: 6m 14s\tremaining: 2m 25s\n",
      "1799:\tlearn: 0.5956874\ttotal: 6m 14s\tremaining: 2m 25s\n",
      "1800:\tlearn: 0.5955765\ttotal: 6m 14s\tremaining: 2m 25s\n",
      "1801:\tlearn: 0.5954755\ttotal: 6m 14s\tremaining: 2m 25s\n",
      "1802:\tlearn: 0.5953477\ttotal: 6m 15s\tremaining: 2m 24s\n",
      "1803:\tlearn: 0.5952478\ttotal: 6m 15s\tremaining: 2m 24s\n",
      "1804:\tlearn: 0.5951490\ttotal: 6m 15s\tremaining: 2m 24s\n",
      "1805:\tlearn: 0.5950894\ttotal: 6m 15s\tremaining: 2m 24s\n",
      "1806:\tlearn: 0.5949940\ttotal: 6m 15s\tremaining: 2m 24s\n",
      "1807:\tlearn: 0.5949281\ttotal: 6m 16s\tremaining: 2m 23s\n",
      "1808:\tlearn: 0.5948609\ttotal: 6m 16s\tremaining: 2m 23s\n",
      "1809:\tlearn: 0.5947510\ttotal: 6m 16s\tremaining: 2m 23s\n",
      "1810:\tlearn: 0.5946578\ttotal: 6m 16s\tremaining: 2m 23s\n",
      "1811:\tlearn: 0.5945744\ttotal: 6m 16s\tremaining: 2m 23s\n",
      "1812:\tlearn: 0.5944844\ttotal: 6m 17s\tremaining: 2m 22s\n",
      "1813:\tlearn: 0.5943911\ttotal: 6m 17s\tremaining: 2m 22s\n",
      "1814:\tlearn: 0.5943089\ttotal: 6m 17s\tremaining: 2m 22s\n",
      "1815:\tlearn: 0.5942429\ttotal: 6m 17s\tremaining: 2m 22s\n",
      "1816:\tlearn: 0.5941591\ttotal: 6m 17s\tremaining: 2m 22s\n",
      "1817:\tlearn: 0.5940804\ttotal: 6m 18s\tremaining: 2m 21s\n",
      "1818:\tlearn: 0.5940203\ttotal: 6m 18s\tremaining: 2m 21s\n",
      "1819:\tlearn: 0.5939259\ttotal: 6m 18s\tremaining: 2m 21s\n",
      "1820:\tlearn: 0.5938425\ttotal: 6m 18s\tremaining: 2m 21s\n",
      "1821:\tlearn: 0.5937339\ttotal: 6m 18s\tremaining: 2m 21s\n",
      "1822:\tlearn: 0.5936244\ttotal: 6m 19s\tremaining: 2m 20s\n",
      "1823:\tlearn: 0.5935232\ttotal: 6m 19s\tremaining: 2m 20s\n",
      "1824:\tlearn: 0.5934431\ttotal: 6m 19s\tremaining: 2m 20s\n",
      "1825:\tlearn: 0.5933457\ttotal: 6m 19s\tremaining: 2m 20s\n",
      "1826:\tlearn: 0.5932473\ttotal: 6m 20s\tremaining: 2m 20s\n",
      "1827:\tlearn: 0.5931796\ttotal: 6m 20s\tremaining: 2m 19s\n",
      "1828:\tlearn: 0.5931125\ttotal: 6m 20s\tremaining: 2m 19s\n",
      "1829:\tlearn: 0.5930052\ttotal: 6m 20s\tremaining: 2m 19s\n",
      "1830:\tlearn: 0.5929247\ttotal: 6m 20s\tremaining: 2m 19s\n",
      "1831:\tlearn: 0.5928291\ttotal: 6m 21s\tremaining: 2m 18s\n",
      "1832:\tlearn: 0.5927323\ttotal: 6m 21s\tremaining: 2m 18s\n",
      "1833:\tlearn: 0.5926255\ttotal: 6m 21s\tremaining: 2m 18s\n",
      "1834:\tlearn: 0.5925821\ttotal: 6m 21s\tremaining: 2m 18s\n",
      "1835:\tlearn: 0.5924931\ttotal: 6m 21s\tremaining: 2m 18s\n",
      "1836:\tlearn: 0.5924052\ttotal: 6m 22s\tremaining: 2m 17s\n",
      "1837:\tlearn: 0.5923183\ttotal: 6m 22s\tremaining: 2m 17s\n",
      "1838:\tlearn: 0.5921945\ttotal: 6m 22s\tremaining: 2m 17s\n",
      "1839:\tlearn: 0.5921066\ttotal: 6m 22s\tremaining: 2m 17s\n",
      "1840:\tlearn: 0.5919853\ttotal: 6m 23s\tremaining: 2m 17s\n",
      "1841:\tlearn: 0.5919113\ttotal: 6m 23s\tremaining: 2m 16s\n",
      "1842:\tlearn: 0.5918357\ttotal: 6m 23s\tremaining: 2m 16s\n",
      "1843:\tlearn: 0.5917171\ttotal: 6m 23s\tremaining: 2m 16s\n",
      "1844:\tlearn: 0.5916353\ttotal: 6m 23s\tremaining: 2m 16s\n",
      "1845:\tlearn: 0.5915271\ttotal: 6m 24s\tremaining: 2m 16s\n",
      "1846:\tlearn: 0.5914349\ttotal: 6m 24s\tremaining: 2m 15s\n",
      "1847:\tlearn: 0.5913273\ttotal: 6m 24s\tremaining: 2m 15s\n",
      "1848:\tlearn: 0.5912244\ttotal: 6m 24s\tremaining: 2m 15s\n",
      "1849:\tlearn: 0.5911457\ttotal: 6m 24s\tremaining: 2m 15s\n",
      "1850:\tlearn: 0.5910597\ttotal: 6m 25s\tremaining: 2m 15s\n",
      "1851:\tlearn: 0.5909664\ttotal: 6m 25s\tremaining: 2m 14s\n",
      "1852:\tlearn: 0.5908963\ttotal: 6m 25s\tremaining: 2m 14s\n",
      "1853:\tlearn: 0.5907932\ttotal: 6m 25s\tremaining: 2m 14s\n",
      "1854:\tlearn: 0.5907034\ttotal: 6m 26s\tremaining: 2m 14s\n",
      "1855:\tlearn: 0.5906349\ttotal: 6m 26s\tremaining: 2m 14s\n",
      "1856:\tlearn: 0.5905545\ttotal: 6m 26s\tremaining: 2m 13s\n",
      "1857:\tlearn: 0.5904750\ttotal: 6m 26s\tremaining: 2m 13s\n",
      "1858:\tlearn: 0.5903591\ttotal: 6m 26s\tremaining: 2m 13s\n",
      "1859:\tlearn: 0.5902690\ttotal: 6m 27s\tremaining: 2m 13s\n",
      "1860:\tlearn: 0.5902045\ttotal: 6m 27s\tremaining: 2m 12s\n",
      "1861:\tlearn: 0.5901210\ttotal: 6m 27s\tremaining: 2m 12s\n",
      "1862:\tlearn: 0.5900542\ttotal: 6m 27s\tremaining: 2m 12s\n",
      "1863:\tlearn: 0.5899803\ttotal: 6m 27s\tremaining: 2m 12s\n",
      "1864:\tlearn: 0.5898908\ttotal: 6m 28s\tremaining: 2m 12s\n",
      "1865:\tlearn: 0.5898291\ttotal: 6m 28s\tremaining: 2m 11s\n",
      "1866:\tlearn: 0.5897296\ttotal: 6m 28s\tremaining: 2m 11s\n",
      "1867:\tlearn: 0.5896379\ttotal: 6m 28s\tremaining: 2m 11s\n",
      "1868:\tlearn: 0.5895483\ttotal: 6m 28s\tremaining: 2m 11s\n",
      "1869:\tlearn: 0.5894877\ttotal: 6m 29s\tremaining: 2m 11s\n",
      "1870:\tlearn: 0.5894027\ttotal: 6m 29s\tremaining: 2m 10s\n",
      "1871:\tlearn: 0.5893144\ttotal: 6m 29s\tremaining: 2m 10s\n",
      "1872:\tlearn: 0.5892664\ttotal: 6m 29s\tremaining: 2m 10s\n",
      "1873:\tlearn: 0.5891888\ttotal: 6m 29s\tremaining: 2m 10s\n",
      "1874:\tlearn: 0.5890965\ttotal: 6m 30s\tremaining: 2m 10s\n",
      "1875:\tlearn: 0.5890016\ttotal: 6m 30s\tremaining: 2m 9s\n",
      "1876:\tlearn: 0.5888976\ttotal: 6m 30s\tremaining: 2m 9s\n",
      "1877:\tlearn: 0.5888102\ttotal: 6m 30s\tremaining: 2m 9s\n",
      "1878:\tlearn: 0.5887252\ttotal: 6m 31s\tremaining: 2m 9s\n",
      "1879:\tlearn: 0.5886383\ttotal: 6m 31s\tremaining: 2m 9s\n",
      "1880:\tlearn: 0.5885614\ttotal: 6m 31s\tremaining: 2m 8s\n",
      "1881:\tlearn: 0.5884849\ttotal: 6m 31s\tremaining: 2m 8s\n",
      "1882:\tlearn: 0.5883909\ttotal: 6m 31s\tremaining: 2m 8s\n",
      "1883:\tlearn: 0.5883235\ttotal: 6m 32s\tremaining: 2m 8s\n",
      "1884:\tlearn: 0.5881969\ttotal: 6m 32s\tremaining: 2m 7s\n",
      "1885:\tlearn: 0.5881390\ttotal: 6m 32s\tremaining: 2m 7s\n",
      "1886:\tlearn: 0.5880593\ttotal: 6m 32s\tremaining: 2m 7s\n",
      "1887:\tlearn: 0.5879833\ttotal: 6m 32s\tremaining: 2m 7s\n",
      "1888:\tlearn: 0.5879211\ttotal: 6m 33s\tremaining: 2m 7s\n",
      "1889:\tlearn: 0.5878496\ttotal: 6m 33s\tremaining: 2m 6s\n",
      "1890:\tlearn: 0.5877588\ttotal: 6m 33s\tremaining: 2m 6s\n",
      "1891:\tlearn: 0.5876502\ttotal: 6m 33s\tremaining: 2m 6s\n",
      "1892:\tlearn: 0.5875742\ttotal: 6m 33s\tremaining: 2m 6s\n",
      "1893:\tlearn: 0.5875142\ttotal: 6m 34s\tremaining: 2m 6s\n",
      "1894:\tlearn: 0.5874712\ttotal: 6m 34s\tremaining: 2m 5s\n",
      "1895:\tlearn: 0.5874231\ttotal: 6m 34s\tremaining: 2m 5s\n",
      "1896:\tlearn: 0.5873650\ttotal: 6m 34s\tremaining: 2m 5s\n",
      "1897:\tlearn: 0.5872939\ttotal: 6m 34s\tremaining: 2m 5s\n",
      "1898:\tlearn: 0.5872072\ttotal: 6m 35s\tremaining: 2m 5s\n",
      "1899:\tlearn: 0.5871173\ttotal: 6m 35s\tremaining: 2m 4s\n",
      "1900:\tlearn: 0.5870342\ttotal: 6m 35s\tremaining: 2m 4s\n",
      "1901:\tlearn: 0.5869561\ttotal: 6m 35s\tremaining: 2m 4s\n",
      "1902:\tlearn: 0.5868786\ttotal: 6m 36s\tremaining: 2m 4s\n",
      "1903:\tlearn: 0.5868114\ttotal: 6m 36s\tremaining: 2m 4s\n",
      "1904:\tlearn: 0.5867276\ttotal: 6m 36s\tremaining: 2m 3s\n",
      "1905:\tlearn: 0.5866461\ttotal: 6m 36s\tremaining: 2m 3s\n",
      "1906:\tlearn: 0.5865529\ttotal: 6m 36s\tremaining: 2m 3s\n",
      "1907:\tlearn: 0.5864426\ttotal: 6m 37s\tremaining: 2m 3s\n",
      "1908:\tlearn: 0.5864013\ttotal: 6m 37s\tremaining: 2m 2s\n",
      "1909:\tlearn: 0.5863088\ttotal: 6m 37s\tremaining: 2m 2s\n",
      "1910:\tlearn: 0.5862420\ttotal: 6m 37s\tremaining: 2m 2s\n",
      "1911:\tlearn: 0.5861483\ttotal: 6m 37s\tremaining: 2m 2s\n",
      "1912:\tlearn: 0.5860969\ttotal: 6m 38s\tremaining: 2m 2s\n",
      "1913:\tlearn: 0.5860362\ttotal: 6m 38s\tremaining: 2m 1s\n",
      "1914:\tlearn: 0.5859262\ttotal: 6m 38s\tremaining: 2m 1s\n",
      "1915:\tlearn: 0.5858403\ttotal: 6m 38s\tremaining: 2m 1s\n",
      "1916:\tlearn: 0.5857555\ttotal: 6m 38s\tremaining: 2m 1s\n",
      "1917:\tlearn: 0.5856726\ttotal: 6m 39s\tremaining: 2m 1s\n",
      "1918:\tlearn: 0.5856070\ttotal: 6m 39s\tremaining: 2m\n",
      "1919:\tlearn: 0.5855219\ttotal: 6m 39s\tremaining: 2m\n",
      "1920:\tlearn: 0.5854597\ttotal: 6m 39s\tremaining: 2m\n",
      "1921:\tlearn: 0.5853588\ttotal: 6m 40s\tremaining: 2m\n",
      "1922:\tlearn: 0.5852938\ttotal: 6m 40s\tremaining: 2m\n",
      "1923:\tlearn: 0.5852262\ttotal: 6m 40s\tremaining: 1m 59s\n",
      "1924:\tlearn: 0.5851379\ttotal: 6m 40s\tremaining: 1m 59s\n",
      "1925:\tlearn: 0.5850548\ttotal: 6m 40s\tremaining: 1m 59s\n",
      "1926:\tlearn: 0.5849502\ttotal: 6m 41s\tremaining: 1m 59s\n",
      "1927:\tlearn: 0.5848632\ttotal: 6m 41s\tremaining: 1m 59s\n",
      "1928:\tlearn: 0.5847589\ttotal: 6m 41s\tremaining: 1m 58s\n",
      "1929:\tlearn: 0.5846746\ttotal: 6m 41s\tremaining: 1m 58s\n",
      "1930:\tlearn: 0.5845893\ttotal: 6m 41s\tremaining: 1m 58s\n",
      "1931:\tlearn: 0.5845225\ttotal: 6m 42s\tremaining: 1m 58s\n",
      "1932:\tlearn: 0.5844350\ttotal: 6m 42s\tremaining: 1m 58s\n",
      "1933:\tlearn: 0.5843420\ttotal: 6m 42s\tremaining: 1m 57s\n",
      "1934:\tlearn: 0.5842774\ttotal: 6m 42s\tremaining: 1m 57s\n",
      "1935:\tlearn: 0.5841950\ttotal: 6m 42s\tremaining: 1m 57s\n",
      "1936:\tlearn: 0.5840932\ttotal: 6m 43s\tremaining: 1m 57s\n",
      "1937:\tlearn: 0.5840119\ttotal: 6m 43s\tremaining: 1m 56s\n",
      "1938:\tlearn: 0.5839399\ttotal: 6m 43s\tremaining: 1m 56s\n",
      "1939:\tlearn: 0.5838535\ttotal: 6m 43s\tremaining: 1m 56s\n",
      "1940:\tlearn: 0.5837621\ttotal: 6m 43s\tremaining: 1m 56s\n",
      "1941:\tlearn: 0.5836617\ttotal: 6m 44s\tremaining: 1m 56s\n",
      "1942:\tlearn: 0.5835479\ttotal: 6m 44s\tremaining: 1m 55s\n",
      "1943:\tlearn: 0.5834617\ttotal: 6m 44s\tremaining: 1m 55s\n",
      "1944:\tlearn: 0.5833701\ttotal: 6m 44s\tremaining: 1m 55s\n",
      "1945:\tlearn: 0.5833014\ttotal: 6m 45s\tremaining: 1m 55s\n",
      "1946:\tlearn: 0.5832266\ttotal: 6m 45s\tremaining: 1m 55s\n",
      "1947:\tlearn: 0.5831643\ttotal: 6m 45s\tremaining: 1m 54s\n",
      "1948:\tlearn: 0.5830970\ttotal: 6m 45s\tremaining: 1m 54s\n",
      "1949:\tlearn: 0.5830336\ttotal: 6m 45s\tremaining: 1m 54s\n",
      "1950:\tlearn: 0.5829566\ttotal: 6m 46s\tremaining: 1m 54s\n",
      "1951:\tlearn: 0.5828977\ttotal: 6m 46s\tremaining: 1m 54s\n",
      "1952:\tlearn: 0.5828105\ttotal: 6m 46s\tremaining: 1m 53s\n",
      "1953:\tlearn: 0.5827518\ttotal: 6m 46s\tremaining: 1m 53s\n",
      "1954:\tlearn: 0.5826716\ttotal: 6m 46s\tremaining: 1m 53s\n",
      "1955:\tlearn: 0.5826126\ttotal: 6m 47s\tremaining: 1m 53s\n",
      "1956:\tlearn: 0.5825411\ttotal: 6m 47s\tremaining: 1m 53s\n",
      "1957:\tlearn: 0.5824740\ttotal: 6m 47s\tremaining: 1m 52s\n",
      "1958:\tlearn: 0.5823978\ttotal: 6m 47s\tremaining: 1m 52s\n",
      "1959:\tlearn: 0.5822993\ttotal: 6m 47s\tremaining: 1m 52s\n",
      "1960:\tlearn: 0.5822032\ttotal: 6m 48s\tremaining: 1m 52s\n",
      "1961:\tlearn: 0.5821363\ttotal: 6m 48s\tremaining: 1m 51s\n",
      "1962:\tlearn: 0.5820812\ttotal: 6m 48s\tremaining: 1m 51s\n",
      "1963:\tlearn: 0.5820253\ttotal: 6m 48s\tremaining: 1m 51s\n",
      "1964:\tlearn: 0.5819500\ttotal: 6m 49s\tremaining: 1m 51s\n",
      "1965:\tlearn: 0.5818414\ttotal: 6m 49s\tremaining: 1m 51s\n",
      "1966:\tlearn: 0.5817789\ttotal: 6m 49s\tremaining: 1m 50s\n",
      "1967:\tlearn: 0.5817011\ttotal: 6m 49s\tremaining: 1m 50s\n",
      "1968:\tlearn: 0.5816191\ttotal: 6m 49s\tremaining: 1m 50s\n",
      "1969:\tlearn: 0.5815360\ttotal: 6m 50s\tremaining: 1m 50s\n",
      "1970:\tlearn: 0.5814416\ttotal: 6m 50s\tremaining: 1m 50s\n",
      "1971:\tlearn: 0.5813434\ttotal: 6m 50s\tremaining: 1m 49s\n",
      "1972:\tlearn: 0.5812855\ttotal: 6m 50s\tremaining: 1m 49s\n",
      "1973:\tlearn: 0.5812234\ttotal: 6m 50s\tremaining: 1m 49s\n",
      "1974:\tlearn: 0.5811432\ttotal: 6m 51s\tremaining: 1m 49s\n",
      "1975:\tlearn: 0.5810837\ttotal: 6m 51s\tremaining: 1m 49s\n",
      "1976:\tlearn: 0.5810406\ttotal: 6m 51s\tremaining: 1m 48s\n",
      "1977:\tlearn: 0.5809663\ttotal: 6m 51s\tremaining: 1m 48s\n",
      "1978:\tlearn: 0.5808868\ttotal: 6m 52s\tremaining: 1m 48s\n",
      "1979:\tlearn: 0.5807793\ttotal: 6m 52s\tremaining: 1m 48s\n",
      "1980:\tlearn: 0.5806660\ttotal: 6m 52s\tremaining: 1m 48s\n",
      "1981:\tlearn: 0.5806097\ttotal: 6m 52s\tremaining: 1m 47s\n",
      "1982:\tlearn: 0.5805383\ttotal: 6m 52s\tremaining: 1m 47s\n",
      "1983:\tlearn: 0.5804714\ttotal: 6m 53s\tremaining: 1m 47s\n",
      "1984:\tlearn: 0.5804065\ttotal: 6m 53s\tremaining: 1m 47s\n",
      "1985:\tlearn: 0.5803219\ttotal: 6m 53s\tremaining: 1m 47s\n",
      "1986:\tlearn: 0.5802133\ttotal: 6m 53s\tremaining: 1m 46s\n",
      "1987:\tlearn: 0.5801189\ttotal: 6m 53s\tremaining: 1m 46s\n",
      "1988:\tlearn: 0.5800479\ttotal: 6m 54s\tremaining: 1m 46s\n",
      "1989:\tlearn: 0.5799695\ttotal: 6m 54s\tremaining: 1m 46s\n",
      "1990:\tlearn: 0.5798880\ttotal: 6m 54s\tremaining: 1m 45s\n",
      "1991:\tlearn: 0.5797963\ttotal: 6m 54s\tremaining: 1m 45s\n",
      "1992:\tlearn: 0.5797479\ttotal: 6m 54s\tremaining: 1m 45s\n",
      "1993:\tlearn: 0.5796699\ttotal: 6m 55s\tremaining: 1m 45s\n",
      "1994:\tlearn: 0.5795822\ttotal: 6m 55s\tremaining: 1m 45s\n",
      "1995:\tlearn: 0.5795002\ttotal: 6m 55s\tremaining: 1m 44s\n",
      "1996:\tlearn: 0.5793961\ttotal: 6m 55s\tremaining: 1m 44s\n",
      "1997:\tlearn: 0.5793383\ttotal: 6m 56s\tremaining: 1m 44s\n",
      "1998:\tlearn: 0.5792526\ttotal: 6m 56s\tremaining: 1m 44s\n",
      "1999:\tlearn: 0.5791685\ttotal: 6m 56s\tremaining: 1m 44s\n",
      "2000:\tlearn: 0.5791014\ttotal: 6m 56s\tremaining: 1m 43s\n",
      "2001:\tlearn: 0.5790398\ttotal: 6m 56s\tremaining: 1m 43s\n",
      "2002:\tlearn: 0.5789828\ttotal: 6m 57s\tremaining: 1m 43s\n",
      "2003:\tlearn: 0.5788817\ttotal: 6m 57s\tremaining: 1m 43s\n",
      "2004:\tlearn: 0.5788103\ttotal: 6m 57s\tremaining: 1m 43s\n",
      "2005:\tlearn: 0.5787540\ttotal: 6m 57s\tremaining: 1m 42s\n",
      "2006:\tlearn: 0.5786770\ttotal: 6m 57s\tremaining: 1m 42s\n",
      "2007:\tlearn: 0.5785728\ttotal: 6m 58s\tremaining: 1m 42s\n",
      "2008:\tlearn: 0.5784794\ttotal: 6m 58s\tremaining: 1m 42s\n",
      "2009:\tlearn: 0.5784036\ttotal: 6m 58s\tremaining: 1m 42s\n",
      "2010:\tlearn: 0.5783312\ttotal: 6m 58s\tremaining: 1m 41s\n",
      "2011:\tlearn: 0.5782611\ttotal: 6m 58s\tremaining: 1m 41s\n",
      "2012:\tlearn: 0.5781738\ttotal: 6m 59s\tremaining: 1m 41s\n",
      "2013:\tlearn: 0.5781090\ttotal: 6m 59s\tremaining: 1m 41s\n",
      "2014:\tlearn: 0.5780292\ttotal: 6m 59s\tremaining: 1m 41s\n",
      "2015:\tlearn: 0.5779633\ttotal: 6m 59s\tremaining: 1m 40s\n",
      "2016:\tlearn: 0.5778635\ttotal: 7m\tremaining: 1m 40s\n",
      "2017:\tlearn: 0.5778037\ttotal: 7m\tremaining: 1m 40s\n",
      "2018:\tlearn: 0.5777347\ttotal: 7m\tremaining: 1m 40s\n",
      "2019:\tlearn: 0.5776624\ttotal: 7m\tremaining: 1m 39s\n",
      "2020:\tlearn: 0.5775915\ttotal: 7m\tremaining: 1m 39s\n",
      "2021:\tlearn: 0.5775083\ttotal: 7m 1s\tremaining: 1m 39s\n",
      "2022:\tlearn: 0.5774442\ttotal: 7m 1s\tremaining: 1m 39s\n",
      "2023:\tlearn: 0.5773611\ttotal: 7m 1s\tremaining: 1m 39s\n",
      "2024:\tlearn: 0.5772672\ttotal: 7m 1s\tremaining: 1m 38s\n",
      "2025:\tlearn: 0.5771995\ttotal: 7m 1s\tremaining: 1m 38s\n",
      "2026:\tlearn: 0.5771447\ttotal: 7m 2s\tremaining: 1m 38s\n",
      "2027:\tlearn: 0.5770594\ttotal: 7m 2s\tremaining: 1m 38s\n",
      "2028:\tlearn: 0.5769932\ttotal: 7m 2s\tremaining: 1m 38s\n",
      "2029:\tlearn: 0.5768985\ttotal: 7m 2s\tremaining: 1m 37s\n",
      "2030:\tlearn: 0.5768162\ttotal: 7m 3s\tremaining: 1m 37s\n",
      "2031:\tlearn: 0.5767281\ttotal: 7m 3s\tremaining: 1m 37s\n",
      "2032:\tlearn: 0.5766438\ttotal: 7m 3s\tremaining: 1m 37s\n",
      "2033:\tlearn: 0.5765673\ttotal: 7m 3s\tremaining: 1m 37s\n",
      "2034:\tlearn: 0.5764978\ttotal: 7m 3s\tremaining: 1m 36s\n",
      "2035:\tlearn: 0.5764197\ttotal: 7m 4s\tremaining: 1m 36s\n",
      "2036:\tlearn: 0.5763257\ttotal: 7m 4s\tremaining: 1m 36s\n",
      "2037:\tlearn: 0.5762626\ttotal: 7m 4s\tremaining: 1m 36s\n",
      "2038:\tlearn: 0.5762107\ttotal: 7m 4s\tremaining: 1m 36s\n",
      "2039:\tlearn: 0.5761624\ttotal: 7m 4s\tremaining: 1m 35s\n",
      "2040:\tlearn: 0.5760980\ttotal: 7m 5s\tremaining: 1m 35s\n",
      "2041:\tlearn: 0.5760164\ttotal: 7m 5s\tremaining: 1m 35s\n",
      "2042:\tlearn: 0.5759358\ttotal: 7m 5s\tremaining: 1m 35s\n",
      "2043:\tlearn: 0.5758601\ttotal: 7m 5s\tremaining: 1m 34s\n",
      "2044:\tlearn: 0.5758215\ttotal: 7m 5s\tremaining: 1m 34s\n",
      "2045:\tlearn: 0.5757427\ttotal: 7m 6s\tremaining: 1m 34s\n",
      "2046:\tlearn: 0.5756823\ttotal: 7m 6s\tremaining: 1m 34s\n",
      "2047:\tlearn: 0.5756270\ttotal: 7m 6s\tremaining: 1m 34s\n",
      "2048:\tlearn: 0.5755713\ttotal: 7m 7s\tremaining: 1m 34s\n",
      "2049:\tlearn: 0.5755148\ttotal: 7m 7s\tremaining: 1m 33s\n",
      "2050:\tlearn: 0.5754557\ttotal: 7m 7s\tremaining: 1m 33s\n",
      "2051:\tlearn: 0.5753630\ttotal: 7m 7s\tremaining: 1m 33s\n",
      "2052:\tlearn: 0.5752812\ttotal: 7m 7s\tremaining: 1m 33s\n",
      "2053:\tlearn: 0.5752227\ttotal: 7m 8s\tremaining: 1m 32s\n",
      "2054:\tlearn: 0.5751496\ttotal: 7m 8s\tremaining: 1m 32s\n",
      "2055:\tlearn: 0.5750638\ttotal: 7m 8s\tremaining: 1m 32s\n",
      "2056:\tlearn: 0.5749883\ttotal: 7m 8s\tremaining: 1m 32s\n",
      "2057:\tlearn: 0.5749056\ttotal: 7m 8s\tremaining: 1m 32s\n",
      "2058:\tlearn: 0.5748443\ttotal: 7m 9s\tremaining: 1m 31s\n",
      "2059:\tlearn: 0.5747496\ttotal: 7m 9s\tremaining: 1m 31s\n",
      "2060:\tlearn: 0.5746763\ttotal: 7m 9s\tremaining: 1m 31s\n",
      "2061:\tlearn: 0.5745773\ttotal: 7m 9s\tremaining: 1m 31s\n",
      "2062:\tlearn: 0.5744990\ttotal: 7m 10s\tremaining: 1m 31s\n",
      "2063:\tlearn: 0.5744260\ttotal: 7m 10s\tremaining: 1m 30s\n",
      "2064:\tlearn: 0.5743406\ttotal: 7m 10s\tremaining: 1m 30s\n",
      "2065:\tlearn: 0.5742895\ttotal: 7m 10s\tremaining: 1m 30s\n",
      "2066:\tlearn: 0.5742253\ttotal: 7m 10s\tremaining: 1m 30s\n",
      "2067:\tlearn: 0.5741426\ttotal: 7m 11s\tremaining: 1m 30s\n",
      "2068:\tlearn: 0.5740682\ttotal: 7m 11s\tremaining: 1m 29s\n",
      "2069:\tlearn: 0.5739958\ttotal: 7m 11s\tremaining: 1m 29s\n",
      "2070:\tlearn: 0.5739012\ttotal: 7m 11s\tremaining: 1m 29s\n",
      "2071:\tlearn: 0.5738218\ttotal: 7m 11s\tremaining: 1m 29s\n",
      "2072:\tlearn: 0.5737412\ttotal: 7m 12s\tremaining: 1m 29s\n",
      "2073:\tlearn: 0.5736658\ttotal: 7m 12s\tremaining: 1m 28s\n",
      "2074:\tlearn: 0.5735868\ttotal: 7m 12s\tremaining: 1m 28s\n",
      "2075:\tlearn: 0.5735096\ttotal: 7m 12s\tremaining: 1m 28s\n",
      "2076:\tlearn: 0.5734175\ttotal: 7m 13s\tremaining: 1m 28s\n",
      "2077:\tlearn: 0.5733525\ttotal: 7m 13s\tremaining: 1m 27s\n",
      "2078:\tlearn: 0.5732732\ttotal: 7m 13s\tremaining: 1m 27s\n",
      "2079:\tlearn: 0.5732091\ttotal: 7m 13s\tremaining: 1m 27s\n",
      "2080:\tlearn: 0.5731162\ttotal: 7m 13s\tremaining: 1m 27s\n",
      "2081:\tlearn: 0.5730399\ttotal: 7m 14s\tremaining: 1m 27s\n",
      "2082:\tlearn: 0.5729813\ttotal: 7m 14s\tremaining: 1m 26s\n",
      "2083:\tlearn: 0.5729377\ttotal: 7m 14s\tremaining: 1m 26s\n",
      "2084:\tlearn: 0.5728781\ttotal: 7m 14s\tremaining: 1m 26s\n",
      "2085:\tlearn: 0.5727927\ttotal: 7m 14s\tremaining: 1m 26s\n",
      "2086:\tlearn: 0.5726994\ttotal: 7m 15s\tremaining: 1m 26s\n",
      "2087:\tlearn: 0.5726232\ttotal: 7m 15s\tremaining: 1m 25s\n",
      "2088:\tlearn: 0.5725393\ttotal: 7m 15s\tremaining: 1m 25s\n",
      "2089:\tlearn: 0.5724841\ttotal: 7m 15s\tremaining: 1m 25s\n",
      "2090:\tlearn: 0.5723911\ttotal: 7m 16s\tremaining: 1m 25s\n",
      "2091:\tlearn: 0.5723128\ttotal: 7m 16s\tremaining: 1m 25s\n",
      "2092:\tlearn: 0.5722614\ttotal: 7m 16s\tremaining: 1m 24s\n",
      "2093:\tlearn: 0.5721890\ttotal: 7m 16s\tremaining: 1m 24s\n",
      "2094:\tlearn: 0.5721318\ttotal: 7m 16s\tremaining: 1m 24s\n",
      "2095:\tlearn: 0.5720631\ttotal: 7m 17s\tremaining: 1m 24s\n",
      "2096:\tlearn: 0.5719973\ttotal: 7m 17s\tremaining: 1m 24s\n",
      "2097:\tlearn: 0.5719355\ttotal: 7m 17s\tremaining: 1m 23s\n",
      "2098:\tlearn: 0.5718674\ttotal: 7m 17s\tremaining: 1m 23s\n",
      "2099:\tlearn: 0.5717896\ttotal: 7m 17s\tremaining: 1m 23s\n",
      "2100:\tlearn: 0.5717485\ttotal: 7m 18s\tremaining: 1m 23s\n",
      "2101:\tlearn: 0.5717001\ttotal: 7m 18s\tremaining: 1m 22s\n",
      "2102:\tlearn: 0.5716399\ttotal: 7m 18s\tremaining: 1m 22s\n",
      "2103:\tlearn: 0.5715687\ttotal: 7m 18s\tremaining: 1m 22s\n",
      "2104:\tlearn: 0.5714795\ttotal: 7m 18s\tremaining: 1m 22s\n",
      "2105:\tlearn: 0.5714134\ttotal: 7m 19s\tremaining: 1m 22s\n",
      "2106:\tlearn: 0.5713316\ttotal: 7m 19s\tremaining: 1m 21s\n",
      "2107:\tlearn: 0.5712510\ttotal: 7m 19s\tremaining: 1m 21s\n",
      "2108:\tlearn: 0.5711604\ttotal: 7m 19s\tremaining: 1m 21s\n",
      "2109:\tlearn: 0.5710542\ttotal: 7m 19s\tremaining: 1m 21s\n",
      "2110:\tlearn: 0.5709731\ttotal: 7m 20s\tremaining: 1m 21s\n",
      "2111:\tlearn: 0.5709055\ttotal: 7m 20s\tremaining: 1m 20s\n",
      "2112:\tlearn: 0.5708313\ttotal: 7m 20s\tremaining: 1m 20s\n",
      "2113:\tlearn: 0.5707659\ttotal: 7m 20s\tremaining: 1m 20s\n",
      "2114:\tlearn: 0.5706883\ttotal: 7m 21s\tremaining: 1m 20s\n",
      "2115:\tlearn: 0.5706474\ttotal: 7m 21s\tremaining: 1m 20s\n",
      "2116:\tlearn: 0.5705781\ttotal: 7m 21s\tremaining: 1m 19s\n",
      "2117:\tlearn: 0.5704890\ttotal: 7m 21s\tremaining: 1m 19s\n",
      "2118:\tlearn: 0.5703989\ttotal: 7m 21s\tremaining: 1m 19s\n",
      "2119:\tlearn: 0.5703210\ttotal: 7m 22s\tremaining: 1m 19s\n",
      "2120:\tlearn: 0.5702305\ttotal: 7m 22s\tremaining: 1m 19s\n",
      "2121:\tlearn: 0.5701756\ttotal: 7m 22s\tremaining: 1m 18s\n",
      "2122:\tlearn: 0.5700917\ttotal: 7m 22s\tremaining: 1m 18s\n",
      "2123:\tlearn: 0.5700037\ttotal: 7m 22s\tremaining: 1m 18s\n",
      "2124:\tlearn: 0.5699564\ttotal: 7m 23s\tremaining: 1m 18s\n",
      "2125:\tlearn: 0.5698777\ttotal: 7m 23s\tremaining: 1m 17s\n",
      "2126:\tlearn: 0.5698061\ttotal: 7m 23s\tremaining: 1m 17s\n",
      "2127:\tlearn: 0.5697574\ttotal: 7m 23s\tremaining: 1m 17s\n",
      "2128:\tlearn: 0.5696950\ttotal: 7m 23s\tremaining: 1m 17s\n",
      "2129:\tlearn: 0.5696271\ttotal: 7m 24s\tremaining: 1m 17s\n",
      "2130:\tlearn: 0.5695259\ttotal: 7m 24s\tremaining: 1m 16s\n",
      "2131:\tlearn: 0.5694251\ttotal: 7m 24s\tremaining: 1m 16s\n",
      "2132:\tlearn: 0.5693422\ttotal: 7m 24s\tremaining: 1m 16s\n",
      "2133:\tlearn: 0.5692909\ttotal: 7m 25s\tremaining: 1m 16s\n",
      "2134:\tlearn: 0.5692133\ttotal: 7m 25s\tremaining: 1m 16s\n",
      "2135:\tlearn: 0.5691441\ttotal: 7m 25s\tremaining: 1m 15s\n",
      "2136:\tlearn: 0.5690697\ttotal: 7m 25s\tremaining: 1m 15s\n",
      "2137:\tlearn: 0.5689899\ttotal: 7m 25s\tremaining: 1m 15s\n",
      "2138:\tlearn: 0.5688701\ttotal: 7m 26s\tremaining: 1m 15s\n",
      "2139:\tlearn: 0.5688216\ttotal: 7m 26s\tremaining: 1m 15s\n",
      "2140:\tlearn: 0.5687732\ttotal: 7m 26s\tremaining: 1m 14s\n",
      "2141:\tlearn: 0.5687019\ttotal: 7m 26s\tremaining: 1m 14s\n",
      "2142:\tlearn: 0.5686083\ttotal: 7m 26s\tremaining: 1m 14s\n",
      "2143:\tlearn: 0.5685202\ttotal: 7m 27s\tremaining: 1m 14s\n",
      "2144:\tlearn: 0.5684418\ttotal: 7m 27s\tremaining: 1m 14s\n",
      "2145:\tlearn: 0.5683827\ttotal: 7m 27s\tremaining: 1m 13s\n",
      "2146:\tlearn: 0.5683168\ttotal: 7m 27s\tremaining: 1m 13s\n",
      "2147:\tlearn: 0.5682496\ttotal: 7m 27s\tremaining: 1m 13s\n",
      "2148:\tlearn: 0.5681686\ttotal: 7m 28s\tremaining: 1m 13s\n",
      "2149:\tlearn: 0.5681169\ttotal: 7m 28s\tremaining: 1m 12s\n",
      "2150:\tlearn: 0.5680470\ttotal: 7m 28s\tremaining: 1m 12s\n",
      "2151:\tlearn: 0.5679458\ttotal: 7m 28s\tremaining: 1m 12s\n",
      "2152:\tlearn: 0.5678654\ttotal: 7m 28s\tremaining: 1m 12s\n",
      "2153:\tlearn: 0.5677958\ttotal: 7m 29s\tremaining: 1m 12s\n",
      "2154:\tlearn: 0.5677276\ttotal: 7m 29s\tremaining: 1m 11s\n",
      "2155:\tlearn: 0.5676767\ttotal: 7m 29s\tremaining: 1m 11s\n",
      "2156:\tlearn: 0.5676077\ttotal: 7m 29s\tremaining: 1m 11s\n",
      "2157:\tlearn: 0.5675263\ttotal: 7m 30s\tremaining: 1m 11s\n",
      "2158:\tlearn: 0.5674654\ttotal: 7m 30s\tremaining: 1m 11s\n",
      "2159:\tlearn: 0.5674077\ttotal: 7m 30s\tremaining: 1m 10s\n",
      "2160:\tlearn: 0.5673215\ttotal: 7m 30s\tremaining: 1m 10s\n",
      "2161:\tlearn: 0.5672464\ttotal: 7m 30s\tremaining: 1m 10s\n",
      "2162:\tlearn: 0.5671997\ttotal: 7m 31s\tremaining: 1m 10s\n",
      "2163:\tlearn: 0.5671514\ttotal: 7m 31s\tremaining: 1m 10s\n",
      "2164:\tlearn: 0.5670764\ttotal: 7m 31s\tremaining: 1m 9s\n",
      "2165:\tlearn: 0.5670031\ttotal: 7m 31s\tremaining: 1m 9s\n",
      "2166:\tlearn: 0.5669413\ttotal: 7m 31s\tremaining: 1m 9s\n",
      "2167:\tlearn: 0.5668675\ttotal: 7m 32s\tremaining: 1m 9s\n",
      "2168:\tlearn: 0.5667951\ttotal: 7m 32s\tremaining: 1m 9s\n",
      "2169:\tlearn: 0.5667565\ttotal: 7m 32s\tremaining: 1m 8s\n",
      "2170:\tlearn: 0.5666772\ttotal: 7m 32s\tremaining: 1m 8s\n",
      "2171:\tlearn: 0.5666308\ttotal: 7m 32s\tremaining: 1m 8s\n",
      "2172:\tlearn: 0.5665852\ttotal: 7m 33s\tremaining: 1m 8s\n",
      "2173:\tlearn: 0.5665176\ttotal: 7m 33s\tremaining: 1m 7s\n",
      "2174:\tlearn: 0.5664490\ttotal: 7m 33s\tremaining: 1m 7s\n",
      "2175:\tlearn: 0.5663892\ttotal: 7m 33s\tremaining: 1m 7s\n",
      "2176:\tlearn: 0.5663261\ttotal: 7m 34s\tremaining: 1m 7s\n",
      "2177:\tlearn: 0.5662517\ttotal: 7m 34s\tremaining: 1m 7s\n",
      "2178:\tlearn: 0.5661990\ttotal: 7m 34s\tremaining: 1m 6s\n",
      "2179:\tlearn: 0.5661584\ttotal: 7m 34s\tremaining: 1m 6s\n",
      "2180:\tlearn: 0.5660804\ttotal: 7m 34s\tremaining: 1m 6s\n",
      "2181:\tlearn: 0.5660238\ttotal: 7m 35s\tremaining: 1m 6s\n",
      "2182:\tlearn: 0.5659544\ttotal: 7m 35s\tremaining: 1m 6s\n",
      "2183:\tlearn: 0.5659016\ttotal: 7m 35s\tremaining: 1m 5s\n",
      "2184:\tlearn: 0.5658125\ttotal: 7m 35s\tremaining: 1m 5s\n",
      "2185:\tlearn: 0.5657446\ttotal: 7m 35s\tremaining: 1m 5s\n",
      "2186:\tlearn: 0.5656490\ttotal: 7m 36s\tremaining: 1m 5s\n",
      "2187:\tlearn: 0.5655657\ttotal: 7m 36s\tremaining: 1m 5s\n",
      "2188:\tlearn: 0.5655041\ttotal: 7m 36s\tremaining: 1m 4s\n",
      "2189:\tlearn: 0.5654509\ttotal: 7m 36s\tremaining: 1m 4s\n",
      "2190:\tlearn: 0.5653742\ttotal: 7m 36s\tremaining: 1m 4s\n",
      "2191:\tlearn: 0.5653049\ttotal: 7m 37s\tremaining: 1m 4s\n",
      "2192:\tlearn: 0.5652385\ttotal: 7m 37s\tremaining: 1m 4s\n",
      "2193:\tlearn: 0.5651419\ttotal: 7m 37s\tremaining: 1m 3s\n",
      "2194:\tlearn: 0.5650871\ttotal: 7m 37s\tremaining: 1m 3s\n",
      "2195:\tlearn: 0.5650288\ttotal: 7m 38s\tremaining: 1m 3s\n",
      "2196:\tlearn: 0.5649490\ttotal: 7m 38s\tremaining: 1m 3s\n",
      "2197:\tlearn: 0.5648826\ttotal: 7m 38s\tremaining: 1m 2s\n",
      "2198:\tlearn: 0.5648186\ttotal: 7m 38s\tremaining: 1m 2s\n",
      "2199:\tlearn: 0.5647360\ttotal: 7m 38s\tremaining: 1m 2s\n",
      "2200:\tlearn: 0.5646317\ttotal: 7m 39s\tremaining: 1m 2s\n",
      "2201:\tlearn: 0.5645690\ttotal: 7m 39s\tremaining: 1m 2s\n",
      "2202:\tlearn: 0.5645084\ttotal: 7m 39s\tremaining: 1m 1s\n",
      "2203:\tlearn: 0.5644365\ttotal: 7m 39s\tremaining: 1m 1s\n",
      "2204:\tlearn: 0.5643877\ttotal: 7m 39s\tremaining: 1m 1s\n",
      "2205:\tlearn: 0.5643107\ttotal: 7m 40s\tremaining: 1m 1s\n",
      "2206:\tlearn: 0.5642617\ttotal: 7m 40s\tremaining: 1m 1s\n",
      "2207:\tlearn: 0.5641885\ttotal: 7m 40s\tremaining: 1m\n",
      "2208:\tlearn: 0.5641342\ttotal: 7m 40s\tremaining: 1m\n",
      "2209:\tlearn: 0.5640534\ttotal: 7m 41s\tremaining: 1m\n",
      "2210:\tlearn: 0.5639810\ttotal: 7m 41s\tremaining: 1m\n",
      "2211:\tlearn: 0.5638828\ttotal: 7m 41s\tremaining: 1m\n",
      "2212:\tlearn: 0.5638152\ttotal: 7m 41s\tremaining: 59.9s\n",
      "2213:\tlearn: 0.5637694\ttotal: 7m 41s\tremaining: 59.7s\n",
      "2214:\tlearn: 0.5637037\ttotal: 7m 42s\tremaining: 59.5s\n",
      "2215:\tlearn: 0.5636387\ttotal: 7m 42s\tremaining: 59.2s\n",
      "2216:\tlearn: 0.5635974\ttotal: 7m 42s\tremaining: 59s\n",
      "2217:\tlearn: 0.5635110\ttotal: 7m 42s\tremaining: 58.8s\n",
      "2218:\tlearn: 0.5634718\ttotal: 7m 42s\tremaining: 58.6s\n",
      "2219:\tlearn: 0.5633991\ttotal: 7m 43s\tremaining: 58.4s\n",
      "2220:\tlearn: 0.5633236\ttotal: 7m 43s\tremaining: 58.2s\n",
      "2221:\tlearn: 0.5632698\ttotal: 7m 43s\tremaining: 58s\n",
      "2222:\tlearn: 0.5632020\ttotal: 7m 43s\tremaining: 57.8s\n",
      "2223:\tlearn: 0.5631047\ttotal: 7m 43s\tremaining: 57.6s\n",
      "2224:\tlearn: 0.5630572\ttotal: 7m 44s\tremaining: 57.4s\n",
      "2225:\tlearn: 0.5629903\ttotal: 7m 44s\tremaining: 57.2s\n",
      "2226:\tlearn: 0.5629134\ttotal: 7m 44s\tremaining: 56.9s\n",
      "2227:\tlearn: 0.5628598\ttotal: 7m 44s\tremaining: 56.7s\n",
      "2228:\tlearn: 0.5628115\ttotal: 7m 44s\tremaining: 56.5s\n",
      "2229:\tlearn: 0.5627384\ttotal: 7m 45s\tremaining: 56.3s\n",
      "2230:\tlearn: 0.5626586\ttotal: 7m 45s\tremaining: 56.1s\n",
      "2231:\tlearn: 0.5625897\ttotal: 7m 45s\tremaining: 55.9s\n",
      "2232:\tlearn: 0.5625170\ttotal: 7m 45s\tremaining: 55.7s\n",
      "2233:\tlearn: 0.5624458\ttotal: 7m 46s\tremaining: 55.5s\n",
      "2234:\tlearn: 0.5623692\ttotal: 7m 46s\tremaining: 55.3s\n",
      "2235:\tlearn: 0.5623093\ttotal: 7m 46s\tremaining: 55.1s\n",
      "2236:\tlearn: 0.5622716\ttotal: 7m 46s\tremaining: 54.9s\n",
      "2237:\tlearn: 0.5621877\ttotal: 7m 46s\tremaining: 54.6s\n",
      "2238:\tlearn: 0.5621229\ttotal: 7m 47s\tremaining: 54.4s\n",
      "2239:\tlearn: 0.5620724\ttotal: 7m 47s\tremaining: 54.2s\n",
      "2240:\tlearn: 0.5620273\ttotal: 7m 47s\tremaining: 54s\n",
      "2241:\tlearn: 0.5619899\ttotal: 7m 47s\tremaining: 53.8s\n",
      "2242:\tlearn: 0.5619042\ttotal: 7m 47s\tremaining: 53.6s\n",
      "2243:\tlearn: 0.5618359\ttotal: 7m 48s\tremaining: 53.4s\n",
      "2244:\tlearn: 0.5617607\ttotal: 7m 48s\tremaining: 53.2s\n",
      "2245:\tlearn: 0.5616956\ttotal: 7m 48s\tremaining: 53s\n",
      "2246:\tlearn: 0.5616231\ttotal: 7m 48s\tremaining: 52.8s\n",
      "2247:\tlearn: 0.5615468\ttotal: 7m 48s\tremaining: 52.6s\n",
      "2248:\tlearn: 0.5614803\ttotal: 7m 49s\tremaining: 52.4s\n",
      "2249:\tlearn: 0.5614080\ttotal: 7m 49s\tremaining: 52.1s\n",
      "2250:\tlearn: 0.5613343\ttotal: 7m 49s\tremaining: 51.9s\n",
      "2251:\tlearn: 0.5612645\ttotal: 7m 49s\tremaining: 51.7s\n",
      "2252:\tlearn: 0.5612163\ttotal: 7m 49s\tremaining: 51.5s\n",
      "2253:\tlearn: 0.5611575\ttotal: 7m 50s\tremaining: 51.3s\n",
      "2254:\tlearn: 0.5610859\ttotal: 7m 50s\tremaining: 51.1s\n",
      "2255:\tlearn: 0.5610393\ttotal: 7m 50s\tremaining: 50.9s\n",
      "2256:\tlearn: 0.5609830\ttotal: 7m 50s\tremaining: 50.7s\n",
      "2257:\tlearn: 0.5609275\ttotal: 7m 51s\tremaining: 50.5s\n",
      "2258:\tlearn: 0.5608903\ttotal: 7m 51s\tremaining: 50.3s\n",
      "2259:\tlearn: 0.5608235\ttotal: 7m 51s\tremaining: 50.1s\n",
      "2260:\tlearn: 0.5607669\ttotal: 7m 51s\tremaining: 49.9s\n",
      "2261:\tlearn: 0.5606748\ttotal: 7m 51s\tremaining: 49.6s\n",
      "2262:\tlearn: 0.5605950\ttotal: 7m 52s\tremaining: 49.4s\n",
      "2263:\tlearn: 0.5605405\ttotal: 7m 52s\tremaining: 49.2s\n",
      "2264:\tlearn: 0.5604920\ttotal: 7m 52s\tremaining: 49s\n",
      "2265:\tlearn: 0.5604610\ttotal: 7m 52s\tremaining: 48.8s\n",
      "2266:\tlearn: 0.5603873\ttotal: 7m 52s\tremaining: 48.6s\n",
      "2267:\tlearn: 0.5603107\ttotal: 7m 53s\tremaining: 48.4s\n",
      "2268:\tlearn: 0.5602473\ttotal: 7m 53s\tremaining: 48.2s\n",
      "2269:\tlearn: 0.5601759\ttotal: 7m 53s\tremaining: 48s\n",
      "2270:\tlearn: 0.5601341\ttotal: 7m 53s\tremaining: 47.8s\n",
      "2271:\tlearn: 0.5600481\ttotal: 7m 53s\tremaining: 47.6s\n",
      "2272:\tlearn: 0.5599850\ttotal: 7m 54s\tremaining: 47.3s\n",
      "2273:\tlearn: 0.5599401\ttotal: 7m 54s\tremaining: 47.1s\n",
      "2274:\tlearn: 0.5598806\ttotal: 7m 54s\tremaining: 46.9s\n",
      "2275:\tlearn: 0.5598219\ttotal: 7m 54s\tremaining: 46.7s\n",
      "2276:\tlearn: 0.5597629\ttotal: 7m 54s\tremaining: 46.5s\n",
      "2277:\tlearn: 0.5596925\ttotal: 7m 55s\tremaining: 46.3s\n",
      "2278:\tlearn: 0.5596489\ttotal: 7m 55s\tremaining: 46.1s\n",
      "2279:\tlearn: 0.5595904\ttotal: 7m 55s\tremaining: 45.9s\n",
      "2280:\tlearn: 0.5595188\ttotal: 7m 55s\tremaining: 45.7s\n",
      "2281:\tlearn: 0.5594540\ttotal: 7m 56s\tremaining: 45.5s\n",
      "2282:\tlearn: 0.5594063\ttotal: 7m 56s\tremaining: 45.3s\n",
      "2283:\tlearn: 0.5593112\ttotal: 7m 56s\tremaining: 45.1s\n",
      "2284:\tlearn: 0.5592582\ttotal: 7m 56s\tremaining: 44.8s\n",
      "2285:\tlearn: 0.5591880\ttotal: 7m 56s\tremaining: 44.6s\n",
      "2286:\tlearn: 0.5591228\ttotal: 7m 57s\tremaining: 44.4s\n",
      "2287:\tlearn: 0.5590738\ttotal: 7m 57s\tremaining: 44.2s\n",
      "2288:\tlearn: 0.5589758\ttotal: 7m 57s\tremaining: 44s\n",
      "2289:\tlearn: 0.5589010\ttotal: 7m 57s\tremaining: 43.8s\n",
      "2290:\tlearn: 0.5588249\ttotal: 7m 57s\tremaining: 43.6s\n",
      "2291:\tlearn: 0.5587556\ttotal: 7m 58s\tremaining: 43.4s\n",
      "2292:\tlearn: 0.5587040\ttotal: 7m 58s\tremaining: 43.2s\n",
      "2293:\tlearn: 0.5586386\ttotal: 7m 58s\tremaining: 43s\n",
      "2294:\tlearn: 0.5585431\ttotal: 7m 58s\tremaining: 42.8s\n",
      "2295:\tlearn: 0.5584895\ttotal: 7m 58s\tremaining: 42.6s\n",
      "2296:\tlearn: 0.5584142\ttotal: 7m 59s\tremaining: 42.4s\n",
      "2297:\tlearn: 0.5583746\ttotal: 7m 59s\tremaining: 42.1s\n",
      "2298:\tlearn: 0.5582936\ttotal: 7m 59s\tremaining: 41.9s\n",
      "2299:\tlearn: 0.5582074\ttotal: 7m 59s\tremaining: 41.7s\n",
      "2300:\tlearn: 0.5581469\ttotal: 8m\tremaining: 41.5s\n",
      "2301:\tlearn: 0.5580864\ttotal: 8m\tremaining: 41.3s\n",
      "2302:\tlearn: 0.5579936\ttotal: 8m\tremaining: 41.1s\n",
      "2303:\tlearn: 0.5579374\ttotal: 8m\tremaining: 40.9s\n",
      "2304:\tlearn: 0.5578825\ttotal: 8m\tremaining: 40.7s\n",
      "2305:\tlearn: 0.5578211\ttotal: 8m 1s\tremaining: 40.5s\n",
      "2306:\tlearn: 0.5577582\ttotal: 8m 1s\tremaining: 40.3s\n",
      "2307:\tlearn: 0.5576800\ttotal: 8m 1s\tremaining: 40s\n",
      "2308:\tlearn: 0.5576341\ttotal: 8m 1s\tremaining: 39.8s\n",
      "2309:\tlearn: 0.5575545\ttotal: 8m 1s\tremaining: 39.6s\n",
      "2310:\tlearn: 0.5575048\ttotal: 8m 2s\tremaining: 39.4s\n",
      "2311:\tlearn: 0.5574480\ttotal: 8m 2s\tremaining: 39.2s\n",
      "2312:\tlearn: 0.5573884\ttotal: 8m 2s\tremaining: 39s\n",
      "2313:\tlearn: 0.5573536\ttotal: 8m 2s\tremaining: 38.8s\n",
      "2314:\tlearn: 0.5572768\ttotal: 8m 2s\tremaining: 38.6s\n",
      "2315:\tlearn: 0.5572295\ttotal: 8m 3s\tremaining: 38.4s\n",
      "2316:\tlearn: 0.5571711\ttotal: 8m 3s\tremaining: 38.2s\n",
      "2317:\tlearn: 0.5571220\ttotal: 8m 3s\tremaining: 38s\n",
      "2318:\tlearn: 0.5570375\ttotal: 8m 3s\tremaining: 37.8s\n",
      "2319:\tlearn: 0.5569738\ttotal: 8m 3s\tremaining: 37.5s\n",
      "2320:\tlearn: 0.5569089\ttotal: 8m 4s\tremaining: 37.3s\n",
      "2321:\tlearn: 0.5568408\ttotal: 8m 4s\tremaining: 37.1s\n",
      "2322:\tlearn: 0.5567981\ttotal: 8m 4s\tremaining: 36.9s\n",
      "2323:\tlearn: 0.5567443\ttotal: 8m 4s\tremaining: 36.7s\n",
      "2324:\tlearn: 0.5566976\ttotal: 8m 5s\tremaining: 36.5s\n",
      "2325:\tlearn: 0.5566372\ttotal: 8m 5s\tremaining: 36.3s\n",
      "2326:\tlearn: 0.5565976\ttotal: 8m 5s\tremaining: 36.1s\n",
      "2327:\tlearn: 0.5565468\ttotal: 8m 5s\tremaining: 35.9s\n",
      "2328:\tlearn: 0.5564805\ttotal: 8m 5s\tremaining: 35.7s\n",
      "2329:\tlearn: 0.5564012\ttotal: 8m 6s\tremaining: 35.5s\n",
      "2330:\tlearn: 0.5563466\ttotal: 8m 6s\tremaining: 35.3s\n",
      "2331:\tlearn: 0.5562803\ttotal: 8m 6s\tremaining: 35s\n",
      "2332:\tlearn: 0.5562217\ttotal: 8m 6s\tremaining: 34.8s\n",
      "2333:\tlearn: 0.5561387\ttotal: 8m 6s\tremaining: 34.6s\n",
      "2334:\tlearn: 0.5560818\ttotal: 8m 7s\tremaining: 34.4s\n",
      "2335:\tlearn: 0.5560120\ttotal: 8m 7s\tremaining: 34.2s\n",
      "2336:\tlearn: 0.5559416\ttotal: 8m 7s\tremaining: 34s\n",
      "2337:\tlearn: 0.5558922\ttotal: 8m 7s\tremaining: 33.8s\n",
      "2338:\tlearn: 0.5558096\ttotal: 8m 7s\tremaining: 33.6s\n",
      "2339:\tlearn: 0.5557338\ttotal: 8m 8s\tremaining: 33.4s\n",
      "2340:\tlearn: 0.5556696\ttotal: 8m 8s\tremaining: 33.2s\n",
      "2341:\tlearn: 0.5555952\ttotal: 8m 8s\tremaining: 33s\n",
      "2342:\tlearn: 0.5555289\ttotal: 8m 8s\tremaining: 32.8s\n",
      "2343:\tlearn: 0.5554561\ttotal: 8m 9s\tremaining: 32.5s\n",
      "2344:\tlearn: 0.5553833\ttotal: 8m 9s\tremaining: 32.3s\n",
      "2345:\tlearn: 0.5553237\ttotal: 8m 9s\tremaining: 32.1s\n",
      "2346:\tlearn: 0.5552721\ttotal: 8m 9s\tremaining: 31.9s\n",
      "2347:\tlearn: 0.5552272\ttotal: 8m 9s\tremaining: 31.7s\n",
      "2348:\tlearn: 0.5551810\ttotal: 8m 10s\tremaining: 31.5s\n",
      "2349:\tlearn: 0.5551102\ttotal: 8m 10s\tremaining: 31.3s\n",
      "2350:\tlearn: 0.5550643\ttotal: 8m 10s\tremaining: 31.1s\n",
      "2351:\tlearn: 0.5550031\ttotal: 8m 10s\tremaining: 30.9s\n",
      "2352:\tlearn: 0.5549632\ttotal: 8m 10s\tremaining: 30.7s\n",
      "2353:\tlearn: 0.5549226\ttotal: 8m 11s\tremaining: 30.5s\n",
      "2354:\tlearn: 0.5548721\ttotal: 8m 11s\tremaining: 30.3s\n",
      "2355:\tlearn: 0.5548082\ttotal: 8m 11s\tremaining: 30s\n",
      "2356:\tlearn: 0.5547445\ttotal: 8m 11s\tremaining: 29.8s\n",
      "2357:\tlearn: 0.5546553\ttotal: 8m 11s\tremaining: 29.6s\n",
      "2358:\tlearn: 0.5545960\ttotal: 8m 12s\tremaining: 29.4s\n",
      "2359:\tlearn: 0.5545473\ttotal: 8m 12s\tremaining: 29.2s\n",
      "2360:\tlearn: 0.5544892\ttotal: 8m 12s\tremaining: 29s\n",
      "2361:\tlearn: 0.5544240\ttotal: 8m 12s\tremaining: 28.8s\n",
      "2362:\tlearn: 0.5543569\ttotal: 8m 13s\tremaining: 28.6s\n",
      "2363:\tlearn: 0.5542816\ttotal: 8m 13s\tremaining: 28.4s\n",
      "2364:\tlearn: 0.5542186\ttotal: 8m 13s\tremaining: 28.2s\n",
      "2365:\tlearn: 0.5541458\ttotal: 8m 13s\tremaining: 28s\n",
      "2366:\tlearn: 0.5540921\ttotal: 8m 13s\tremaining: 27.7s\n",
      "2367:\tlearn: 0.5540291\ttotal: 8m 14s\tremaining: 27.5s\n",
      "2368:\tlearn: 0.5539687\ttotal: 8m 14s\tremaining: 27.3s\n",
      "2369:\tlearn: 0.5539218\ttotal: 8m 14s\tremaining: 27.1s\n",
      "2370:\tlearn: 0.5538666\ttotal: 8m 14s\tremaining: 26.9s\n",
      "2371:\tlearn: 0.5538197\ttotal: 8m 14s\tremaining: 26.7s\n",
      "2372:\tlearn: 0.5537545\ttotal: 8m 15s\tremaining: 26.5s\n",
      "2373:\tlearn: 0.5537170\ttotal: 8m 15s\tremaining: 26.3s\n",
      "2374:\tlearn: 0.5536261\ttotal: 8m 15s\tremaining: 26.1s\n",
      "2375:\tlearn: 0.5535576\ttotal: 8m 15s\tremaining: 25.9s\n",
      "2376:\tlearn: 0.5535005\ttotal: 8m 15s\tremaining: 25.7s\n",
      "2377:\tlearn: 0.5534536\ttotal: 8m 16s\tremaining: 25.5s\n",
      "2378:\tlearn: 0.5534226\ttotal: 8m 16s\tremaining: 25.2s\n",
      "2379:\tlearn: 0.5533670\ttotal: 8m 16s\tremaining: 25s\n",
      "2380:\tlearn: 0.5532901\ttotal: 8m 16s\tremaining: 24.8s\n",
      "2381:\tlearn: 0.5532533\ttotal: 8m 16s\tremaining: 24.6s\n",
      "2382:\tlearn: 0.5531903\ttotal: 8m 17s\tremaining: 24.4s\n",
      "2383:\tlearn: 0.5531476\ttotal: 8m 17s\tremaining: 24.2s\n",
      "2384:\tlearn: 0.5530938\ttotal: 8m 17s\tremaining: 24s\n",
      "2385:\tlearn: 0.5530431\ttotal: 8m 17s\tremaining: 23.8s\n",
      "2386:\tlearn: 0.5529701\ttotal: 8m 18s\tremaining: 23.6s\n",
      "2387:\tlearn: 0.5529361\ttotal: 8m 18s\tremaining: 23.4s\n",
      "2388:\tlearn: 0.5528761\ttotal: 8m 18s\tremaining: 23.2s\n",
      "2389:\tlearn: 0.5528170\ttotal: 8m 18s\tremaining: 22.9s\n",
      "2390:\tlearn: 0.5527803\ttotal: 8m 18s\tremaining: 22.7s\n",
      "2391:\tlearn: 0.5527183\ttotal: 8m 19s\tremaining: 22.5s\n",
      "2392:\tlearn: 0.5526756\ttotal: 8m 19s\tremaining: 22.3s\n",
      "2393:\tlearn: 0.5526015\ttotal: 8m 19s\tremaining: 22.1s\n",
      "2394:\tlearn: 0.5525460\ttotal: 8m 19s\tremaining: 21.9s\n",
      "2395:\tlearn: 0.5524911\ttotal: 8m 19s\tremaining: 21.7s\n",
      "2396:\tlearn: 0.5524315\ttotal: 8m 20s\tremaining: 21.5s\n",
      "2397:\tlearn: 0.5523984\ttotal: 8m 20s\tremaining: 21.3s\n",
      "2398:\tlearn: 0.5523577\ttotal: 8m 20s\tremaining: 21.1s\n",
      "2399:\tlearn: 0.5522976\ttotal: 8m 20s\tremaining: 20.9s\n",
      "2400:\tlearn: 0.5522370\ttotal: 8m 20s\tremaining: 20.7s\n",
      "2401:\tlearn: 0.5521620\ttotal: 8m 21s\tremaining: 20.4s\n",
      "2402:\tlearn: 0.5521206\ttotal: 8m 21s\tremaining: 20.2s\n",
      "2403:\tlearn: 0.5520478\ttotal: 8m 21s\tremaining: 20s\n",
      "2404:\tlearn: 0.5519420\ttotal: 8m 21s\tremaining: 19.8s\n",
      "2405:\tlearn: 0.5518929\ttotal: 8m 21s\tremaining: 19.6s\n",
      "2406:\tlearn: 0.5518079\ttotal: 8m 22s\tremaining: 19.4s\n",
      "2407:\tlearn: 0.5517341\ttotal: 8m 22s\tremaining: 19.2s\n",
      "2408:\tlearn: 0.5516848\ttotal: 8m 22s\tremaining: 19s\n",
      "2409:\tlearn: 0.5516294\ttotal: 8m 22s\tremaining: 18.8s\n",
      "2410:\tlearn: 0.5515755\ttotal: 8m 23s\tremaining: 18.6s\n",
      "2411:\tlearn: 0.5515297\ttotal: 8m 23s\tremaining: 18.4s\n",
      "2412:\tlearn: 0.5514863\ttotal: 8m 23s\tremaining: 18.2s\n",
      "2413:\tlearn: 0.5514490\ttotal: 8m 23s\tremaining: 17.9s\n",
      "2414:\tlearn: 0.5513729\ttotal: 8m 23s\tremaining: 17.7s\n",
      "2415:\tlearn: 0.5513275\ttotal: 8m 24s\tremaining: 17.5s\n",
      "2416:\tlearn: 0.5512690\ttotal: 8m 24s\tremaining: 17.3s\n",
      "2417:\tlearn: 0.5512356\ttotal: 8m 24s\tremaining: 17.1s\n",
      "2418:\tlearn: 0.5512036\ttotal: 8m 24s\tremaining: 16.9s\n",
      "2419:\tlearn: 0.5511402\ttotal: 8m 24s\tremaining: 16.7s\n",
      "2420:\tlearn: 0.5511015\ttotal: 8m 25s\tremaining: 16.5s\n",
      "2421:\tlearn: 0.5510530\ttotal: 8m 25s\tremaining: 16.3s\n",
      "2422:\tlearn: 0.5509895\ttotal: 8m 25s\tremaining: 16.1s\n",
      "2423:\tlearn: 0.5509095\ttotal: 8m 25s\tremaining: 15.9s\n",
      "2424:\tlearn: 0.5508488\ttotal: 8m 25s\tremaining: 15.6s\n",
      "2425:\tlearn: 0.5507861\ttotal: 8m 26s\tremaining: 15.4s\n",
      "2426:\tlearn: 0.5507245\ttotal: 8m 26s\tremaining: 15.2s\n",
      "2427:\tlearn: 0.5506557\ttotal: 8m 26s\tremaining: 15s\n",
      "2428:\tlearn: 0.5506009\ttotal: 8m 26s\tremaining: 14.8s\n",
      "2429:\tlearn: 0.5505236\ttotal: 8m 27s\tremaining: 14.6s\n",
      "2430:\tlearn: 0.5504865\ttotal: 8m 27s\tremaining: 14.4s\n",
      "2431:\tlearn: 0.5504057\ttotal: 8m 27s\tremaining: 14.2s\n",
      "2432:\tlearn: 0.5503578\ttotal: 8m 27s\tremaining: 14s\n",
      "2433:\tlearn: 0.5503183\ttotal: 8m 27s\tremaining: 13.8s\n",
      "2434:\tlearn: 0.5502646\ttotal: 8m 28s\tremaining: 13.6s\n",
      "2435:\tlearn: 0.5502242\ttotal: 8m 28s\tremaining: 13.4s\n",
      "2436:\tlearn: 0.5501663\ttotal: 8m 28s\tremaining: 13.1s\n",
      "2437:\tlearn: 0.5501168\ttotal: 8m 28s\tremaining: 12.9s\n",
      "2438:\tlearn: 0.5500451\ttotal: 8m 28s\tremaining: 12.7s\n",
      "2439:\tlearn: 0.5499916\ttotal: 8m 29s\tremaining: 12.5s\n",
      "2440:\tlearn: 0.5499069\ttotal: 8m 29s\tremaining: 12.3s\n",
      "2441:\tlearn: 0.5498605\ttotal: 8m 29s\tremaining: 12.1s\n",
      "2442:\tlearn: 0.5497828\ttotal: 8m 29s\tremaining: 11.9s\n",
      "2443:\tlearn: 0.5497002\ttotal: 8m 29s\tremaining: 11.7s\n",
      "2444:\tlearn: 0.5496286\ttotal: 8m 30s\tremaining: 11.5s\n",
      "2445:\tlearn: 0.5495896\ttotal: 8m 30s\tremaining: 11.3s\n",
      "2446:\tlearn: 0.5495322\ttotal: 8m 30s\tremaining: 11.1s\n",
      "2447:\tlearn: 0.5494624\ttotal: 8m 30s\tremaining: 10.8s\n",
      "2448:\tlearn: 0.5494052\ttotal: 8m 31s\tremaining: 10.6s\n",
      "2449:\tlearn: 0.5493701\ttotal: 8m 31s\tremaining: 10.4s\n",
      "2450:\tlearn: 0.5493086\ttotal: 8m 31s\tremaining: 10.2s\n",
      "2451:\tlearn: 0.5492780\ttotal: 8m 31s\tremaining: 10s\n",
      "2452:\tlearn: 0.5492227\ttotal: 8m 31s\tremaining: 9.81s\n",
      "2453:\tlearn: 0.5491672\ttotal: 8m 32s\tremaining: 9.6s\n",
      "2454:\tlearn: 0.5490901\ttotal: 8m 32s\tremaining: 9.39s\n",
      "2455:\tlearn: 0.5490515\ttotal: 8m 32s\tremaining: 9.18s\n",
      "2456:\tlearn: 0.5489799\ttotal: 8m 32s\tremaining: 8.97s\n",
      "2457:\tlearn: 0.5489123\ttotal: 8m 32s\tremaining: 8.76s\n",
      "2458:\tlearn: 0.5488396\ttotal: 8m 33s\tremaining: 8.55s\n",
      "2459:\tlearn: 0.5487763\ttotal: 8m 33s\tremaining: 8.35s\n",
      "2460:\tlearn: 0.5487039\ttotal: 8m 33s\tremaining: 8.14s\n",
      "2461:\tlearn: 0.5486714\ttotal: 8m 33s\tremaining: 7.93s\n",
      "2462:\tlearn: 0.5486373\ttotal: 8m 33s\tremaining: 7.72s\n",
      "2463:\tlearn: 0.5485991\ttotal: 8m 34s\tremaining: 7.51s\n",
      "2464:\tlearn: 0.5485570\ttotal: 8m 34s\tremaining: 7.3s\n",
      "2465:\tlearn: 0.5485021\ttotal: 8m 34s\tremaining: 7.09s\n",
      "2466:\tlearn: 0.5484568\ttotal: 8m 34s\tremaining: 6.89s\n",
      "2467:\tlearn: 0.5483972\ttotal: 8m 35s\tremaining: 6.68s\n",
      "2468:\tlearn: 0.5483379\ttotal: 8m 35s\tremaining: 6.47s\n",
      "2469:\tlearn: 0.5482705\ttotal: 8m 35s\tremaining: 6.26s\n",
      "2470:\tlearn: 0.5481975\ttotal: 8m 35s\tremaining: 6.05s\n",
      "2471:\tlearn: 0.5481452\ttotal: 8m 35s\tremaining: 5.84s\n",
      "2472:\tlearn: 0.5480652\ttotal: 8m 36s\tremaining: 5.63s\n",
      "2473:\tlearn: 0.5479853\ttotal: 8m 36s\tremaining: 5.42s\n",
      "2474:\tlearn: 0.5479301\ttotal: 8m 36s\tremaining: 5.22s\n",
      "2475:\tlearn: 0.5478758\ttotal: 8m 36s\tremaining: 5.01s\n",
      "2476:\tlearn: 0.5478082\ttotal: 8m 36s\tremaining: 4.8s\n",
      "2477:\tlearn: 0.5477474\ttotal: 8m 37s\tremaining: 4.59s\n",
      "2478:\tlearn: 0.5477104\ttotal: 8m 37s\tremaining: 4.38s\n",
      "2479:\tlearn: 0.5476535\ttotal: 8m 37s\tremaining: 4.17s\n",
      "2480:\tlearn: 0.5475791\ttotal: 8m 37s\tremaining: 3.96s\n",
      "2481:\tlearn: 0.5475133\ttotal: 8m 37s\tremaining: 3.76s\n",
      "2482:\tlearn: 0.5474524\ttotal: 8m 38s\tremaining: 3.55s\n",
      "2483:\tlearn: 0.5473811\ttotal: 8m 38s\tremaining: 3.34s\n",
      "2484:\tlearn: 0.5473161\ttotal: 8m 38s\tremaining: 3.13s\n",
      "2485:\tlearn: 0.5472544\ttotal: 8m 38s\tremaining: 2.92s\n",
      "2486:\tlearn: 0.5471620\ttotal: 8m 39s\tremaining: 2.71s\n",
      "2487:\tlearn: 0.5471152\ttotal: 8m 39s\tremaining: 2.5s\n",
      "2488:\tlearn: 0.5470724\ttotal: 8m 39s\tremaining: 2.29s\n",
      "2489:\tlearn: 0.5470090\ttotal: 8m 39s\tremaining: 2.09s\n",
      "2490:\tlearn: 0.5469595\ttotal: 8m 39s\tremaining: 1.88s\n",
      "2491:\tlearn: 0.5468949\ttotal: 8m 40s\tremaining: 1.67s\n",
      "2492:\tlearn: 0.5468335\ttotal: 8m 40s\tremaining: 1.46s\n",
      "2493:\tlearn: 0.5467598\ttotal: 8m 40s\tremaining: 1.25s\n",
      "2494:\tlearn: 0.5467016\ttotal: 8m 40s\tremaining: 1.04s\n",
      "2495:\tlearn: 0.5466340\ttotal: 8m 40s\tremaining: 835ms\n",
      "2496:\tlearn: 0.5465669\ttotal: 8m 41s\tremaining: 626ms\n",
      "2497:\tlearn: 0.5464906\ttotal: 8m 41s\tremaining: 417ms\n",
      "2498:\tlearn: 0.5464326\ttotal: 8m 41s\tremaining: 209ms\n",
      "2499:\tlearn: 0.5463834\ttotal: 8m 41s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/Case Study 01/Data/catboost_Model02_Rev02.sav']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_reg = CatBoostRegressor(iterations=None, learning_rate=0.01, n_estimators = 2500,\n",
    "                                 depth=15,loss_function='RMSE', task_type = 'GPU')\n",
    "\n",
    "catboost_reg.fit(X_train_arr, y_train_arr)\n",
    "\n",
    "# save the model to disk\n",
    "filename = '/content/drive/MyDrive/Case Study 01/Data/catboost_Model02_Rev02.sav'\n",
    "joblib.dump(catboost_reg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206102,
     "status": "ok",
     "timestamp": 1639656295032,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "igvC1I8QDqZb",
    "outputId": "e20a6ac2-7504-449e-dfe7-9760a155f46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For catboost_reg the RMSLE values are \n",
      "    train RMSLE = 0.14 \n",
      "    CV RMSLE = 0.17\n",
      "--------------------------------------------------\n",
      "2.For catboost_reg the RMSE values are \n",
      "    train RMSE = 0.55 \n",
      "    CV RMSLE = 0.67\n",
      "--------------------------------------------------\n",
      "3.For catboost_reg the RMSLE(CV) values are \n",
      "    train RMSE(CV) = 0.12  \n",
      "    CV RMSE(CV) = 0.15 \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "catboost_reg = joblib.load('/content/drive/MyDrive/Case Study 01/Data/catboost_Model02_Rev02.sav')\n",
    "\n",
    "#Predictions\n",
    "y_cv_pred = catboost_reg.predict(X_cv)\n",
    "y_train_pred = catboost_reg.predict(X_train)\n",
    "\n",
    "#Evaluation\n",
    "catboost_reg_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "catboost_reg_train_RMSLE = compute_RMSLE(y_train[:], y_train_pred)\n",
    "\n",
    "catboost_reg_cv_RMSE = compute_RMSE(y_cv[:], y_cv_pred)\n",
    "catboost_reg_train_RMSE = compute_RMSE(y_train[:], y_train_pred)\n",
    "\n",
    "catboost_reg_cv_RMSECV = compute_RMSE_CV(y_cv, y_cv_pred)\n",
    "catboost_reg_train_RMSECV = compute_RMSE_CV(y_train, y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"1.For catboost_reg the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(catboost_reg_train_RMSLE, catboost_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For catboost_reg the RMSE values are \\n    train RMSE = {0} \\n    CV RMSLE = {1}\".format(catboost_reg_train_RMSE, catboost_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "print(\"3.For catboost_reg the RMSLE(CV) values are \\n    train RMSE(CV) = {0}  \\n    CV RMSE(CV) = {1} \".format(catboost_reg_train_RMSECV, catboost_reg_cv_RMSECV))\n",
    "print(\"--\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvTFp6P3DpRD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ck-3KDapB_aB"
   },
   "source": [
    "### **4.6 MLP Regression:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sht7LGmIVfQn"
   },
   "source": [
    "##### **Defining Custom Matrics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_igy4kKOL04q"
   },
   "outputs": [],
   "source": [
    "tensorflow.keras.backend.clear_session()\n",
    "\n",
    "if 'MLP_reg' in locals():\n",
    "  del(MLP_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxuQpmUoCO_k"
   },
   "outputs": [],
   "source": [
    "# Root Mean Square Error\n",
    "def RMSE_error(y_true, y_pred):\n",
    "\n",
    "  RMSE_Error = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "  return RMSE_Error\n",
    "\n",
    "#Root Mean Square Log. Error\n",
    "def RMSLE_error(y_true, y_pred):\n",
    "  #Reducing negative predicted values to 0.\n",
    "  #Reference:https://stackoverflow.com/questions/41043894/setting-all-negative-values-of-a-tensor-to-zero-in-tensorflow\n",
    "  y_pred_ = tensorflow.nn.relu(y_pred)\n",
    "\n",
    "  RMSLE_Error = K.sqrt(K.mean(K.square(K.log(1+ y_true)-K.log(1+y_pred_))))\n",
    "  return RMSLE_Error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9w2kWT7yFGcW"
   },
   "outputs": [],
   "source": [
    "#Reference:https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
    "# plot diagnostic learning curves\n",
    "import matplotlib.pyplot as plt\n",
    "def summarize_diagnostics(history):\n",
    "  #plot Loss\n",
    "  plt.figure(figsize=(10,10))\n",
    "  plt.subplot(211)\n",
    "  plt.title('MSE Loss')\n",
    "  plt.plot(history.history['loss'], color='blue', label='train')\n",
    "  plt.plot(history.history['val_loss'], color='orange', label='test')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "\n",
    "  # plot accuracy\n",
    "  #plt.subplot(212)\n",
    "  #plt.title('Classification Accuracy')\n",
    "  #plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "  #plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "  #plt.xlabel(\"Epochs\")\n",
    "\n",
    "  #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI3ifdu3Os_l"
   },
   "source": [
    "##### **4.6.1 Model 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUCpcWQtE2ri"
   },
   "source": [
    "**Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05XzQOS_UQqk"
   },
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "\n",
    "MLP_reg = Sequential()\n",
    "MLP_reg.add(Dense(128, kernel_initializer='normal',input_dim =X_train_lr.shape[1], activation='relu'))\n",
    "\n",
    "MLP_reg.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "MLP_reg.add(BatchNormalization())\n",
    "MLP_reg.add(Dropout(0.2))\n",
    "\n",
    "MLP_reg.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "MLP_reg.add(BatchNormalization())\n",
    "MLP_reg.add(Dropout(0.4))\n",
    "\n",
    "MLP_reg.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
    "MLP_reg.add(BatchNormalization())\n",
    "MLP_reg.add(Dropout(0.5))\n",
    "\n",
    "MLP_reg.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "MLP_reg.add(BatchNormalization())\n",
    "MLP_reg.add(Dropout(0.5))\n",
    "\n",
    "MLP_reg.add(Dense(128, kernel_initializer='normal',input_dim =X_train_lr.shape[1], activation='relu'))\n",
    "MLP_reg.add(BatchNormalization())\n",
    "MLP_reg.add(Dropout(0.4))\n",
    "\n",
    "MLP_reg.add(Dense(64, kernel_initializer='normal',input_dim =X_train_lr.shape[1], activation='relu'))\n",
    "MLP_reg.add(BatchNormalization())\n",
    "MLP_reg.add(Dropout(0.3))\n",
    "\n",
    "MLP_reg.add(Dense(16, kernel_initializer='normal',input_dim =X_train_lr.shape[1], activation='relu'))\n",
    "\n",
    "MLP_reg.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6808335,
     "status": "ok",
     "timestamp": 1639684425632,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "yI9iRLqXTGrK",
    "outputId": "80feab5d-112a-4a15-dcb3-07cb5196e185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.3446\n",
      "Epoch 00001: val_loss improved from inf to 1.87285, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 26s 159ms/step - loss: 1.3446 - val_loss: 1.8729 - lr: 0.0100\n",
      "Epoch 2/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.1105\n",
      "Epoch 00002: val_loss improved from 1.87285 to 1.19098, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 158ms/step - loss: 1.1105 - val_loss: 1.1910 - lr: 0.0100\n",
      "Epoch 3/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0050\n",
      "Epoch 00003: val_loss improved from 1.19098 to 1.03523, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 1.0050 - val_loss: 1.0352 - lr: 0.0100\n",
      "Epoch 4/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9339\n",
      "Epoch 00004: val_loss improved from 1.03523 to 0.94440, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 158ms/step - loss: 0.9339 - val_loss: 0.9444 - lr: 0.0100\n",
      "Epoch 5/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.8891\n",
      "Epoch 00005: val_loss improved from 0.94440 to 0.91721, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.8891 - val_loss: 0.9172 - lr: 0.0100\n",
      "Epoch 6/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.8619\n",
      "Epoch 00006: val_loss did not improve from 0.91721\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.8619 - val_loss: 0.9633 - lr: 0.0100\n",
      "Epoch 7/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.8446\n",
      "Epoch 00007: val_loss improved from 0.91721 to 0.89463, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 159ms/step - loss: 0.8446 - val_loss: 0.8946 - lr: 0.0100\n",
      "Epoch 8/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.8253\n",
      "Epoch 00008: val_loss improved from 0.89463 to 0.84038, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.8253 - val_loss: 0.8404 - lr: 0.0100\n",
      "Epoch 9/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.8188\n",
      "Epoch 00009: val_loss did not improve from 0.84038\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.8188 - val_loss: 0.9424 - lr: 0.0100\n",
      "Epoch 10/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.8057\n",
      "Epoch 00010: val_loss did not improve from 0.84038\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.8057 - val_loss: 0.8436 - lr: 0.0100\n",
      "Epoch 11/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7976\n",
      "Epoch 00011: val_loss did not improve from 0.84038\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7976 - val_loss: 0.8548 - lr: 0.0100\n",
      "Epoch 12/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7905\n",
      "Epoch 00012: val_loss did not improve from 0.84038\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7905 - val_loss: 0.8481 - lr: 0.0100\n",
      "Epoch 13/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7805\n",
      "Epoch 00013: val_loss improved from 0.84038 to 0.83128, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7805 - val_loss: 0.8313 - lr: 0.0100\n",
      "Epoch 14/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7769\n",
      "Epoch 00014: val_loss did not improve from 0.83128\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7769 - val_loss: 0.8705 - lr: 0.0100\n",
      "Epoch 15/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7708\n",
      "Epoch 00015: val_loss did not improve from 0.83128\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7708 - val_loss: 0.8417 - lr: 0.0100\n",
      "Epoch 16/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7677\n",
      "Epoch 00016: val_loss did not improve from 0.83128\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7677 - val_loss: 0.8320 - lr: 0.0100\n",
      "Epoch 17/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7640\n",
      "Epoch 00017: val_loss did not improve from 0.83128\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7640 - val_loss: 0.8417 - lr: 0.0100\n",
      "Epoch 18/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7560\n",
      "Epoch 00018: val_loss improved from 0.83128 to 0.82733, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7560 - val_loss: 0.8273 - lr: 0.0100\n",
      "Epoch 19/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7527\n",
      "Epoch 00019: val_loss did not improve from 0.82733\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7527 - val_loss: 0.8443 - lr: 0.0100\n",
      "Epoch 20/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7481\n",
      "Epoch 00020: val_loss did not improve from 0.82733\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7481 - val_loss: 0.8981 - lr: 0.0100\n",
      "Epoch 21/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7424\n",
      "Epoch 00021: val_loss improved from 0.82733 to 0.82403, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7424 - val_loss: 0.8240 - lr: 0.0100\n",
      "Epoch 22/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7413\n",
      "Epoch 00022: val_loss did not improve from 0.82403\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7413 - val_loss: 0.8390 - lr: 0.0100\n",
      "Epoch 23/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7378\n",
      "Epoch 00023: val_loss did not improve from 0.82403\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7378 - val_loss: 0.8257 - lr: 0.0100\n",
      "Epoch 24/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7374\n",
      "Epoch 00024: val_loss did not improve from 0.82403\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7374 - val_loss: 0.8269 - lr: 0.0100\n",
      "Epoch 25/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7323\n",
      "Epoch 00025: val_loss improved from 0.82403 to 0.79662, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7323 - val_loss: 0.7966 - lr: 0.0100\n",
      "Epoch 26/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7321\n",
      "Epoch 00026: val_loss improved from 0.79662 to 0.79552, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 158ms/step - loss: 0.7321 - val_loss: 0.7955 - lr: 0.0100\n",
      "Epoch 27/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7251\n",
      "Epoch 00027: val_loss did not improve from 0.79552\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7251 - val_loss: 0.8002 - lr: 0.0100\n",
      "Epoch 28/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7249\n",
      "Epoch 00028: val_loss did not improve from 0.79552\n",
      "146/146 [==============================] - 22s 154ms/step - loss: 0.7249 - val_loss: 0.7964 - lr: 0.0100\n",
      "Epoch 29/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7242\n",
      "Epoch 00029: val_loss did not improve from 0.79552\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7242 - val_loss: 0.8127 - lr: 0.0100\n",
      "Epoch 30/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7195\n",
      "Epoch 00030: val_loss improved from 0.79552 to 0.79348, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7195 - val_loss: 0.7935 - lr: 0.0100\n",
      "Epoch 31/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7179\n",
      "Epoch 00031: val_loss did not improve from 0.79348\n",
      "146/146 [==============================] - 22s 154ms/step - loss: 0.7179 - val_loss: 0.8073 - lr: 0.0100\n",
      "Epoch 32/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7156\n",
      "Epoch 00032: val_loss did not improve from 0.79348\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7156 - val_loss: 0.7997 - lr: 0.0100\n",
      "Epoch 33/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7133\n",
      "Epoch 00033: val_loss did not improve from 0.79348\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7133 - val_loss: 0.8032 - lr: 0.0100\n",
      "Epoch 34/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7143\n",
      "Epoch 00034: val_loss did not improve from 0.79348\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7143 - val_loss: 0.8105 - lr: 0.0100\n",
      "Epoch 35/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7133\n",
      "Epoch 00035: val_loss improved from 0.79348 to 0.79301, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7133 - val_loss: 0.7930 - lr: 0.0100\n",
      "Epoch 36/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7076\n",
      "Epoch 00036: val_loss improved from 0.79301 to 0.78716, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.7076 - val_loss: 0.7872 - lr: 0.0100\n",
      "Epoch 37/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7120\n",
      "Epoch 00037: val_loss did not improve from 0.78716\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7120 - val_loss: 0.7951 - lr: 0.0100\n",
      "Epoch 38/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7024\n",
      "Epoch 00038: val_loss did not improve from 0.78716\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7024 - val_loss: 0.7902 - lr: 0.0100\n",
      "Epoch 39/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7035\n",
      "Epoch 00039: val_loss improved from 0.78716 to 0.78628, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 158ms/step - loss: 0.7035 - val_loss: 0.7863 - lr: 0.0100\n",
      "Epoch 40/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7024\n",
      "Epoch 00040: val_loss did not improve from 0.78628\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.7024 - val_loss: 0.8383 - lr: 0.0100\n",
      "Epoch 41/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6995\n",
      "Epoch 00041: val_loss did not improve from 0.78628\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6995 - val_loss: 0.8084 - lr: 0.0100\n",
      "Epoch 42/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.7012\n",
      "Epoch 00042: val_loss did not improve from 0.78628\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.7012 - val_loss: 0.8064 - lr: 0.0100\n",
      "Epoch 43/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6999\n",
      "Epoch 00043: val_loss did not improve from 0.78628\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6999 - val_loss: 0.7974 - lr: 0.0100\n",
      "Epoch 44/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6985\n",
      "Epoch 00044: val_loss did not improve from 0.78628\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6985 - val_loss: 0.8003 - lr: 0.0100\n",
      "Epoch 45/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6961\n",
      "Epoch 00045: val_loss improved from 0.78628 to 0.78350, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6961 - val_loss: 0.7835 - lr: 0.0100\n",
      "Epoch 46/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6962\n",
      "Epoch 00046: val_loss did not improve from 0.78350\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6962 - val_loss: 0.8110 - lr: 0.0100\n",
      "Epoch 47/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6933\n",
      "Epoch 00047: val_loss improved from 0.78350 to 0.77288, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6933 - val_loss: 0.7729 - lr: 0.0100\n",
      "Epoch 48/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6905\n",
      "Epoch 00048: val_loss did not improve from 0.77288\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6905 - val_loss: 0.7762 - lr: 0.0100\n",
      "Epoch 49/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6908\n",
      "Epoch 00049: val_loss did not improve from 0.77288\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6908 - val_loss: 0.8241 - lr: 0.0100\n",
      "Epoch 50/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6919\n",
      "Epoch 00050: val_loss did not improve from 0.77288\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6919 - val_loss: 0.7866 - lr: 0.0100\n",
      "Epoch 51/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6892\n",
      "Epoch 00051: val_loss did not improve from 0.77288\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6892 - val_loss: 0.7783 - lr: 0.0100\n",
      "Epoch 52/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6887\n",
      "Epoch 00052: val_loss did not improve from 0.77288\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6887 - val_loss: 0.8228 - lr: 0.0100\n",
      "Epoch 53/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6848\n",
      "Epoch 00053: val_loss improved from 0.77288 to 0.77189, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 158ms/step - loss: 0.6848 - val_loss: 0.7719 - lr: 0.0100\n",
      "Epoch 54/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6847\n",
      "Epoch 00054: val_loss did not improve from 0.77189\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6847 - val_loss: 0.7933 - lr: 0.0100\n",
      "Epoch 55/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6865\n",
      "Epoch 00055: val_loss did not improve from 0.77189\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6865 - val_loss: 0.7941 - lr: 0.0100\n",
      "Epoch 56/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6913\n",
      "Epoch 00056: val_loss did not improve from 0.77189\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6913 - val_loss: 0.7777 - lr: 0.0100\n",
      "Epoch 57/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6815\n",
      "Epoch 00057: val_loss did not improve from 0.77189\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.009499999787658453.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6815 - val_loss: 0.7888 - lr: 0.0100\n",
      "Epoch 58/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6790\n",
      "Epoch 00058: val_loss did not improve from 0.77189\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6790 - val_loss: 0.7760 - lr: 0.0095\n",
      "Epoch 59/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6788\n",
      "Epoch 00059: val_loss improved from 0.77189 to 0.76760, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6788 - val_loss: 0.7676 - lr: 0.0095\n",
      "Epoch 60/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6799\n",
      "Epoch 00060: val_loss did not improve from 0.76760\n",
      "146/146 [==============================] - 22s 154ms/step - loss: 0.6799 - val_loss: 0.8202 - lr: 0.0095\n",
      "Epoch 61/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6809\n",
      "Epoch 00061: val_loss did not improve from 0.76760\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6809 - val_loss: 0.7759 - lr: 0.0095\n",
      "Epoch 62/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6769\n",
      "Epoch 00062: val_loss did not improve from 0.76760\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6769 - val_loss: 0.7846 - lr: 0.0095\n",
      "Epoch 63/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6768\n",
      "Epoch 00063: val_loss did not improve from 0.76760\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6768 - val_loss: 0.7703 - lr: 0.0095\n",
      "Epoch 64/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6759\n",
      "Epoch 00064: val_loss did not improve from 0.76760\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6759 - val_loss: 0.7846 - lr: 0.0095\n",
      "Epoch 65/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6744\n",
      "Epoch 00065: val_loss did not improve from 0.76760\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6744 - val_loss: 0.8200 - lr: 0.0095\n",
      "Epoch 66/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6741\n",
      "Epoch 00066: val_loss improved from 0.76760 to 0.76563, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6741 - val_loss: 0.7656 - lr: 0.0095\n",
      "Epoch 67/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6726\n",
      "Epoch 00067: val_loss improved from 0.76563 to 0.76256, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6726 - val_loss: 0.7626 - lr: 0.0095\n",
      "Epoch 68/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6743\n",
      "Epoch 00068: val_loss improved from 0.76256 to 0.76059, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6743 - val_loss: 0.7606 - lr: 0.0095\n",
      "Epoch 69/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6693\n",
      "Epoch 00069: val_loss improved from 0.76059 to 0.75161, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6693 - val_loss: 0.7516 - lr: 0.0095\n",
      "Epoch 70/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6730\n",
      "Epoch 00070: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6730 - val_loss: 0.7817 - lr: 0.0095\n",
      "Epoch 71/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6741\n",
      "Epoch 00071: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6741 - val_loss: 0.7687 - lr: 0.0095\n",
      "Epoch 72/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6673\n",
      "Epoch 00072: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6673 - val_loss: 0.7707 - lr: 0.0095\n",
      "Epoch 73/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6682\n",
      "Epoch 00073: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6682 - val_loss: 0.7701 - lr: 0.0095\n",
      "Epoch 74/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6717\n",
      "Epoch 00074: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 159ms/step - loss: 0.6717 - val_loss: 0.7792 - lr: 0.0095\n",
      "Epoch 75/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6704\n",
      "Epoch 00075: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6704 - val_loss: 0.7629 - lr: 0.0095\n",
      "Epoch 76/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6675\n",
      "Epoch 00076: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6675 - val_loss: 0.7828 - lr: 0.0095\n",
      "Epoch 77/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6684\n",
      "Epoch 00077: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6684 - val_loss: 0.7551 - lr: 0.0095\n",
      "Epoch 78/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6663\n",
      "Epoch 00078: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6663 - val_loss: 0.7798 - lr: 0.0095\n",
      "Epoch 79/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6632\n",
      "Epoch 00079: val_loss did not improve from 0.75161\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.009024999709799886.\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6632 - val_loss: 0.7618 - lr: 0.0095\n",
      "Epoch 80/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6601\n",
      "Epoch 00080: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6601 - val_loss: 0.7619 - lr: 0.0090\n",
      "Epoch 81/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6664\n",
      "Epoch 00081: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 22s 154ms/step - loss: 0.6664 - val_loss: 0.7699 - lr: 0.0090\n",
      "Epoch 82/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6616\n",
      "Epoch 00082: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6616 - val_loss: 0.7529 - lr: 0.0090\n",
      "Epoch 83/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6654\n",
      "Epoch 00083: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6654 - val_loss: 0.7745 - lr: 0.0090\n",
      "Epoch 84/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6627\n",
      "Epoch 00084: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6627 - val_loss: 0.7616 - lr: 0.0090\n",
      "Epoch 85/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6610\n",
      "Epoch 00085: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6610 - val_loss: 0.7687 - lr: 0.0090\n",
      "Epoch 86/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6612\n",
      "Epoch 00086: val_loss did not improve from 0.75161\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6612 - val_loss: 0.7627 - lr: 0.0090\n",
      "Epoch 87/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6581\n",
      "Epoch 00087: val_loss improved from 0.75161 to 0.74788, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6581 - val_loss: 0.7479 - lr: 0.0090\n",
      "Epoch 88/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6614\n",
      "Epoch 00088: val_loss did not improve from 0.74788\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6614 - val_loss: 0.7650 - lr: 0.0090\n",
      "Epoch 89/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6611\n",
      "Epoch 00089: val_loss did not improve from 0.74788\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6611 - val_loss: 0.7559 - lr: 0.0090\n",
      "Epoch 90/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6602\n",
      "Epoch 00090: val_loss did not improve from 0.74788\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6602 - val_loss: 0.7513 - lr: 0.0090\n",
      "Epoch 91/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6588\n",
      "Epoch 00091: val_loss improved from 0.74788 to 0.74675, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6588 - val_loss: 0.7467 - lr: 0.0090\n",
      "Epoch 92/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6613\n",
      "Epoch 00092: val_loss did not improve from 0.74675\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6613 - val_loss: 0.7533 - lr: 0.0090\n",
      "Epoch 93/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6579\n",
      "Epoch 00093: val_loss improved from 0.74675 to 0.73863, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6579 - val_loss: 0.7386 - lr: 0.0090\n",
      "Epoch 94/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6566\n",
      "Epoch 00094: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6566 - val_loss: 0.7427 - lr: 0.0090\n",
      "Epoch 95/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6543\n",
      "Epoch 00095: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6543 - val_loss: 0.7636 - lr: 0.0090\n",
      "Epoch 96/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6631\n",
      "Epoch 00096: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6631 - val_loss: 0.7613 - lr: 0.0090\n",
      "Epoch 97/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6585\n",
      "Epoch 00097: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 22s 154ms/step - loss: 0.6585 - val_loss: 0.7504 - lr: 0.0090\n",
      "Epoch 98/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6569\n",
      "Epoch 00098: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6569 - val_loss: 0.7511 - lr: 0.0090\n",
      "Epoch 99/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6549\n",
      "Epoch 00099: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6549 - val_loss: 0.7561 - lr: 0.0090\n",
      "Epoch 100/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6572\n",
      "Epoch 00100: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6572 - val_loss: 0.7427 - lr: 0.0090\n",
      "Epoch 101/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6541\n",
      "Epoch 00101: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6541 - val_loss: 0.7540 - lr: 0.0090\n",
      "Epoch 102/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6536\n",
      "Epoch 00102: val_loss did not improve from 0.73863\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6536 - val_loss: 0.7606 - lr: 0.0090\n",
      "Epoch 103/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6544\n",
      "Epoch 00103: val_loss did not improve from 0.73863\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.008573750033974648.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6544 - val_loss: 0.7548 - lr: 0.0090\n",
      "Epoch 104/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6516\n",
      "Epoch 00104: val_loss improved from 0.73863 to 0.72392, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6516 - val_loss: 0.7239 - lr: 0.0086\n",
      "Epoch 105/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6536\n",
      "Epoch 00105: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6536 - val_loss: 0.7275 - lr: 0.0086\n",
      "Epoch 106/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6500\n",
      "Epoch 00106: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6500 - val_loss: 0.7694 - lr: 0.0086\n",
      "Epoch 107/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6526\n",
      "Epoch 00107: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6526 - val_loss: 0.7632 - lr: 0.0086\n",
      "Epoch 108/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6498\n",
      "Epoch 00108: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6498 - val_loss: 0.7349 - lr: 0.0086\n",
      "Epoch 109/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6473\n",
      "Epoch 00109: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6473 - val_loss: 0.7301 - lr: 0.0086\n",
      "Epoch 110/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6513\n",
      "Epoch 00110: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6513 - val_loss: 0.7416 - lr: 0.0086\n",
      "Epoch 111/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6518\n",
      "Epoch 00111: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6518 - val_loss: 0.7501 - lr: 0.0086\n",
      "Epoch 112/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6498\n",
      "Epoch 00112: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6498 - val_loss: 0.7457 - lr: 0.0086\n",
      "Epoch 113/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6465\n",
      "Epoch 00113: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6465 - val_loss: 0.7477 - lr: 0.0086\n",
      "Epoch 114/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6477\n",
      "Epoch 00114: val_loss did not improve from 0.72392\n",
      "\n",
      "Epoch 00114: ReduceLROnPlateau reducing learning rate to 0.008145062532275914.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6477 - val_loss: 0.7812 - lr: 0.0086\n",
      "Epoch 115/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6505\n",
      "Epoch 00115: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6505 - val_loss: 0.7455 - lr: 0.0081\n",
      "Epoch 116/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6443\n",
      "Epoch 00116: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6443 - val_loss: 0.7411 - lr: 0.0081\n",
      "Epoch 117/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6469\n",
      "Epoch 00117: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6469 - val_loss: 0.7345 - lr: 0.0081\n",
      "Epoch 118/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6445\n",
      "Epoch 00118: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6445 - val_loss: 0.7415 - lr: 0.0081\n",
      "Epoch 119/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6532\n",
      "Epoch 00119: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6532 - val_loss: 0.7746 - lr: 0.0081\n",
      "Epoch 120/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6453\n",
      "Epoch 00120: val_loss improved from 0.72392 to 0.72392, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6453 - val_loss: 0.7239 - lr: 0.0081\n",
      "Epoch 121/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6444\n",
      "Epoch 00121: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6444 - val_loss: 0.7315 - lr: 0.0081\n",
      "Epoch 122/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6445\n",
      "Epoch 00122: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6445 - val_loss: 0.7371 - lr: 0.0081\n",
      "Epoch 123/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6441\n",
      "Epoch 00123: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6441 - val_loss: 0.7533 - lr: 0.0081\n",
      "Epoch 124/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6447\n",
      "Epoch 00124: val_loss did not improve from 0.72392\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.0077378091402351854.\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6447 - val_loss: 0.7473 - lr: 0.0081\n",
      "Epoch 125/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6429\n",
      "Epoch 00125: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6429 - val_loss: 0.7502 - lr: 0.0077\n",
      "Epoch 126/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6435\n",
      "Epoch 00126: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6435 - val_loss: 0.7363 - lr: 0.0077\n",
      "Epoch 127/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6411\n",
      "Epoch 00127: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6411 - val_loss: 0.7450 - lr: 0.0077\n",
      "Epoch 128/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6394\n",
      "Epoch 00128: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6394 - val_loss: 0.7354 - lr: 0.0077\n",
      "Epoch 129/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6421\n",
      "Epoch 00129: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6421 - val_loss: 0.7534 - lr: 0.0077\n",
      "Epoch 130/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6414\n",
      "Epoch 00130: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6414 - val_loss: 0.7509 - lr: 0.0077\n",
      "Epoch 131/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6424\n",
      "Epoch 00131: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6424 - val_loss: 0.7407 - lr: 0.0077\n",
      "Epoch 132/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6428\n",
      "Epoch 00132: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6428 - val_loss: 0.7434 - lr: 0.0077\n",
      "Epoch 133/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6412\n",
      "Epoch 00133: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6412 - val_loss: 0.7431 - lr: 0.0077\n",
      "Epoch 134/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6399\n",
      "Epoch 00134: val_loss did not improve from 0.72392\n",
      "\n",
      "Epoch 00134: ReduceLROnPlateau reducing learning rate to 0.007350918860174715.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6399 - val_loss: 0.7311 - lr: 0.0077\n",
      "Epoch 135/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6372\n",
      "Epoch 00135: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6372 - val_loss: 0.7444 - lr: 0.0074\n",
      "Epoch 136/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6379\n",
      "Epoch 00136: val_loss did not improve from 0.72392\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6379 - val_loss: 0.7438 - lr: 0.0074\n",
      "Epoch 137/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6379\n",
      "Epoch 00137: val_loss improved from 0.72392 to 0.71837, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6379 - val_loss: 0.7184 - lr: 0.0074\n",
      "Epoch 138/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6382\n",
      "Epoch 00138: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6382 - val_loss: 0.7289 - lr: 0.0074\n",
      "Epoch 139/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6381\n",
      "Epoch 00139: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6381 - val_loss: 0.7341 - lr: 0.0074\n",
      "Epoch 140/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6380\n",
      "Epoch 00140: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6380 - val_loss: 0.7371 - lr: 0.0074\n",
      "Epoch 141/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6392\n",
      "Epoch 00141: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6392 - val_loss: 0.7646 - lr: 0.0074\n",
      "Epoch 142/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6376\n",
      "Epoch 00142: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6376 - val_loss: 0.7188 - lr: 0.0074\n",
      "Epoch 143/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6360\n",
      "Epoch 00143: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6360 - val_loss: 0.7322 - lr: 0.0074\n",
      "Epoch 144/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6380\n",
      "Epoch 00144: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6380 - val_loss: 0.7231 - lr: 0.0074\n",
      "Epoch 145/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6380\n",
      "Epoch 00145: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6380 - val_loss: 0.7285 - lr: 0.0074\n",
      "Epoch 146/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6359\n",
      "Epoch 00146: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6359 - val_loss: 0.7376 - lr: 0.0074\n",
      "Epoch 147/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6365\n",
      "Epoch 00147: val_loss did not improve from 0.71837\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.006983372895047068.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6365 - val_loss: 0.7286 - lr: 0.0074\n",
      "Epoch 148/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6338\n",
      "Epoch 00148: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6338 - val_loss: 0.7246 - lr: 0.0070\n",
      "Epoch 149/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6344\n",
      "Epoch 00149: val_loss did not improve from 0.71837\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6344 - val_loss: 0.7370 - lr: 0.0070\n",
      "Epoch 150/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6354\n",
      "Epoch 00150: val_loss improved from 0.71837 to 0.71255, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6354 - val_loss: 0.7126 - lr: 0.0070\n",
      "Epoch 151/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6340\n",
      "Epoch 00151: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6340 - val_loss: 0.7280 - lr: 0.0070\n",
      "Epoch 152/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6382\n",
      "Epoch 00152: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6382 - val_loss: 0.7398 - lr: 0.0070\n",
      "Epoch 153/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6321\n",
      "Epoch 00153: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6321 - val_loss: 0.7418 - lr: 0.0070\n",
      "Epoch 154/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6339\n",
      "Epoch 00154: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6339 - val_loss: 0.7349 - lr: 0.0070\n",
      "Epoch 155/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6320\n",
      "Epoch 00155: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 154ms/step - loss: 0.6320 - val_loss: 0.7216 - lr: 0.0070\n",
      "Epoch 156/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6330\n",
      "Epoch 00156: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6330 - val_loss: 0.7392 - lr: 0.0070\n",
      "Epoch 157/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6321\n",
      "Epoch 00157: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6321 - val_loss: 0.7395 - lr: 0.0070\n",
      "Epoch 158/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6324\n",
      "Epoch 00158: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6324 - val_loss: 0.7381 - lr: 0.0070\n",
      "Epoch 159/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6331\n",
      "Epoch 00159: val_loss did not improve from 0.71255\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6331 - val_loss: 0.7588 - lr: 0.0070\n",
      "Epoch 160/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6332\n",
      "Epoch 00160: val_loss did not improve from 0.71255\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 0.006634204206056892.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6332 - val_loss: 0.7358 - lr: 0.0070\n",
      "Epoch 161/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6320\n",
      "Epoch 00161: val_loss improved from 0.71255 to 0.71134, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6320 - val_loss: 0.7113 - lr: 0.0066\n",
      "Epoch 162/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6305\n",
      "Epoch 00162: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6305 - val_loss: 0.7124 - lr: 0.0066\n",
      "Epoch 163/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6313\n",
      "Epoch 00163: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6313 - val_loss: 0.7190 - lr: 0.0066\n",
      "Epoch 164/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6312\n",
      "Epoch 00164: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6312 - val_loss: 0.7160 - lr: 0.0066\n",
      "Epoch 165/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6322\n",
      "Epoch 00165: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6322 - val_loss: 0.7210 - lr: 0.0066\n",
      "Epoch 166/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6312\n",
      "Epoch 00166: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6312 - val_loss: 0.7549 - lr: 0.0066\n",
      "Epoch 167/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6294\n",
      "Epoch 00167: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6294 - val_loss: 0.7308 - lr: 0.0066\n",
      "Epoch 168/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6321\n",
      "Epoch 00168: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6321 - val_loss: 0.7315 - lr: 0.0066\n",
      "Epoch 169/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6313\n",
      "Epoch 00169: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6313 - val_loss: 0.7206 - lr: 0.0066\n",
      "Epoch 170/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6310\n",
      "Epoch 00170: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6310 - val_loss: 0.7213 - lr: 0.0066\n",
      "Epoch 171/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6302\n",
      "Epoch 00171: val_loss did not improve from 0.71134\n",
      "\n",
      "Epoch 00171: ReduceLROnPlateau reducing learning rate to 0.006302493973635137.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6302 - val_loss: 0.7196 - lr: 0.0066\n",
      "Epoch 172/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6276\n",
      "Epoch 00172: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6276 - val_loss: 0.7274 - lr: 0.0063\n",
      "Epoch 173/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6270\n",
      "Epoch 00173: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6270 - val_loss: 0.7151 - lr: 0.0063\n",
      "Epoch 174/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6289\n",
      "Epoch 00174: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6289 - val_loss: 0.7242 - lr: 0.0063\n",
      "Epoch 175/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6280\n",
      "Epoch 00175: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6280 - val_loss: 0.7290 - lr: 0.0063\n",
      "Epoch 176/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6276\n",
      "Epoch 00176: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6276 - val_loss: 0.7184 - lr: 0.0063\n",
      "Epoch 177/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6284\n",
      "Epoch 00177: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6284 - val_loss: 0.7344 - lr: 0.0063\n",
      "Epoch 178/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6274\n",
      "Epoch 00178: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6274 - val_loss: 0.7253 - lr: 0.0063\n",
      "Epoch 179/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6266\n",
      "Epoch 00179: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6266 - val_loss: 0.7225 - lr: 0.0063\n",
      "Epoch 180/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6268\n",
      "Epoch 00180: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6268 - val_loss: 0.7227 - lr: 0.0063\n",
      "Epoch 181/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6278\n",
      "Epoch 00181: val_loss did not improve from 0.71134\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.005987369385547936.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6278 - val_loss: 0.7230 - lr: 0.0063\n",
      "Epoch 182/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6268\n",
      "Epoch 00182: val_loss did not improve from 0.71134\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6268 - val_loss: 0.7152 - lr: 0.0060\n",
      "Epoch 183/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6261\n",
      "Epoch 00183: val_loss improved from 0.71134 to 0.70828, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6261 - val_loss: 0.7083 - lr: 0.0060\n",
      "Epoch 184/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6261\n",
      "Epoch 00184: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6261 - val_loss: 0.7202 - lr: 0.0060\n",
      "Epoch 185/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6253\n",
      "Epoch 00185: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6253 - val_loss: 0.7252 - lr: 0.0060\n",
      "Epoch 186/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6248\n",
      "Epoch 00186: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6248 - val_loss: 0.7214 - lr: 0.0060\n",
      "Epoch 187/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6256\n",
      "Epoch 00187: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6256 - val_loss: 0.7225 - lr: 0.0060\n",
      "Epoch 188/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6294\n",
      "Epoch 00188: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6294 - val_loss: 0.7592 - lr: 0.0060\n",
      "Epoch 189/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6271\n",
      "Epoch 00189: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6271 - val_loss: 0.7134 - lr: 0.0060\n",
      "Epoch 190/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6236\n",
      "Epoch 00190: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6236 - val_loss: 0.7196 - lr: 0.0060\n",
      "Epoch 191/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6245\n",
      "Epoch 00191: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6245 - val_loss: 0.7153 - lr: 0.0060\n",
      "Epoch 192/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6254\n",
      "Epoch 00192: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6254 - val_loss: 0.7221 - lr: 0.0060\n",
      "Epoch 193/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6255\n",
      "Epoch 00193: val_loss did not improve from 0.70828\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.005688000982627272.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6255 - val_loss: 0.7201 - lr: 0.0060\n",
      "Epoch 194/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6222\n",
      "Epoch 00194: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6222 - val_loss: 0.7107 - lr: 0.0057\n",
      "Epoch 195/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6237\n",
      "Epoch 00195: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6237 - val_loss: 0.7303 - lr: 0.0057\n",
      "Epoch 196/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6253\n",
      "Epoch 00196: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6253 - val_loss: 0.7279 - lr: 0.0057\n",
      "Epoch 197/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6229\n",
      "Epoch 00197: val_loss did not improve from 0.70828\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6229 - val_loss: 0.7286 - lr: 0.0057\n",
      "Epoch 198/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6219\n",
      "Epoch 00198: val_loss improved from 0.70828 to 0.70451, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 158ms/step - loss: 0.6219 - val_loss: 0.7045 - lr: 0.0057\n",
      "Epoch 199/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6261\n",
      "Epoch 00199: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6261 - val_loss: 0.7363 - lr: 0.0057\n",
      "Epoch 200/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6218\n",
      "Epoch 00200: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6218 - val_loss: 0.7199 - lr: 0.0057\n",
      "Epoch 201/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6224\n",
      "Epoch 00201: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6224 - val_loss: 0.7286 - lr: 0.0057\n",
      "Epoch 202/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6228\n",
      "Epoch 00202: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6228 - val_loss: 0.7190 - lr: 0.0057\n",
      "Epoch 203/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6208\n",
      "Epoch 00203: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6208 - val_loss: 0.7163 - lr: 0.0057\n",
      "Epoch 204/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6220\n",
      "Epoch 00204: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6220 - val_loss: 0.7379 - lr: 0.0057\n",
      "Epoch 205/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6227\n",
      "Epoch 00205: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6227 - val_loss: 0.7213 - lr: 0.0057\n",
      "Epoch 206/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6200\n",
      "Epoch 00206: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6200 - val_loss: 0.7200 - lr: 0.0057\n",
      "Epoch 207/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6229\n",
      "Epoch 00207: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6229 - val_loss: 0.7219 - lr: 0.0057\n",
      "Epoch 208/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6209\n",
      "Epoch 00208: val_loss did not improve from 0.70451\n",
      "\n",
      "Epoch 00208: ReduceLROnPlateau reducing learning rate to 0.005403600889258086.\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6209 - val_loss: 0.7173 - lr: 0.0057\n",
      "Epoch 209/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6215\n",
      "Epoch 00209: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6215 - val_loss: 0.7127 - lr: 0.0054\n",
      "Epoch 210/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6220\n",
      "Epoch 00210: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6220 - val_loss: 0.7180 - lr: 0.0054\n",
      "Epoch 211/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6191\n",
      "Epoch 00211: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6191 - val_loss: 0.7403 - lr: 0.0054\n",
      "Epoch 212/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6210\n",
      "Epoch 00212: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6210 - val_loss: 0.7100 - lr: 0.0054\n",
      "Epoch 213/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6204\n",
      "Epoch 00213: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6204 - val_loss: 0.7154 - lr: 0.0054\n",
      "Epoch 214/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6212\n",
      "Epoch 00214: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6212 - val_loss: 0.7100 - lr: 0.0054\n",
      "Epoch 215/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6202\n",
      "Epoch 00215: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6202 - val_loss: 0.7179 - lr: 0.0054\n",
      "Epoch 216/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6207\n",
      "Epoch 00216: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6207 - val_loss: 0.7147 - lr: 0.0054\n",
      "Epoch 217/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6186\n",
      "Epoch 00217: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6186 - val_loss: 0.7139 - lr: 0.0054\n",
      "Epoch 218/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6183\n",
      "Epoch 00218: val_loss did not improve from 0.70451\n",
      "\n",
      "Epoch 00218: ReduceLROnPlateau reducing learning rate to 0.005133421043865383.\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6183 - val_loss: 0.7117 - lr: 0.0054\n",
      "Epoch 219/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6181\n",
      "Epoch 00219: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6181 - val_loss: 0.7248 - lr: 0.0051\n",
      "Epoch 220/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6182\n",
      "Epoch 00220: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6182 - val_loss: 0.7179 - lr: 0.0051\n",
      "Epoch 221/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6198\n",
      "Epoch 00221: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6198 - val_loss: 0.7156 - lr: 0.0051\n",
      "Epoch 222/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6181\n",
      "Epoch 00222: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6181 - val_loss: 0.7148 - lr: 0.0051\n",
      "Epoch 223/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6196\n",
      "Epoch 00223: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6196 - val_loss: 0.7192 - lr: 0.0051\n",
      "Epoch 224/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6229\n",
      "Epoch 00224: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6229 - val_loss: 0.7136 - lr: 0.0051\n",
      "Epoch 225/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6187\n",
      "Epoch 00225: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6187 - val_loss: 0.7205 - lr: 0.0051\n",
      "Epoch 226/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6170\n",
      "Epoch 00226: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6170 - val_loss: 0.7192 - lr: 0.0051\n",
      "Epoch 227/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6174\n",
      "Epoch 00227: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6174 - val_loss: 0.7156 - lr: 0.0051\n",
      "Epoch 228/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6171\n",
      "Epoch 00228: val_loss did not improve from 0.70451\n",
      "\n",
      "Epoch 00228: ReduceLROnPlateau reducing learning rate to 0.004876750102266669.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6171 - val_loss: 0.7335 - lr: 0.0051\n",
      "Epoch 229/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6186\n",
      "Epoch 00229: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6186 - val_loss: 0.7151 - lr: 0.0049\n",
      "Epoch 230/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6158\n",
      "Epoch 00230: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6158 - val_loss: 0.7274 - lr: 0.0049\n",
      "Epoch 231/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6161\n",
      "Epoch 00231: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6161 - val_loss: 0.7252 - lr: 0.0049\n",
      "Epoch 232/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6166\n",
      "Epoch 00232: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6166 - val_loss: 0.7184 - lr: 0.0049\n",
      "Epoch 233/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6167\n",
      "Epoch 00233: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6167 - val_loss: 0.7143 - lr: 0.0049\n",
      "Epoch 234/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6169\n",
      "Epoch 00234: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6169 - val_loss: 0.7136 - lr: 0.0049\n",
      "Epoch 235/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6160\n",
      "Epoch 00235: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6160 - val_loss: 0.7227 - lr: 0.0049\n",
      "Epoch 236/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6174\n",
      "Epoch 00236: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6174 - val_loss: 0.7214 - lr: 0.0049\n",
      "Epoch 237/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6155\n",
      "Epoch 00237: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6155 - val_loss: 0.7198 - lr: 0.0049\n",
      "Epoch 238/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6151\n",
      "Epoch 00238: val_loss did not improve from 0.70451\n",
      "\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 0.004632912552915513.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6151 - val_loss: 0.7171 - lr: 0.0049\n",
      "Epoch 239/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6143\n",
      "Epoch 00239: val_loss did not improve from 0.70451\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6143 - val_loss: 0.7130 - lr: 0.0046\n",
      "Epoch 240/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6165\n",
      "Epoch 00240: val_loss improved from 0.70451 to 0.70446, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\n",
      "146/146 [==============================] - 23s 157ms/step - loss: 0.6165 - val_loss: 0.7045 - lr: 0.0046\n",
      "Epoch 241/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6143\n",
      "Epoch 00241: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6143 - val_loss: 0.7370 - lr: 0.0046\n",
      "Epoch 242/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6173\n",
      "Epoch 00242: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6173 - val_loss: 0.7137 - lr: 0.0046\n",
      "Epoch 243/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6132\n",
      "Epoch 00243: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6132 - val_loss: 0.7110 - lr: 0.0046\n",
      "Epoch 244/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6144\n",
      "Epoch 00244: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6144 - val_loss: 0.7190 - lr: 0.0046\n",
      "Epoch 245/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6160\n",
      "Epoch 00245: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6160 - val_loss: 0.7198 - lr: 0.0046\n",
      "Epoch 246/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6140\n",
      "Epoch 00246: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6140 - val_loss: 0.7208 - lr: 0.0046\n",
      "Epoch 247/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6141\n",
      "Epoch 00247: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6141 - val_loss: 0.7171 - lr: 0.0046\n",
      "Epoch 248/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6168\n",
      "Epoch 00248: val_loss did not improve from 0.70446\n",
      "\n",
      "Epoch 00248: ReduceLROnPlateau reducing learning rate to 0.0044012669473886485.\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6168 - val_loss: 0.7273 - lr: 0.0046\n",
      "Epoch 249/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6126\n",
      "Epoch 00249: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6126 - val_loss: 0.7172 - lr: 0.0044\n",
      "Epoch 250/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6135\n",
      "Epoch 00250: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6135 - val_loss: 0.7250 - lr: 0.0044\n",
      "Epoch 251/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6120\n",
      "Epoch 00251: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6120 - val_loss: 0.7173 - lr: 0.0044\n",
      "Epoch 252/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6134\n",
      "Epoch 00252: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6134 - val_loss: 0.7177 - lr: 0.0044\n",
      "Epoch 253/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6154\n",
      "Epoch 00253: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6154 - val_loss: 0.7125 - lr: 0.0044\n",
      "Epoch 254/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6129\n",
      "Epoch 00254: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6129 - val_loss: 0.7198 - lr: 0.0044\n",
      "Epoch 255/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6132\n",
      "Epoch 00255: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6132 - val_loss: 0.7093 - lr: 0.0044\n",
      "Epoch 256/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6138\n",
      "Epoch 00256: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6138 - val_loss: 0.7167 - lr: 0.0044\n",
      "Epoch 257/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6129\n",
      "Epoch 00257: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6129 - val_loss: 0.7176 - lr: 0.0044\n",
      "Epoch 258/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6128\n",
      "Epoch 00258: val_loss did not improve from 0.70446\n",
      "\n",
      "Epoch 00258: ReduceLROnPlateau reducing learning rate to 0.004181203688494861.\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6128 - val_loss: 0.7303 - lr: 0.0044\n",
      "Epoch 259/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6103\n",
      "Epoch 00259: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6103 - val_loss: 0.7213 - lr: 0.0042\n",
      "Epoch 260/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6107\n",
      "Epoch 00260: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6107 - val_loss: 0.7188 - lr: 0.0042\n",
      "Epoch 261/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6140\n",
      "Epoch 00261: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6140 - val_loss: 0.7173 - lr: 0.0042\n",
      "Epoch 262/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6113\n",
      "Epoch 00262: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6113 - val_loss: 0.7194 - lr: 0.0042\n",
      "Epoch 263/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6115\n",
      "Epoch 00263: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6115 - val_loss: 0.7211 - lr: 0.0042\n",
      "Epoch 264/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6138\n",
      "Epoch 00264: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6138 - val_loss: 0.7168 - lr: 0.0042\n",
      "Epoch 265/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6121\n",
      "Epoch 00265: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6121 - val_loss: 0.7143 - lr: 0.0042\n",
      "Epoch 266/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6100\n",
      "Epoch 00266: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6100 - val_loss: 0.7169 - lr: 0.0042\n",
      "Epoch 267/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6118\n",
      "Epoch 00267: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6118 - val_loss: 0.7118 - lr: 0.0042\n",
      "Epoch 268/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6112\n",
      "Epoch 00268: val_loss did not improve from 0.70446\n",
      "\n",
      "Epoch 00268: ReduceLROnPlateau reducing learning rate to 0.003972143703140319.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6112 - val_loss: 0.7211 - lr: 0.0042\n",
      "Epoch 269/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6097\n",
      "Epoch 00269: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6097 - val_loss: 0.7174 - lr: 0.0040\n",
      "Epoch 270/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6096\n",
      "Epoch 00270: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6096 - val_loss: 0.7135 - lr: 0.0040\n",
      "Epoch 271/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6120\n",
      "Epoch 00271: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6120 - val_loss: 0.7191 - lr: 0.0040\n",
      "Epoch 272/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6096\n",
      "Epoch 00272: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6096 - val_loss: 0.7267 - lr: 0.0040\n",
      "Epoch 273/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6108\n",
      "Epoch 00273: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6108 - val_loss: 0.7203 - lr: 0.0040\n",
      "Epoch 274/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6132\n",
      "Epoch 00274: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6132 - val_loss: 0.7107 - lr: 0.0040\n",
      "Epoch 275/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6088\n",
      "Epoch 00275: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6088 - val_loss: 0.7106 - lr: 0.0040\n",
      "Epoch 276/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6129\n",
      "Epoch 00276: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6129 - val_loss: 0.7534 - lr: 0.0040\n",
      "Epoch 277/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6095\n",
      "Epoch 00277: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6095 - val_loss: 0.7148 - lr: 0.0040\n",
      "Epoch 278/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6087\n",
      "Epoch 00278: val_loss did not improve from 0.70446\n",
      "\n",
      "Epoch 00278: ReduceLROnPlateau reducing learning rate to 0.0037735366728156804.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6087 - val_loss: 0.7216 - lr: 0.0040\n",
      "Epoch 279/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6075\n",
      "Epoch 00279: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6075 - val_loss: 0.7236 - lr: 0.0038\n",
      "Epoch 280/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6088\n",
      "Epoch 00280: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6088 - val_loss: 0.7247 - lr: 0.0038\n",
      "Epoch 281/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6095\n",
      "Epoch 00281: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6095 - val_loss: 0.7199 - lr: 0.0038\n",
      "Epoch 282/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6097\n",
      "Epoch 00282: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6097 - val_loss: 0.7165 - lr: 0.0038\n",
      "Epoch 283/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6079\n",
      "Epoch 00283: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6079 - val_loss: 0.7090 - lr: 0.0038\n",
      "Epoch 284/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6084\n",
      "Epoch 00284: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6084 - val_loss: 0.7161 - lr: 0.0038\n",
      "Epoch 285/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6103\n",
      "Epoch 00285: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6103 - val_loss: 0.7109 - lr: 0.0038\n",
      "Epoch 286/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6069\n",
      "Epoch 00286: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6069 - val_loss: 0.7159 - lr: 0.0038\n",
      "Epoch 287/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6105\n",
      "Epoch 00287: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6105 - val_loss: 0.7115 - lr: 0.0038\n",
      "Epoch 288/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6069\n",
      "Epoch 00288: val_loss did not improve from 0.70446\n",
      "\n",
      "Epoch 00288: ReduceLROnPlateau reducing learning rate to 0.0035848599276505407.\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6069 - val_loss: 0.7127 - lr: 0.0038\n",
      "Epoch 289/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6064\n",
      "Epoch 00289: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6064 - val_loss: 0.7142 - lr: 0.0036\n",
      "Epoch 290/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6079\n",
      "Epoch 00290: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6079 - val_loss: 0.7213 - lr: 0.0036\n",
      "Epoch 291/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6078\n",
      "Epoch 00291: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6078 - val_loss: 0.7161 - lr: 0.0036\n",
      "Epoch 292/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6086\n",
      "Epoch 00292: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6086 - val_loss: 0.7166 - lr: 0.0036\n",
      "Epoch 293/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6090\n",
      "Epoch 00293: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6090 - val_loss: 0.7203 - lr: 0.0036\n",
      "Epoch 294/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6068\n",
      "Epoch 00294: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6068 - val_loss: 0.7078 - lr: 0.0036\n",
      "Epoch 295/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6065\n",
      "Epoch 00295: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6065 - val_loss: 0.7107 - lr: 0.0036\n",
      "Epoch 296/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6055\n",
      "Epoch 00296: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6055 - val_loss: 0.7207 - lr: 0.0036\n",
      "Epoch 297/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6078\n",
      "Epoch 00297: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6078 - val_loss: 0.7236 - lr: 0.0036\n",
      "Epoch 298/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6064\n",
      "Epoch 00298: val_loss did not improve from 0.70446\n",
      "\n",
      "Epoch 00298: ReduceLROnPlateau reducing learning rate to 0.003405616898089647.\n",
      "146/146 [==============================] - 23s 156ms/step - loss: 0.6064 - val_loss: 0.7330 - lr: 0.0036\n",
      "Epoch 299/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6068\n",
      "Epoch 00299: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6068 - val_loss: 0.7152 - lr: 0.0034\n",
      "Epoch 300/300\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.6060\n",
      "Epoch 00300: val_loss did not improve from 0.70446\n",
      "146/146 [==============================] - 23s 155ms/step - loss: 0.6060 - val_loss: 0.7281 - lr: 0.0034\n",
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Case Study 01/MLP Output//MLP_reg_normal_Rev02/assets\n"
     ]
    }
   ],
   "source": [
    "epoch = 300\n",
    "optimizer = tensorflow.keras.optimizers.Adam( learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "\n",
    "\n",
    "# Model Compilation\n",
    "#MLP_reg.compile(optimizer=optimizer, loss='mse', metrics=[RMSE_error])\n",
    "MLP_reg.compile(optimizer=optimizer, loss= RMSE_error)\n",
    "\n",
    "\n",
    "#Saving Best Model and Representation of results\n",
    "filepath = \"/content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, save_weights_only=True,\n",
    "                              monitor='val_loss', verbose=1,\n",
    "                              save_best_only=True, mode='min') \n",
    "\n",
    "decay_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=10, \n",
    "                                                verbose=1, mode='min', min_delta=0.001, \n",
    "                                                cooldown=0, min_lr=0.000001)\n",
    "\n",
    "callback_list = [checkpoint, decay_lr]\n",
    "\n",
    "# Fitting Model and Evaluation\n",
    "history = MLP_reg.fit(X_train_lr, y_train_lr, epochs=epoch, \n",
    "          batch_size=100000, validation_data=(X_cv_lr,y_cv_lr),\n",
    "          verbose='auto', callbacks = callback_list)\n",
    "\n",
    "#Saving the model for future use:\n",
    "MLP_reg.save(\"/content/drive/MyDrive/Case Study 01/MLP Output//MLP_reg_normal_Rev02\", save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9LBIrm0SG_a"
   },
   "outputs": [],
   "source": [
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhGE62KkUQuQ"
   },
   "outputs": [],
   "source": [
    "#Loading saved model from disk\n",
    "MLP_reg = models.load_model(\"/content/drive/MyDrive/Case Study 01/MLP Output//MLP_reg_normal_Rev02\", custom_objects={'RMSE_error':RMSE_error, 'RMSLE_error':RMSLE_error})\n",
    "\n",
    "#Loading weights from best model\n",
    "MLP_reg.load_weights(\"/content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\")\n",
    "\n",
    "y_train_pred = MLP_reg.predict(X_train_lr[:])\n",
    "y_cv_pred = MLP_reg.predict(X_cv_lr[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2070,
     "status": "ok",
     "timestamp": 1639746277343,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "AdP-8kVUR2x-",
    "outputId": "07597a65-41b4-4bf8-c348-f10210b0b75f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For MLP Regression Model the RMSLE values are \n",
      "    train RMSLE = 0.14 \n",
      "    CV RMSLE = 0.18\n",
      "--------------------------------------------------\n",
      "2.For MLP Regression Model the RMSE values are \n",
      "    train RMSE = 0.56 \n",
      "    CV RMSE = 0.71\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "MLP_reg_cv_RMSLE = compute_RMSLE(np.array(y_cv_lr), np.array(y_cv_pred))\n",
    "MLP_reg_train_RMSLE = compute_RMSLE(np.array(y_train_lr), np.array(y_train_pred))\n",
    "\n",
    "MLP_reg_cv_RMSE = compute_RMSE(np.array(y_cv_lr), np.array(y_cv_pred))\n",
    "MLP_reg_train_RMSE = compute_RMSE(np.array(y_train_lr), np.array(y_train_pred))\n",
    "\n",
    "#MLP_reg_cv_RMSE_CV = compute_RMSE_CV(np.array(y_cv_lr), np.array(y_cv_pred))\n",
    "#MLP_reg_train_RMSE_CV = compute_RMSE_CV(np.array(y_train_lr), np.array(y_train_pred))\n",
    "\n",
    "#Report\n",
    "print(\"1.For MLP Regression Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(MLP_reg_train_RMSLE, MLP_reg_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For MLP Regression Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(MLP_reg_train_RMSE, MLP_reg_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "#print(\"2.For MLP Regression Model the RMSE(CV) values are \\n    train RMSE(CV) = {0} \\n    CV RMSE(CV) = {1}\".format(MLP_reg_train_RMSE_CV, MLP_reg_cv_RMSE_CV))\n",
    "#print(\"--\"*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSi-kAJjIJ8W"
   },
   "source": [
    "##### **4.6.2 Model 2:LSTM:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q0KK1-O4DuC"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "X_in = Input(shape=(20,1), name='Input')\n",
    "X_LSTM = LSTM(128)(X_in)\n",
    "X_flatten = Flatten(name='text_flatten')(X_in)\n",
    "\n",
    "X_dense_1 = Dense(532, activation='LeakyReLU', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.L1L2(l1=0.000001, l2=0.00001))(X_flatten)\n",
    "X_dropout_1 = Dropout(0.4)(X_dense_1)\n",
    "\n",
    "X_dense_2 = Dense(256, activation='LeakyReLU', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.L1L2(l1=0.000001, l2=0.00001))(X_dropout_1)\n",
    "X_dropout_2 = Dropout(0.4)(X_dense_2)\n",
    "X_bn_1 = BatchNormalization()(X_dropout_2)\n",
    "\n",
    "X_dense_3 =Dense(128, activation='LeakyReLU', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.L1L2(l1=0.000001, l2=0.00001))(X_bn_1)\n",
    "X_bn_2 = Dropout(0.4)(X_dense_3)\n",
    "\n",
    "X_dense_4 =Dense(64, activation='LeakyReLU', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.L1L2(l1=0.000001, l2=0.00001))(X_bn_2)\n",
    "\n",
    "X_out = Dense(1, activation='linear', kernel_initializer='glorot_uniform')(X_dense_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VwKpe9O5T3R"
   },
   "outputs": [],
   "source": [
    "LSTM_MLP_reg = Model(inputs=X_in, outputs=X_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2005928,
     "status": "ok",
     "timestamp": 1639739435394,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "Ybxz72VkrfoV",
    "outputId": "2ad9cb17-d399-43e5-b44e-dabadbdbbc19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.7802\n",
      "Epoch 00001: val_loss improved from inf to 2.52197, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 10s 60ms/step - loss: 1.7784 - val_loss: 2.5220 - lr: 0.0010\n",
      "Epoch 2/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.3945\n",
      "Epoch 00002: val_loss improved from 2.52197 to 2.02505, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.3942 - val_loss: 2.0250 - lr: 0.0010\n",
      "Epoch 3/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.3170\n",
      "Epoch 00003: val_loss improved from 2.02505 to 1.86120, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.3170 - val_loss: 1.8612 - lr: 0.0010\n",
      "Epoch 4/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.2619\n",
      "Epoch 00004: val_loss improved from 1.86120 to 1.77415, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.2618 - val_loss: 1.7741 - lr: 0.0010\n",
      "Epoch 5/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.2250\n",
      "Epoch 00005: val_loss improved from 1.77415 to 1.68868, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.2249 - val_loss: 1.6887 - lr: 0.0010\n",
      "Epoch 6/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.2013\n",
      "Epoch 00006: val_loss improved from 1.68868 to 1.64849, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.2012 - val_loss: 1.6485 - lr: 0.0010\n",
      "Epoch 7/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1847\n",
      "Epoch 00007: val_loss improved from 1.64849 to 1.58888, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.1847 - val_loss: 1.5889 - lr: 0.0010\n",
      "Epoch 8/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1721\n",
      "Epoch 00008: val_loss improved from 1.58888 to 1.54234, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.1721 - val_loss: 1.5423 - lr: 0.0010\n",
      "Epoch 9/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1617\n",
      "Epoch 00009: val_loss improved from 1.54234 to 1.47167, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 1.1616 - val_loss: 1.4717 - lr: 0.0010\n",
      "Epoch 10/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1532\n",
      "Epoch 00010: val_loss improved from 1.47167 to 1.44235, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.1531 - val_loss: 1.4423 - lr: 0.0010\n",
      "Epoch 11/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1451\n",
      "Epoch 00011: val_loss improved from 1.44235 to 1.40458, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.1450 - val_loss: 1.4046 - lr: 0.0010\n",
      "Epoch 12/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1381\n",
      "Epoch 00012: val_loss improved from 1.40458 to 1.39312, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.1381 - val_loss: 1.3931 - lr: 0.0010\n",
      "Epoch 13/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1309\n",
      "Epoch 00013: val_loss improved from 1.39312 to 1.37839, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 59ms/step - loss: 1.1308 - val_loss: 1.3784 - lr: 0.0010\n",
      "Epoch 14/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1243\n",
      "Epoch 00014: val_loss improved from 1.37839 to 1.34416, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.1242 - val_loss: 1.3442 - lr: 0.0010\n",
      "Epoch 15/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1175\n",
      "Epoch 00015: val_loss improved from 1.34416 to 1.33051, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.1174 - val_loss: 1.3305 - lr: 0.0010\n",
      "Epoch 16/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1107\n",
      "Epoch 00016: val_loss improved from 1.33051 to 1.29704, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 1.1108 - val_loss: 1.2970 - lr: 0.0010\n",
      "Epoch 17/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.1044\n",
      "Epoch 00017: val_loss improved from 1.29704 to 1.24488, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.1044 - val_loss: 1.2449 - lr: 0.0010\n",
      "Epoch 18/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0985\n",
      "Epoch 00018: val_loss improved from 1.24488 to 1.22716, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0984 - val_loss: 1.2272 - lr: 0.0010\n",
      "Epoch 19/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0926\n",
      "Epoch 00019: val_loss improved from 1.22716 to 1.18147, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0926 - val_loss: 1.1815 - lr: 0.0010\n",
      "Epoch 20/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0878\n",
      "Epoch 00020: val_loss did not improve from 1.18147\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0877 - val_loss: 1.1885 - lr: 0.0010\n",
      "Epoch 21/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0824\n",
      "Epoch 00021: val_loss improved from 1.18147 to 1.17445, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0824 - val_loss: 1.1744 - lr: 0.0010\n",
      "Epoch 22/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0782\n",
      "Epoch 00022: val_loss improved from 1.17445 to 1.17225, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0782 - val_loss: 1.1723 - lr: 0.0010\n",
      "Epoch 23/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0743\n",
      "Epoch 00023: val_loss improved from 1.17225 to 1.15759, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0743 - val_loss: 1.1576 - lr: 0.0010\n",
      "Epoch 24/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0710\n",
      "Epoch 00024: val_loss improved from 1.15759 to 1.12178, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0709 - val_loss: 1.1218 - lr: 0.0010\n",
      "Epoch 25/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0680\n",
      "Epoch 00025: val_loss did not improve from 1.12178\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0680 - val_loss: 1.1398 - lr: 0.0010\n",
      "Epoch 26/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0654\n",
      "Epoch 00026: val_loss improved from 1.12178 to 1.10760, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 1.0654 - val_loss: 1.1076 - lr: 0.0010\n",
      "Epoch 27/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0629\n",
      "Epoch 00027: val_loss did not improve from 1.10760\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0629 - val_loss: 1.1086 - lr: 0.0010\n",
      "Epoch 28/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0608\n",
      "Epoch 00028: val_loss improved from 1.10760 to 1.10460, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 1.0607 - val_loss: 1.1046 - lr: 0.0010\n",
      "Epoch 29/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0589\n",
      "Epoch 00029: val_loss improved from 1.10460 to 1.09561, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0589 - val_loss: 1.0956 - lr: 0.0010\n",
      "Epoch 30/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0567\n",
      "Epoch 00030: val_loss improved from 1.09561 to 1.07354, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0567 - val_loss: 1.0735 - lr: 0.0010\n",
      "Epoch 31/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0550\n",
      "Epoch 00031: val_loss improved from 1.07354 to 1.07162, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0550 - val_loss: 1.0716 - lr: 0.0010\n",
      "Epoch 32/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0541\n",
      "Epoch 00032: val_loss did not improve from 1.07162\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0541 - val_loss: 1.0860 - lr: 0.0010\n",
      "Epoch 33/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0519\n",
      "Epoch 00033: val_loss improved from 1.07162 to 1.06350, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 1.0519 - val_loss: 1.0635 - lr: 0.0010\n",
      "Epoch 34/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0507\n",
      "Epoch 00034: val_loss did not improve from 1.06350\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0506 - val_loss: 1.0810 - lr: 0.0010\n",
      "Epoch 35/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0492\n",
      "Epoch 00035: val_loss did not improve from 1.06350\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0492 - val_loss: 1.0799 - lr: 0.0010\n",
      "Epoch 36/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0478\n",
      "Epoch 00036: val_loss improved from 1.06350 to 1.06009, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 1.0478 - val_loss: 1.0601 - lr: 0.0010\n",
      "Epoch 37/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0463\n",
      "Epoch 00037: val_loss did not improve from 1.06009\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0464 - val_loss: 1.0724 - lr: 0.0010\n",
      "Epoch 38/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0453\n",
      "Epoch 00038: val_loss did not improve from 1.06009\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0453 - val_loss: 1.0808 - lr: 0.0010\n",
      "Epoch 39/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0433\n",
      "Epoch 00039: val_loss improved from 1.06009 to 1.05558, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0433 - val_loss: 1.0556 - lr: 9.5000e-04\n",
      "Epoch 40/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0434\n",
      "Epoch 00040: val_loss improved from 1.05558 to 1.04958, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0434 - val_loss: 1.0496 - lr: 9.5000e-04\n",
      "Epoch 41/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0414\n",
      "Epoch 00041: val_loss did not improve from 1.04958\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0414 - val_loss: 1.0659 - lr: 9.5000e-04\n",
      "Epoch 42/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0406\n",
      "Epoch 00042: val_loss did not improve from 1.04958\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0406 - val_loss: 1.0525 - lr: 9.5000e-04\n",
      "Epoch 43/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0398\n",
      "Epoch 00043: val_loss improved from 1.04958 to 1.03038, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0398 - val_loss: 1.0304 - lr: 9.5000e-04\n",
      "Epoch 44/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0388\n",
      "Epoch 00044: val_loss did not improve from 1.03038\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0388 - val_loss: 1.0443 - lr: 9.5000e-04\n",
      "Epoch 45/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0374\n",
      "Epoch 00045: val_loss did not improve from 1.03038\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0373 - val_loss: 1.0452 - lr: 9.5000e-04\n",
      "Epoch 46/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0369\n",
      "Epoch 00046: val_loss did not improve from 1.03038\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0369 - val_loss: 1.0336 - lr: 9.5000e-04\n",
      "Epoch 47/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0360\n",
      "Epoch 00047: val_loss improved from 1.03038 to 1.02332, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0360 - val_loss: 1.0233 - lr: 9.5000e-04\n",
      "Epoch 48/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0355\n",
      "Epoch 00048: val_loss did not improve from 1.02332\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0355 - val_loss: 1.0344 - lr: 9.5000e-04\n",
      "Epoch 49/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0342\n",
      "Epoch 00049: val_loss did not improve from 1.02332\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0342 - val_loss: 1.0327 - lr: 9.0250e-04\n",
      "Epoch 50/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0336\n",
      "Epoch 00050: val_loss improved from 1.02332 to 1.01853, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0335 - val_loss: 1.0185 - lr: 9.0250e-04\n",
      "Epoch 51/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0328\n",
      "Epoch 00051: val_loss did not improve from 1.01853\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0328 - val_loss: 1.0377 - lr: 9.0250e-04\n",
      "Epoch 52/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0323\n",
      "Epoch 00052: val_loss did not improve from 1.01853\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0323 - val_loss: 1.0191 - lr: 9.0250e-04\n",
      "Epoch 53/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0315\n",
      "Epoch 00053: val_loss improved from 1.01853 to 1.01636, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0315 - val_loss: 1.0164 - lr: 9.0250e-04\n",
      "Epoch 54/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0307\n",
      "Epoch 00054: val_loss improved from 1.01636 to 1.01202, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0308 - val_loss: 1.0120 - lr: 9.0250e-04\n",
      "Epoch 55/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0301\n",
      "Epoch 00055: val_loss did not improve from 1.01202\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0301 - val_loss: 1.0371 - lr: 9.0250e-04\n",
      "Epoch 56/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0295\n",
      "Epoch 00056: val_loss did not improve from 1.01202\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0295 - val_loss: 1.0145 - lr: 8.5737e-04\n",
      "Epoch 57/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0289\n",
      "Epoch 00057: val_loss did not improve from 1.01202\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0289 - val_loss: 1.0338 - lr: 8.5737e-04\n",
      "Epoch 58/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0280\n",
      "Epoch 00058: val_loss did not improve from 1.01202\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0280 - val_loss: 1.0266 - lr: 8.5737e-04\n",
      "Epoch 59/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0276\n",
      "Epoch 00059: val_loss did not improve from 1.01202\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0276 - val_loss: 1.0305 - lr: 8.5737e-04\n",
      "Epoch 60/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0272\n",
      "Epoch 00060: val_loss did not improve from 1.01202\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0272 - val_loss: 1.0121 - lr: 8.5737e-04\n",
      "Epoch 61/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0257\n",
      "Epoch 00061: val_loss did not improve from 1.01202\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0257 - val_loss: 1.0174 - lr: 8.1451e-04\n",
      "Epoch 62/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0257\n",
      "Epoch 00062: val_loss did not improve from 1.01202\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0256 - val_loss: 1.0132 - lr: 8.1451e-04\n",
      "Epoch 63/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0249\n",
      "Epoch 00063: val_loss improved from 1.01202 to 1.01129, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 1.0249 - val_loss: 1.0113 - lr: 8.1451e-04\n",
      "Epoch 64/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0248\n",
      "Epoch 00064: val_loss did not improve from 1.01129\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0248 - val_loss: 1.0114 - lr: 8.1451e-04\n",
      "Epoch 65/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0244\n",
      "Epoch 00065: val_loss improved from 1.01129 to 1.00544, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0243 - val_loss: 1.0054 - lr: 8.1451e-04\n",
      "Epoch 66/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0242\n",
      "Epoch 00066: val_loss did not improve from 1.00544\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0242 - val_loss: 1.0100 - lr: 8.1451e-04\n",
      "Epoch 67/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0233\n",
      "Epoch 00067: val_loss did not improve from 1.00544\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0234 - val_loss: 1.0095 - lr: 8.1451e-04\n",
      "Epoch 68/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0232\n",
      "Epoch 00068: val_loss did not improve from 1.00544\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0233 - val_loss: 1.0111 - lr: 8.1451e-04\n",
      "Epoch 69/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0226\n",
      "Epoch 00069: val_loss did not improve from 1.00544\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0226 - val_loss: 1.0087 - lr: 8.1451e-04\n",
      "Epoch 70/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0223\n",
      "Epoch 00070: val_loss did not improve from 1.00544\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.0007737808919046074.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0223 - val_loss: 1.0083 - lr: 8.1451e-04\n",
      "Epoch 71/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0213\n",
      "Epoch 00071: val_loss improved from 1.00544 to 1.00280, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0213 - val_loss: 1.0028 - lr: 7.7378e-04\n",
      "Epoch 72/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0209\n",
      "Epoch 00072: val_loss improved from 1.00280 to 0.99627, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 1.0208 - val_loss: 0.9963 - lr: 7.7378e-04\n",
      "Epoch 73/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0204\n",
      "Epoch 00073: val_loss did not improve from 0.99627\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0204 - val_loss: 1.0032 - lr: 7.7378e-04\n",
      "Epoch 74/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0202\n",
      "Epoch 00074: val_loss did not improve from 0.99627\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0202 - val_loss: 1.0063 - lr: 7.7378e-04\n",
      "Epoch 75/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0199\n",
      "Epoch 00075: val_loss did not improve from 0.99627\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.000735091819660738.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0199 - val_loss: 1.0016 - lr: 7.7378e-04\n",
      "Epoch 76/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0197\n",
      "Epoch 00076: val_loss did not improve from 0.99627\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0197 - val_loss: 1.0026 - lr: 7.3509e-04\n",
      "Epoch 77/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0193\n",
      "Epoch 00077: val_loss improved from 0.99627 to 0.99057, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0192 - val_loss: 0.9906 - lr: 7.3509e-04\n",
      "Epoch 78/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0186\n",
      "Epoch 00078: val_loss improved from 0.99057 to 0.98996, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0186 - val_loss: 0.9900 - lr: 7.3509e-04\n",
      "Epoch 79/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0182\n",
      "Epoch 00079: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0182 - val_loss: 0.9929 - lr: 7.3509e-04\n",
      "Epoch 80/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0180\n",
      "Epoch 00080: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0179 - val_loss: 1.0025 - lr: 7.3509e-04\n",
      "Epoch 81/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0172\n",
      "Epoch 00081: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0172 - val_loss: 0.9916 - lr: 7.3509e-04\n",
      "Epoch 82/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0174\n",
      "Epoch 00082: val_loss did not improve from 0.98996\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0006983372120885178.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0174 - val_loss: 1.0029 - lr: 7.3509e-04\n",
      "Epoch 83/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0167\n",
      "Epoch 00083: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0167 - val_loss: 1.0025 - lr: 6.9834e-04\n",
      "Epoch 84/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0162\n",
      "Epoch 00084: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0162 - val_loss: 0.9940 - lr: 6.9834e-04\n",
      "Epoch 85/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0161\n",
      "Epoch 00085: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0161 - val_loss: 0.9924 - lr: 6.9834e-04\n",
      "Epoch 86/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0160\n",
      "Epoch 00086: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0160 - val_loss: 1.0052 - lr: 6.9834e-04\n",
      "Epoch 87/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0154\n",
      "Epoch 00087: val_loss did not improve from 0.98996\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 0.0006634203542489559.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0154 - val_loss: 0.9906 - lr: 6.9834e-04\n",
      "Epoch 88/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0149\n",
      "Epoch 00088: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0149 - val_loss: 0.9976 - lr: 6.6342e-04\n",
      "Epoch 89/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0148\n",
      "Epoch 00089: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0148 - val_loss: 0.9953 - lr: 6.6342e-04\n",
      "Epoch 90/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0145\n",
      "Epoch 00090: val_loss did not improve from 0.98996\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0145 - val_loss: 0.9970 - lr: 6.6342e-04\n",
      "Epoch 91/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0142\n",
      "Epoch 00091: val_loss improved from 0.98996 to 0.98488, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0142 - val_loss: 0.9849 - lr: 6.6342e-04\n",
      "Epoch 92/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0142\n",
      "Epoch 00092: val_loss did not improve from 0.98488\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0006302493420662358.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0142 - val_loss: 0.9927 - lr: 6.6342e-04\n",
      "Epoch 93/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0137\n",
      "Epoch 00093: val_loss did not improve from 0.98488\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0137 - val_loss: 0.9921 - lr: 6.3025e-04\n",
      "Epoch 94/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0131\n",
      "Epoch 00094: val_loss did not improve from 0.98488\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0131 - val_loss: 0.9893 - lr: 6.3025e-04\n",
      "Epoch 95/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0128\n",
      "Epoch 00095: val_loss improved from 0.98488 to 0.98420, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0128 - val_loss: 0.9842 - lr: 6.3025e-04\n",
      "Epoch 96/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0127\n",
      "Epoch 00096: val_loss improved from 0.98420 to 0.97966, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0127 - val_loss: 0.9797 - lr: 6.3025e-04\n",
      "Epoch 97/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0126\n",
      "Epoch 00097: val_loss did not improve from 0.97966\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0126 - val_loss: 0.9860 - lr: 6.3025e-04\n",
      "Epoch 98/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0125\n",
      "Epoch 00098: val_loss did not improve from 0.97966\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0125 - val_loss: 0.9925 - lr: 6.3025e-04\n",
      "Epoch 99/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0123\n",
      "Epoch 00099: val_loss did not improve from 0.97966\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0123 - val_loss: 0.9880 - lr: 6.3025e-04\n",
      "Epoch 100/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0123\n",
      "Epoch 00100: val_loss did not improve from 0.97966\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0122 - val_loss: 0.9880 - lr: 6.3025e-04\n",
      "Epoch 101/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0117\n",
      "Epoch 00101: val_loss did not improve from 0.97966\n",
      "\n",
      "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.0005987368611386045.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0118 - val_loss: 0.9833 - lr: 6.3025e-04\n",
      "Epoch 102/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0110\n",
      "Epoch 00102: val_loss did not improve from 0.97966\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0110 - val_loss: 0.9907 - lr: 5.9874e-04\n",
      "Epoch 103/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0109\n",
      "Epoch 00103: val_loss improved from 0.97966 to 0.97729, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0108 - val_loss: 0.9773 - lr: 5.9874e-04\n",
      "Epoch 104/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0109\n",
      "Epoch 00104: val_loss did not improve from 0.97729\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0109 - val_loss: 0.9786 - lr: 5.9874e-04\n",
      "Epoch 105/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0107\n",
      "Epoch 00105: val_loss did not improve from 0.97729\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0107 - val_loss: 0.9861 - lr: 5.9874e-04\n",
      "Epoch 106/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0103\n",
      "Epoch 00106: val_loss did not improve from 0.97729\n",
      "\n",
      "Epoch 00106: ReduceLROnPlateau reducing learning rate to 0.0005688000208465382.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0103 - val_loss: 0.9920 - lr: 5.9874e-04\n",
      "Epoch 107/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0101\n",
      "Epoch 00107: val_loss did not improve from 0.97729\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0100 - val_loss: 0.9958 - lr: 5.6880e-04\n",
      "Epoch 108/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0097\n",
      "Epoch 00108: val_loss did not improve from 0.97729\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0097 - val_loss: 0.9882 - lr: 5.6880e-04\n",
      "Epoch 109/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0093\n",
      "Epoch 00109: val_loss improved from 0.97729 to 0.97559, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 1.0093 - val_loss: 0.9756 - lr: 5.6880e-04\n",
      "Epoch 110/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0094\n",
      "Epoch 00110: val_loss did not improve from 0.97559\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0094 - val_loss: 0.9815 - lr: 5.6880e-04\n",
      "Epoch 111/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0092\n",
      "Epoch 00111: val_loss did not improve from 0.97559\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 0.0005403600225690752.\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0092 - val_loss: 0.9789 - lr: 5.6880e-04\n",
      "Epoch 112/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0085\n",
      "Epoch 00112: val_loss improved from 0.97559 to 0.97286, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 59ms/step - loss: 1.0085 - val_loss: 0.9729 - lr: 5.4036e-04\n",
      "Epoch 113/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00113: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0083 - val_loss: 0.9877 - lr: 5.4036e-04\n",
      "Epoch 114/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0083\n",
      "Epoch 00114: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0082 - val_loss: 0.9749 - lr: 5.4036e-04\n",
      "Epoch 115/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0081\n",
      "Epoch 00115: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0081 - val_loss: 0.9752 - lr: 5.4036e-04\n",
      "Epoch 116/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0082\n",
      "Epoch 00116: val_loss did not improve from 0.97286\n",
      "\n",
      "Epoch 00116: ReduceLROnPlateau reducing learning rate to 0.0005133419937919825.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0083 - val_loss: 0.9796 - lr: 5.4036e-04\n",
      "Epoch 117/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0077\n",
      "Epoch 00117: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0077 - val_loss: 0.9756 - lr: 5.1334e-04\n",
      "Epoch 118/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0076\n",
      "Epoch 00118: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0076 - val_loss: 0.9781 - lr: 5.1334e-04\n",
      "Epoch 119/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0073\n",
      "Epoch 00119: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0073 - val_loss: 0.9759 - lr: 5.1334e-04\n",
      "Epoch 120/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0071\n",
      "Epoch 00120: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0071 - val_loss: 0.9790 - lr: 5.1334e-04\n",
      "Epoch 121/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0069\n",
      "Epoch 00121: val_loss did not improve from 0.97286\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.0004876748775132.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0069 - val_loss: 0.9782 - lr: 5.1334e-04\n",
      "Epoch 122/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0066\n",
      "Epoch 00122: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0066 - val_loss: 0.9773 - lr: 4.8767e-04\n",
      "Epoch 123/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0063\n",
      "Epoch 00123: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0063 - val_loss: 0.9763 - lr: 4.8767e-04\n",
      "Epoch 124/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0062\n",
      "Epoch 00124: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0062 - val_loss: 0.9775 - lr: 4.8767e-04\n",
      "Epoch 125/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0061\n",
      "Epoch 00125: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0061 - val_loss: 0.9782 - lr: 4.8767e-04\n",
      "Epoch 126/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0063\n",
      "Epoch 00126: val_loss did not improve from 0.97286\n",
      "\n",
      "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.00046329112810781223.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0063 - val_loss: 0.9814 - lr: 4.8767e-04\n",
      "Epoch 127/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0058\n",
      "Epoch 00127: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0058 - val_loss: 0.9824 - lr: 4.6329e-04\n",
      "Epoch 128/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0060\n",
      "Epoch 00128: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0060 - val_loss: 0.9775 - lr: 4.6329e-04\n",
      "Epoch 129/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0053\n",
      "Epoch 00129: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 1.0053 - val_loss: 0.9759 - lr: 4.6329e-04\n",
      "Epoch 130/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0054\n",
      "Epoch 00130: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0055 - val_loss: 0.9735 - lr: 4.6329e-04\n",
      "Epoch 131/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0052\n",
      "Epoch 00131: val_loss did not improve from 0.97286\n",
      "\n",
      "Epoch 00131: ReduceLROnPlateau reducing learning rate to 0.00044012657308485355.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0051 - val_loss: 0.9749 - lr: 4.6329e-04\n",
      "Epoch 132/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0050\n",
      "Epoch 00132: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0049 - val_loss: 0.9740 - lr: 4.4013e-04\n",
      "Epoch 133/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0049\n",
      "Epoch 00133: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0049 - val_loss: 0.9772 - lr: 4.4013e-04\n",
      "Epoch 134/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0048\n",
      "Epoch 00134: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0048 - val_loss: 0.9747 - lr: 4.4013e-04\n",
      "Epoch 135/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0043\n",
      "Epoch 00135: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0043 - val_loss: 0.9767 - lr: 4.4013e-04\n",
      "Epoch 136/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0043\n",
      "Epoch 00136: val_loss did not improve from 0.97286\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.00041812024719547477.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0044 - val_loss: 0.9819 - lr: 4.4013e-04\n",
      "Epoch 137/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0038\n",
      "Epoch 00137: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0038 - val_loss: 0.9796 - lr: 4.1812e-04\n",
      "Epoch 138/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0037\n",
      "Epoch 00138: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0036 - val_loss: 0.9787 - lr: 4.1812e-04\n",
      "Epoch 139/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0033\n",
      "Epoch 00139: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0034 - val_loss: 0.9747 - lr: 4.1812e-04\n",
      "Epoch 140/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0037\n",
      "Epoch 00140: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0037 - val_loss: 0.9769 - lr: 4.1812e-04\n",
      "Epoch 141/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0037\n",
      "Epoch 00141: val_loss did not improve from 0.97286\n",
      "\n",
      "Epoch 00141: ReduceLROnPlateau reducing learning rate to 0.00039721422654110934.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0037 - val_loss: 0.9732 - lr: 4.1812e-04\n",
      "Epoch 142/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0030\n",
      "Epoch 00142: val_loss did not improve from 0.97286\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0030 - val_loss: 0.9759 - lr: 3.9721e-04\n",
      "Epoch 143/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0028\n",
      "Epoch 00143: val_loss improved from 0.97286 to 0.97196, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 1.0029 - val_loss: 0.9720 - lr: 3.9721e-04\n",
      "Epoch 144/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0030\n",
      "Epoch 00144: val_loss improved from 0.97196 to 0.96978, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 58ms/step - loss: 1.0030 - val_loss: 0.9698 - lr: 3.9721e-04\n",
      "Epoch 145/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0029\n",
      "Epoch 00145: val_loss did not improve from 0.96978\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0029 - val_loss: 0.9752 - lr: 3.9721e-04\n",
      "Epoch 146/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0029\n",
      "Epoch 00146: val_loss did not improve from 0.96978\n",
      "\n",
      "Epoch 00146: ReduceLROnPlateau reducing learning rate to 0.00037735351797891776.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0028 - val_loss: 0.9767 - lr: 3.9721e-04\n",
      "Epoch 147/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0027\n",
      "Epoch 00147: val_loss improved from 0.96978 to 0.96815, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 63ms/step - loss: 1.0027 - val_loss: 0.9681 - lr: 3.7735e-04\n",
      "Epoch 148/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0021\n",
      "Epoch 00148: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0021 - val_loss: 0.9694 - lr: 3.7735e-04\n",
      "Epoch 149/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0023\n",
      "Epoch 00149: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0023 - val_loss: 0.9715 - lr: 3.7735e-04\n",
      "Epoch 150/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0024\n",
      "Epoch 00150: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0024 - val_loss: 0.9783 - lr: 3.7735e-04\n",
      "Epoch 151/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0020\n",
      "Epoch 00151: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0020 - val_loss: 0.9682 - lr: 3.7735e-04\n",
      "Epoch 152/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0022\n",
      "Epoch 00152: val_loss did not improve from 0.96815\n",
      "\n",
      "Epoch 00152: ReduceLROnPlateau reducing learning rate to 0.00035848583793267607.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0023 - val_loss: 0.9777 - lr: 3.7735e-04\n",
      "Epoch 153/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00153: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0019 - val_loss: 0.9699 - lr: 3.5849e-04\n",
      "Epoch 154/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0018\n",
      "Epoch 00154: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0017 - val_loss: 0.9688 - lr: 3.5849e-04\n",
      "Epoch 155/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0019\n",
      "Epoch 00155: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0019 - val_loss: 0.9713 - lr: 3.5849e-04\n",
      "Epoch 156/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0014\n",
      "Epoch 00156: val_loss did not improve from 0.96815\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0014 - val_loss: 0.9740 - lr: 3.5849e-04\n",
      "Epoch 157/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0015\n",
      "Epoch 00157: val_loss improved from 0.96815 to 0.96699, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 0.00034056155709549785.\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0015 - val_loss: 0.9670 - lr: 3.5849e-04\n",
      "Epoch 158/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0012\n",
      "Epoch 00158: val_loss did not improve from 0.96699\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0012 - val_loss: 0.9747 - lr: 3.4056e-04\n",
      "Epoch 159/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0010\n",
      "Epoch 00159: val_loss did not improve from 0.96699\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0010 - val_loss: 0.9675 - lr: 3.4056e-04\n",
      "Epoch 160/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0010\n",
      "Epoch 00160: val_loss improved from 0.96699 to 0.96528, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 1.0010 - val_loss: 0.9653 - lr: 3.4056e-04\n",
      "Epoch 161/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0006\n",
      "Epoch 00161: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0007 - val_loss: 0.9659 - lr: 3.4056e-04\n",
      "Epoch 162/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0008\n",
      "Epoch 00162: val_loss did not improve from 0.96528\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.00032353347924072293.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0008 - val_loss: 0.9667 - lr: 3.4056e-04\n",
      "Epoch 163/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 1.0004\n",
      "Epoch 00163: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0004 - val_loss: 0.9759 - lr: 3.2353e-04\n",
      "Epoch 164/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0002\n",
      "Epoch 00164: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 1.0002 - val_loss: 0.9655 - lr: 3.2353e-04\n",
      "Epoch 165/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0004\n",
      "Epoch 00165: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0004 - val_loss: 0.9730 - lr: 3.2353e-04\n",
      "Epoch 166/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0003\n",
      "Epoch 00166: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0003 - val_loss: 0.9734 - lr: 3.2353e-04\n",
      "Epoch 167/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 1.0002\n",
      "Epoch 00167: val_loss did not improve from 0.96528\n",
      "\n",
      "Epoch 00167: ReduceLROnPlateau reducing learning rate to 0.00030735681357327847.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 1.0002 - val_loss: 0.9654 - lr: 3.2353e-04\n",
      "Epoch 168/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9998\n",
      "Epoch 00168: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9998 - val_loss: 0.9728 - lr: 3.0736e-04\n",
      "Epoch 169/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9999\n",
      "Epoch 00169: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9999 - val_loss: 0.9767 - lr: 3.0736e-04\n",
      "Epoch 170/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9996\n",
      "Epoch 00170: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9997 - val_loss: 0.9685 - lr: 3.0736e-04\n",
      "Epoch 171/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9996\n",
      "Epoch 00171: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9996 - val_loss: 0.9657 - lr: 3.0736e-04\n",
      "Epoch 172/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9995\n",
      "Epoch 00172: val_loss did not improve from 0.96528\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.00029198898118920624.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9995 - val_loss: 0.9692 - lr: 3.0736e-04\n",
      "Epoch 173/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9994\n",
      "Epoch 00173: val_loss did not improve from 0.96528\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9993 - val_loss: 0.9699 - lr: 2.9199e-04\n",
      "Epoch 174/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9991\n",
      "Epoch 00174: val_loss improved from 0.96528 to 0.96234, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 59ms/step - loss: 0.9992 - val_loss: 0.9623 - lr: 2.9199e-04\n",
      "Epoch 175/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9995\n",
      "Epoch 00175: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9994 - val_loss: 0.9674 - lr: 2.9199e-04\n",
      "Epoch 176/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9991\n",
      "Epoch 00176: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9991 - val_loss: 0.9766 - lr: 2.9199e-04\n",
      "Epoch 177/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9987\n",
      "Epoch 00177: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00177: ReduceLROnPlateau reducing learning rate to 0.00027738953212974593.\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.9986 - val_loss: 0.9669 - lr: 2.9199e-04\n",
      "Epoch 178/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9987\n",
      "Epoch 00178: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9987 - val_loss: 0.9668 - lr: 2.7739e-04\n",
      "Epoch 179/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9986\n",
      "Epoch 00179: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9986 - val_loss: 0.9654 - lr: 2.7739e-04\n",
      "Epoch 180/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9985\n",
      "Epoch 00180: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.9985 - val_loss: 0.9757 - lr: 2.7739e-04\n",
      "Epoch 181/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9987\n",
      "Epoch 00181: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9987 - val_loss: 0.9699 - lr: 2.7739e-04\n",
      "Epoch 182/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9983\n",
      "Epoch 00182: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 0.0002635200624354184.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9983 - val_loss: 0.9654 - lr: 2.7739e-04\n",
      "Epoch 183/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9982\n",
      "Epoch 00183: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9982 - val_loss: 0.9661 - lr: 2.6352e-04\n",
      "Epoch 184/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9980\n",
      "Epoch 00184: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9980 - val_loss: 0.9670 - lr: 2.6352e-04\n",
      "Epoch 185/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9979\n",
      "Epoch 00185: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9979 - val_loss: 0.9701 - lr: 2.6352e-04\n",
      "Epoch 186/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9980\n",
      "Epoch 00186: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 9s 59ms/step - loss: 0.9980 - val_loss: 0.9713 - lr: 2.6352e-04\n",
      "Epoch 187/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9979\n",
      "Epoch 00187: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 0.0002503440482541919.\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.9979 - val_loss: 0.9662 - lr: 2.6352e-04\n",
      "Epoch 188/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9976\n",
      "Epoch 00188: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9976 - val_loss: 0.9692 - lr: 2.5034e-04\n",
      "Epoch 189/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9975\n",
      "Epoch 00189: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9975 - val_loss: 0.9646 - lr: 2.5034e-04\n",
      "Epoch 190/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9977\n",
      "Epoch 00190: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.9977 - val_loss: 0.9689 - lr: 2.5034e-04\n",
      "Epoch 191/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9976\n",
      "Epoch 00191: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 56ms/step - loss: 0.9975 - val_loss: 0.9678 - lr: 2.5034e-04\n",
      "Epoch 192/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9973\n",
      "Epoch 00192: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 0.00023782684584148226.\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.9973 - val_loss: 0.9681 - lr: 2.5034e-04\n",
      "Epoch 193/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9972\n",
      "Epoch 00193: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9972 - val_loss: 0.9624 - lr: 2.3783e-04\n",
      "Epoch 194/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9969\n",
      "Epoch 00194: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9969 - val_loss: 0.9642 - lr: 2.3783e-04\n",
      "Epoch 195/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9970\n",
      "Epoch 00195: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9970 - val_loss: 0.9692 - lr: 2.3783e-04\n",
      "Epoch 196/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9969\n",
      "Epoch 00196: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9969 - val_loss: 0.9629 - lr: 2.3783e-04\n",
      "Epoch 197/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9968\n",
      "Epoch 00197: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00197: ReduceLROnPlateau reducing learning rate to 0.00022593549801968037.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9968 - val_loss: 0.9653 - lr: 2.3783e-04\n",
      "Epoch 198/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9967\n",
      "Epoch 00198: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9967 - val_loss: 0.9680 - lr: 2.2594e-04\n",
      "Epoch 199/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9965\n",
      "Epoch 00199: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9965 - val_loss: 0.9707 - lr: 2.2594e-04\n",
      "Epoch 200/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9964\n",
      "Epoch 00200: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.9964 - val_loss: 0.9663 - lr: 2.2594e-04\n",
      "Epoch 201/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9963\n",
      "Epoch 00201: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9963 - val_loss: 0.9645 - lr: 2.2594e-04\n",
      "Epoch 202/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9964\n",
      "Epoch 00202: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.00021463872035383245.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9964 - val_loss: 0.9641 - lr: 2.2594e-04\n",
      "Epoch 203/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9962\n",
      "Epoch 00203: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 53ms/step - loss: 0.9962 - val_loss: 0.9659 - lr: 2.1464e-04\n",
      "Epoch 204/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9960\n",
      "Epoch 00204: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9960 - val_loss: 0.9679 - lr: 2.1464e-04\n",
      "Epoch 205/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9960\n",
      "Epoch 00205: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9960 - val_loss: 0.9687 - lr: 2.1464e-04\n",
      "Epoch 206/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9959\n",
      "Epoch 00206: val_loss did not improve from 0.96234\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9959 - val_loss: 0.9634 - lr: 2.1464e-04\n",
      "Epoch 207/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9958\n",
      "Epoch 00207: val_loss did not improve from 0.96234\n",
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 0.0002039067905570846.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9959 - val_loss: 0.9682 - lr: 2.1464e-04\n",
      "Epoch 208/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9956\n",
      "Epoch 00208: val_loss improved from 0.96234 to 0.96155, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 57ms/step - loss: 0.9956 - val_loss: 0.9615 - lr: 2.0391e-04\n",
      "Epoch 209/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9956\n",
      "Epoch 00209: val_loss did not improve from 0.96155\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9956 - val_loss: 0.9712 - lr: 2.0391e-04\n",
      "Epoch 210/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9958\n",
      "Epoch 00210: val_loss improved from 0.96155 to 0.95981, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 8s 58ms/step - loss: 0.9958 - val_loss: 0.9598 - lr: 2.0391e-04\n",
      "Epoch 211/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9954\n",
      "Epoch 00211: val_loss improved from 0.95981 to 0.95556, saving model to /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\n",
      "146/146 [==============================] - 9s 60ms/step - loss: 0.9955 - val_loss: 0.9556 - lr: 2.0391e-04\n",
      "Epoch 212/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9954\n",
      "Epoch 00212: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9954 - val_loss: 0.9689 - lr: 2.0391e-04\n",
      "Epoch 213/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9954\n",
      "Epoch 00213: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9954 - val_loss: 0.9659 - lr: 2.0391e-04\n",
      "Epoch 214/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9954\n",
      "Epoch 00214: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9953 - val_loss: 0.9629 - lr: 2.0391e-04\n",
      "Epoch 215/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9952\n",
      "Epoch 00215: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9952 - val_loss: 0.9659 - lr: 2.0391e-04\n",
      "Epoch 216/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9950\n",
      "Epoch 00216: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00216: ReduceLROnPlateau reducing learning rate to 0.00019371145172044634.\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9950 - val_loss: 0.9653 - lr: 2.0391e-04\n",
      "Epoch 217/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9949\n",
      "Epoch 00217: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9949 - val_loss: 0.9641 - lr: 1.9371e-04\n",
      "Epoch 218/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9945\n",
      "Epoch 00218: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9945 - val_loss: 0.9691 - lr: 1.9371e-04\n",
      "Epoch 219/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9948\n",
      "Epoch 00219: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9947 - val_loss: 0.9596 - lr: 1.9371e-04\n",
      "Epoch 220/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9948\n",
      "Epoch 00220: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9949 - val_loss: 0.9646 - lr: 1.9371e-04\n",
      "Epoch 221/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9948\n",
      "Epoch 00221: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00221: ReduceLROnPlateau reducing learning rate to 0.00018402588466415182.\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9948 - val_loss: 0.9609 - lr: 1.9371e-04\n",
      "Epoch 222/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9944\n",
      "Epoch 00222: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9944 - val_loss: 0.9611 - lr: 1.8403e-04\n",
      "Epoch 223/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9944\n",
      "Epoch 00223: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9943 - val_loss: 0.9637 - lr: 1.8403e-04\n",
      "Epoch 224/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9945\n",
      "Epoch 00224: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9945 - val_loss: 0.9593 - lr: 1.8403e-04\n",
      "Epoch 225/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9942\n",
      "Epoch 00225: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9942 - val_loss: 0.9615 - lr: 1.8403e-04\n",
      "Epoch 226/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9941\n",
      "Epoch 00226: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00226: ReduceLROnPlateau reducing learning rate to 0.00017482458351878447.\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9941 - val_loss: 0.9657 - lr: 1.8403e-04\n",
      "Epoch 227/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9940\n",
      "Epoch 00227: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9940 - val_loss: 0.9632 - lr: 1.7482e-04\n",
      "Epoch 228/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9940\n",
      "Epoch 00228: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9940 - val_loss: 0.9613 - lr: 1.7482e-04\n",
      "Epoch 229/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9941\n",
      "Epoch 00229: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9941 - val_loss: 0.9585 - lr: 1.7482e-04\n",
      "Epoch 230/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9938\n",
      "Epoch 00230: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9938 - val_loss: 0.9669 - lr: 1.7482e-04\n",
      "Epoch 231/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9939\n",
      "Epoch 00231: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00231: ReduceLROnPlateau reducing learning rate to 0.0001660833557252772.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9939 - val_loss: 0.9571 - lr: 1.7482e-04\n",
      "Epoch 232/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9936\n",
      "Epoch 00232: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9936 - val_loss: 0.9627 - lr: 1.6608e-04\n",
      "Epoch 233/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9937\n",
      "Epoch 00233: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9937 - val_loss: 0.9614 - lr: 1.6608e-04\n",
      "Epoch 234/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9938\n",
      "Epoch 00234: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9938 - val_loss: 0.9652 - lr: 1.6608e-04\n",
      "Epoch 235/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9935\n",
      "Epoch 00235: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9935 - val_loss: 0.9613 - lr: 1.6608e-04\n",
      "Epoch 236/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9933\n",
      "Epoch 00236: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00236: ReduceLROnPlateau reducing learning rate to 0.0001577791837917175.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9933 - val_loss: 0.9588 - lr: 1.6608e-04\n",
      "Epoch 237/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9933\n",
      "Epoch 00237: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9933 - val_loss: 0.9593 - lr: 1.5778e-04\n",
      "Epoch 238/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9934\n",
      "Epoch 00238: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9934 - val_loss: 0.9567 - lr: 1.5778e-04\n",
      "Epoch 239/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9932\n",
      "Epoch 00239: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9932 - val_loss: 0.9609 - lr: 1.5778e-04\n",
      "Epoch 240/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9929\n",
      "Epoch 00240: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9929 - val_loss: 0.9570 - lr: 1.5778e-04\n",
      "Epoch 241/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9929\n",
      "Epoch 00241: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.0001498902252933476.\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9930 - val_loss: 0.9597 - lr: 1.5778e-04\n",
      "Epoch 242/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9930\n",
      "Epoch 00242: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9930 - val_loss: 0.9628 - lr: 1.4989e-04\n",
      "Epoch 243/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9929\n",
      "Epoch 00243: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9928 - val_loss: 0.9581 - lr: 1.4989e-04\n",
      "Epoch 244/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9928\n",
      "Epoch 00244: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9928 - val_loss: 0.9613 - lr: 1.4989e-04\n",
      "Epoch 245/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9927\n",
      "Epoch 00245: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9926 - val_loss: 0.9584 - lr: 1.4989e-04\n",
      "Epoch 246/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9928\n",
      "Epoch 00246: val_loss did not improve from 0.95556\n",
      "\n",
      "Epoch 00246: ReduceLROnPlateau reducing learning rate to 0.00014239571610232815.\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9929 - val_loss: 0.9632 - lr: 1.4989e-04\n",
      "Epoch 247/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9924\n",
      "Epoch 00247: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9924 - val_loss: 0.9595 - lr: 1.4240e-04\n",
      "Epoch 248/250\n",
      "145/146 [============================>.] - ETA: 0s - loss: 0.9926\n",
      "Epoch 00248: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 55ms/step - loss: 0.9926 - val_loss: 0.9614 - lr: 1.4240e-04\n",
      "Epoch 249/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9925\n",
      "Epoch 00249: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9925 - val_loss: 0.9589 - lr: 1.4240e-04\n",
      "Epoch 250/250\n",
      "146/146 [==============================] - ETA: 0s - loss: 0.9923\n",
      "Epoch 00250: val_loss did not improve from 0.95556\n",
      "146/146 [==============================] - 8s 54ms/step - loss: 0.9923 - val_loss: 0.9602 - lr: 1.4240e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as leaky_re_lu_20_layer_call_fn, leaky_re_lu_20_layer_call_and_return_conditional_losses, leaky_re_lu_21_layer_call_fn, leaky_re_lu_21_layer_call_and_return_conditional_losses, leaky_re_lu_22_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_model/assets\n"
     ]
    }
   ],
   "source": [
    "epoch = 250\n",
    "optimizer = tensorflow.keras.optimizers.Adam( learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "\n",
    "\n",
    "# Model Compilation\n",
    "#LSTM_MLP_reg.compile(optimizer=optimizer, loss='mse', metrics=[RMSE_error])\n",
    "LSTM_MLP_reg.compile(optimizer=optimizer, loss= RMSE_error)\n",
    "\n",
    "\n",
    "#Saving Best Model and Representation of results\n",
    "filepath = \"/content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath= filepath, save_weights_only=True,\n",
    "                              monitor='val_loss', verbose=1,\n",
    "                              save_best_only=True, mode='min') \n",
    "\n",
    "decay_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, patience=5, \n",
    "                                                verbose=1, mode='min', min_delta=0.01, \n",
    "                                                cooldown=0, min_lr=0.000001)\n",
    "\n",
    "callback_list = [checkpoint, decay_lr]\n",
    "\n",
    "# Fitting Model and Evaluation\n",
    "history = LSTM_MLP_reg.fit(X_train_lr, y_train_lr, epochs=epoch, \n",
    "          batch_size=100000, validation_data=(X_cv_lr,y_cv_lr),\n",
    "          verbose='auto', callbacks = callback_list)\n",
    "\n",
    "#Saving the model for future use:\n",
    "LSTM_MLP_reg.save(\"/content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_model\", save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1639739462931,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "M1lwrbaXrfoZ",
    "outputId": "14bf4aa1-569d-4eb9-dc93-2beb3ad7e4c9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywIachZ3rfoa"
   },
   "outputs": [],
   "source": [
    "#Loading saved model from disk\n",
    "LSTM_MLP_reg = models.load_model(\"/content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_model\", custom_objects={'RMSE_error':RMSE_error, 'RMSLE_error':RMSLE_error})\n",
    "\n",
    "#Loading weights from best model\n",
    "LSTM_MLP_reg.load_weights(\"/content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\")\n",
    "\n",
    "y_train_pred = LSTM_MLP_reg.predict(X_train_lr[:])\n",
    "y_cv_pred = LSTM_MLP_reg.predict(X_cv_lr[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1659,
     "status": "ok",
     "timestamp": 1639747576571,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "tfEEi5azrfob",
    "outputId": "c086f183-0291-4cac-fd67-19b1ecfea4fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.For LSTM_MLP Regression Model the RMSLE values are \n",
      "    train RMSLE = 0.22 \n",
      "    CV RMSLE = 0.23\n",
      "--------------------------------------------------\n",
      "2.For LSTM_MLP Regression Model the RMSE values are \n",
      "    train RMSE = 0.93 \n",
      "    CV RMSE = 0.95\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "LSTM_MLP_cv_RMSLE = compute_RMSLE(np.array(y_cv_lr), np.array(y_cv_pred))\n",
    "LSTM_MLP_train_RMSLE = compute_RMSLE(np.array(y_train_lr), np.array(y_train_pred))\n",
    "\n",
    "LSTM_MLP_cv_RMSE = compute_RMSE(np.array(y_cv_lr), np.array(y_cv_pred))\n",
    "LSTM_MLP_train_RMSE = compute_RMSE(np.array(y_train_lr), np.array(y_train_pred))\n",
    "\n",
    "#LSTM_MLP_cv_RMSE_CV = compute_RMSE_CV(y_cv_lr[:], y_cv_pred)\n",
    "#LSTM_MLP_train_RMSE_CV = compute_RMSE_CV(y_train_lr[:], y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"1.For LSTM_MLP Regression Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(LSTM_MLP_train_RMSLE, LSTM_MLP_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For LSTM_MLP Regression Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(LSTM_MLP_train_RMSE, LSTM_MLP_cv_RMSE))\n",
    "print(\"--\"*25)\n",
    "#print(\"2.For LSTM_MLP Regression Model the RMSE(CV) values are \\n    train RMSE(CV) = {0} \\n    CV RMSE(CV) = {1}\".format(LSTM_MLP_train_RMSE_CV, LSTM_MLP_cv_RMSE_CV))\n",
    "#print(\"--\"*25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5IPPMDoL5Wi"
   },
   "source": [
    "### **4.7 Stacking: Mean-Model**\n",
    "Let's combining predictions of other regression models and compute mean as a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Kam3z7vmKSR"
   },
   "outputs": [],
   "source": [
    "# LightGBM GBDT Model 1\n",
    "LGBM_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_Rev02.sav')\n",
    "y_train_pred_LGBM = LGBM_reg_best.predict(X_train)\n",
    "y_cv_pred_LGBM = LGBM_reg_best.predict(X_cv)\n",
    "\n",
    "# LightGBM GBDT Model 2\n",
    "LGBM_reg_best_2 = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev02.sav')\n",
    "y_train_pred_LGBM_2 = LGBM_reg_best_2.predict(X_train)\n",
    "y_cv_pred_LGBM_2 = LGBM_reg_best_2.predict(X_cv)\n",
    "\n",
    "# CatBoost Model 01\n",
    "CatBoost_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/catboost_reg_Rev02.sav')\n",
    "y_train_pred_CatBoost = CatBoost_reg_best.predict(X_train)\n",
    "y_cv_pred_CatBoost = CatBoost_reg_best.predict(X_cv)\n",
    "\n",
    "#CatBoost Model 02\n",
    "CatBoost_reg_best_2 = joblib.load('/content/drive/MyDrive/Case Study 01/Data/catboost_Model02_Rev02.sav')\n",
    "y_train_pred_CatBoost_2 = CatBoost_reg_best_2.predict(X_train)\n",
    "y_cv_pred_CatBoost_2 = CatBoost_reg_best_2.predict(X_cv)\n",
    "\n",
    "# Decision Tree\n",
    "DT_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/dt_reg_best_Rev02.sav')\n",
    "y_train_pred_DT = DT_reg_best.predict(X_train)\n",
    "y_cv_pred_DT = DT_reg_best.predict(X_cv)\n",
    "\n",
    "# Random Forest\n",
    "RF_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/RF_reg_depth15_Rev02.sav')\n",
    "y_train_pred_RF = RF_reg_best.predict(X_train)\n",
    "y_cv_pred_RF = RF_reg_best.predict(X_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtC1Gqkb5V4O"
   },
   "outputs": [],
   "source": [
    "#Taking avg of the prediction of above models\n",
    "y_train_pred_df = pd.DataFrame(list(zip( y_train_pred_LGBM, y_train_pred_LGBM_2, y_train_pred_CatBoost,\n",
    "                                        y_train_pred_CatBoost_2, y_train_pred_DT, y_train_pred_RF )), \n",
    "             columns=[ 'y_tr_LGBM', 'y_tr_LGBM_2', 'y_tr_CatBoost', 'y_tr_CatBoost_2', 'y_train_DT', 'y_train_RF'])\n",
    "\n",
    "y_cv_pred_df = pd.DataFrame(list(zip(y_cv_pred_LGBM, y_cv_pred_LGBM_2, y_cv_pred_CatBoost,\n",
    "                                     y_cv_pred_CatBoost_2, y_cv_pred_DT, y_cv_pred_RF)), \n",
    "             columns=['y_cv_LGBM', 'y_cv_LGBM_2', 'y_cv_CatBoost', 'y_cv_CatBoost_2', 'y_train-DT', 'y_train-RF'])\n",
    "\n",
    "y_train_pred_df['y_train_stack'] = y_train_pred_df.mean(axis=1)\n",
    "y_train_pred = y_train_pred_df.y_train_stack\n",
    "\n",
    "y_cv_pred_df['y_cv_stack'] = y_cv_pred_df.mean(axis=1)\n",
    "y_cv_pred = y_cv_pred_df.y_cv_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vk9RutOi5yQ_"
   },
   "outputs": [],
   "source": [
    "#Saving Outputs\n",
    "y_train_pred_df.to_feather('/content/drive/MyDrive/Case Study 01/Data/stack_train_df.ftr')\n",
    "y_cv_pred_df.to_feather('/content/drive/MyDrive/Case Study 01/Data/stack_cv_df.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1941,
     "status": "ok",
     "timestamp": 1639753490923,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "2hiotXi9mKVc",
    "outputId": "187523aa-35e5-4a5a-ff12-bd934c3fc55d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "1.For Stacking Regression Model the RMSLE values are \n",
      "    train RMSLE = 0.13 \n",
      "    CV RMSLE = 0.17\n",
      "--------------------------------------------------\n",
      "2.For Stacking Model the RMSE values are \n",
      "    train RMSE = 0.48 \n",
      "    CV RMSE = 0.63\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "Stack_cv_RMSLE = compute_RMSLE(y_cv[:], y_cv_pred)\n",
    "Stack_train_RMSLE = compute_RMSLE(y_train, y_train_pred)\n",
    "\n",
    "Stack_cv_RMSE = compute_RMSE(y_cv, y_cv_pred)\n",
    "Stack_train_RMSE = compute_RMSE(y_train, y_train_pred)\n",
    "\n",
    "#Report\n",
    "print(\"--\"*25)\n",
    "print(\"1.For Stacking Regression Model the RMSLE values are \\n    train RMSLE = {0} \\n    CV RMSLE = {1}\".format(Stack_train_RMSLE, Stack_cv_RMSLE))\n",
    "print(\"--\"*25)\n",
    "print(\"2.For Stacking Model the RMSE values are \\n    train RMSE = {0} \\n    CV RMSE = {1}\".format(Stack_train_RMSE, Stack_cv_RMSE))\n",
    "print(\"--\"*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--lst41F2iv6"
   },
   "source": [
    "### **4.8 Custom Stacking Regressor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiPU8sGgiVZf"
   },
   "source": [
    "Attaching target variable to training data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1640167204170,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "f6pn8ceah0j2",
    "outputId": "590644c2-6aec-41b0-bbaf-f2557483d7ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ede8f4bd-1def-44e6-a25a-0a80b30d1db2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>floor_info</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_Week</th>\n",
       "      <th>day_name</th>\n",
       "      <th>hour</th>\n",
       "      <th>season</th>\n",
       "      <th>RH</th>\n",
       "      <th>Cloud_base</th>\n",
       "      <th>WCI</th>\n",
       "      <th>D/N</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>9045.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>2.810547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>830.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>15120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>2.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>831.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4065.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>3.087891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>832.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>26507.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>4.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6392.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>3.044922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ede8f4bd-1def-44e6-a25a-0a80b30d1db2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ede8f4bd-1def-44e6-a25a-0a80b30d1db2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ede8f4bd-1def-44e6-a25a-0a80b30d1db2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   building_id  meter  primary_use  ...       WCI  D/N         y\n",
       "0         46.0    0.0           11  ...  28.65625    1  2.810547\n",
       "1        830.0    0.0           15  ...  28.65625    1  2.773438\n",
       "2        831.0    0.0            1  ...  28.65625    1  3.087891\n",
       "3        832.0    0.0            9  ...  28.65625    1  4.011719\n",
       "4        833.0    0.0            9  ...  28.65625    1  3.044922\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X_train.copy()\n",
    "data['y'] = y_train\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPcIyHyg3n0r"
   },
   "outputs": [],
   "source": [
    "#Base Model Configuration\n",
    "\n",
    "DT_reg_1 = DecisionTreeRegressor(max_depth= 27, min_samples_split= 100)\n",
    "DT_reg_2 = DecisionTreeRegressor(max_depth= 15, min_samples_split= 150)\n",
    "\n",
    "RF_reg_1 = RandomForestRegressor(n_estimators=250, min_samples_split=50, max_features=0.3, \n",
    "                               max_depth=15, n_jobs=50, bootstrap=True, verbose=1)\n",
    "RF_reg_2 = RandomForestRegressor(n_estimators=250, min_samples_split=50, max_features=0.3, \n",
    "                               max_depth=15, n_jobs=50, bootstrap=True, verbose=1)\n",
    "\n",
    "LGBM_reg_1 = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='rmse', n_estimators=2100)\n",
    "LGBM_reg_2 = LGBMRegressor(boosting_type='gbdt', num_leaves=20, objective='rmse', n_estimators=2000)\n",
    "\n",
    "\n",
    "catboost_reg_1 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 2100,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "catboost_reg_2 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 1500,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "\n",
    "\n",
    "#Base Model List\n",
    "base_models = [DT_reg_1, DT_reg_2, RF_reg_1, RF_reg_2, LGBM_reg_1, LGBM_reg_2,  catboost_reg_1, catboost_reg_2]\n",
    "\n",
    "#Base Model Names\n",
    "#base_model_names = [str(type(i).__name__) + \"_pred\"  for i in base_models ]\n",
    "base_model_names = [\"BM_\" + str(i)  for i in range(len(base_models)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKkgx0VxAFeI"
   },
   "outputs": [],
   "source": [
    "#Attaching target variable to training data before splitting\n",
    "data = X_train.copy()\n",
    "data['y'] = y_train\n",
    "\n",
    "#Diving Train data into two dataframes D1 and D2.\n",
    "D1 = data[:int(len(data)*0.5)]\n",
    "D2 = data[int(len(data)*0.5):]\n",
    "\n",
    "\n",
    "#Setting up target variable for D1\n",
    "y_train_D2 = D2.y\n",
    "X_train_D2 = D2.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4g5El_WXDuI"
   },
   "outputs": [],
   "source": [
    "def cust_ensemble(D1, D2 , base_models, sample_frac = 0.6):\n",
    "  \n",
    "  base_model_names = [\"BM_\" + str(i)  for i in range(len(base_models))]\n",
    "\n",
    "  #Setting up target variable for D2\n",
    "  y_train_D2 = D2.y\n",
    "  X_train_D2 = D2.drop(['y'], axis=1)\n",
    "\n",
    "  prediction_df = pd.DataFrame()\n",
    "\n",
    "  #Training Base Learners on D1 \n",
    "  for i in tqdm(range(len(base_models))):\n",
    "\n",
    "    #Generating Sample from D1\n",
    "    sample = D1.sample(n=None, frac= sample_frac, replace=True).reset_index(drop=True)\n",
    "\n",
    "    #Setting up target variable for D1\n",
    "    y_train_D1 = sample.y\n",
    "    X_train_D1 = sample.drop(['y'], axis=1)\n",
    "\n",
    "    #Training Base Models on D1\n",
    "    base_models[i].fit(X_train_D1, y_train_D1)\n",
    "\n",
    "    #Predicting D2\n",
    "    base_model_pred = base_models[i].predict(X_train_D2)\n",
    "\n",
    "    #Attaching basemodel predictions to dataframe\n",
    "    prediction_df[base_model_names[i]] = base_model_pred\n",
    "\n",
    "  base_models_trained = base_models\n",
    "\n",
    "  return base_models_trained, prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4433349,
     "status": "ok",
     "timestamp": 1640171638323,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "esfHhQxywCkL",
    "outputId": "5d76e78c-609a-4e38-9c57-2f1c9b63ad8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [01:59<05:52, 58.68s/it][Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed: 22.6min finished\n",
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed:  1.5min finished\n",
      " 38%|███▊      | 3/8 [26:06<57:41, 692.38s/it][Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed: 24.1min finished\n",
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed:  1.4min finished\n",
      "100%|██████████| 8/8 [1:13:50<00:00, 553.80s/it]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"/content/drive/MyDrive/Case Study 01/Data/base_models.pckl\"):\n",
    "  base_models_trained, D2_pred_df = cust_ensemble(D1, D2, base_models, sample_frac = 0.6)\n",
    "\n",
    "  #Saving Trained models into disk.\n",
    "  with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models.pckl\", \"wb\") as file:\n",
    "    pickle.dump(base_models_trained, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xJL9ygYE0l7"
   },
   "outputs": [],
   "source": [
    "def test_pred(test_data, base_models_trained):\n",
    "  test_pred_df = pd.DataFrame()\n",
    "  for i in tqdm(range(len(base_models_trained))):\n",
    "    test_pred = base_models_trained[i].predict(test_data)\n",
    "    test_pred_df[\"BM_\" + str(i)] = test_pred\n",
    "  return test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416905,
     "status": "ok",
     "timestamp": 1640175012934,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "6Ro6N_MmzC80",
    "outputId": "ba9766c5-4387-4db7-9193-e3d55d7ea51d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:02<00:06,  1.11s/it][Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed:   46.6s finished\n",
      " 38%|███▊      | 3/8 [00:49<01:50, 22.01s/it][Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed:   46.1s finished\n",
      "100%|██████████| 8/8 [06:55<00:00, 51.88s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models.pckl\" , 'rb') as f:\n",
    "    base_models_trained = pickle.load(f)\n",
    "\n",
    "CV_pred_df = test_pred(X_cv, base_models_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1640175033833,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "oTZfbUqdF6yw",
    "outputId": "34157562-e7c8-49b3-b018-0d9b0a33ce01"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ff3fcac7-408f-48b9-9018-7aeb39e1247b\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_0</th>\n",
       "      <th>BM_1</th>\n",
       "      <th>BM_2</th>\n",
       "      <th>BM_3</th>\n",
       "      <th>BM_4</th>\n",
       "      <th>BM_5</th>\n",
       "      <th>BM_6</th>\n",
       "      <th>BM_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.421404</td>\n",
       "      <td>4.395725</td>\n",
       "      <td>4.589173</td>\n",
       "      <td>4.512558</td>\n",
       "      <td>4.649163</td>\n",
       "      <td>4.684261</td>\n",
       "      <td>4.513004</td>\n",
       "      <td>4.673037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.598166</td>\n",
       "      <td>4.395725</td>\n",
       "      <td>4.516213</td>\n",
       "      <td>4.495396</td>\n",
       "      <td>4.380764</td>\n",
       "      <td>4.602960</td>\n",
       "      <td>4.558722</td>\n",
       "      <td>4.755367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.878548</td>\n",
       "      <td>3.960264</td>\n",
       "      <td>3.795249</td>\n",
       "      <td>3.878863</td>\n",
       "      <td>3.453527</td>\n",
       "      <td>3.372506</td>\n",
       "      <td>3.300869</td>\n",
       "      <td>3.478809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.434559</td>\n",
       "      <td>4.775727</td>\n",
       "      <td>4.395662</td>\n",
       "      <td>4.404587</td>\n",
       "      <td>4.586109</td>\n",
       "      <td>4.416220</td>\n",
       "      <td>4.471923</td>\n",
       "      <td>4.559396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.575396</td>\n",
       "      <td>3.653439</td>\n",
       "      <td>3.895208</td>\n",
       "      <td>3.881914</td>\n",
       "      <td>4.095831</td>\n",
       "      <td>4.060068</td>\n",
       "      <td>3.910902</td>\n",
       "      <td>4.065018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff3fcac7-408f-48b9-9018-7aeb39e1247b')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ff3fcac7-408f-48b9-9018-7aeb39e1247b button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ff3fcac7-408f-48b9-9018-7aeb39e1247b');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       BM_0      BM_1      BM_2  ...      BM_5      BM_6      BM_7\n",
       "0  4.421404  4.395725  4.589173  ...  4.684261  4.513004  4.673037\n",
       "1  4.598166  4.395725  4.516213  ...  4.602960  4.558722  4.755367\n",
       "2  3.878548  3.960264  3.795249  ...  3.372506  3.300869  3.478809\n",
       "3  4.434559  4.775727  4.395662  ...  4.416220  4.471923  4.559396\n",
       "4  4.575396  3.653439  3.895208  ...  4.060068  3.910902  4.065018\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4U1IUr_wCn6"
   },
   "outputs": [],
   "source": [
    "# Config Meta Regressor LightGBM\n",
    "if not os.path.isfile(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg.pckl\"):\n",
    "  meta_reg = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='rmse', n_estimators=2100)\n",
    "  meta_reg.fit(D2_pred_df, y_train_D2)\n",
    "  with open(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg.pckl\", \"wb\") as file:\n",
    "    pickle.dump(meta_reg, file)\n",
    "else:\n",
    "  with open(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg.pckl\" , 'rb') as file:\n",
    "    meta_reg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecDo5b8MLm0X"
   },
   "outputs": [],
   "source": [
    "#Predicting test \n",
    "y_CV_pred = meta_reg.predict(CV_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 527,
     "status": "ok",
     "timestamp": 1640175547539,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "AunibzidOcJz",
    "outputId": "cc066cd1-0632-4f70-d6a2-2f711841bd46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing RMSE\n",
    "compute_RMSE(y_cv ,y_CV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm2hCRgF923f"
   },
   "source": [
    "### **4.9 Custom Stacking Regressor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kd-wHV2D923h"
   },
   "source": [
    "Attaching target variable to training data before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "executionInfo": {
     "elapsed": 563,
     "status": "ok",
     "timestamp": 1640500229149,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "V0xD5CDX923i",
    "outputId": "cffab7cc-5a9c-479e-9966-8ff96e163b96"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-62cb848a-5885-4d50-9611-9259f3091f3d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>building_id</th>\n",
       "      <th>meter</th>\n",
       "      <th>primary_use</th>\n",
       "      <th>square_feet</th>\n",
       "      <th>floor_info</th>\n",
       "      <th>air_temperature</th>\n",
       "      <th>cloud_coverage</th>\n",
       "      <th>precip_depth_1_hr</th>\n",
       "      <th>sea_level_pressure</th>\n",
       "      <th>wind_direction</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_Week</th>\n",
       "      <th>day_name</th>\n",
       "      <th>hour</th>\n",
       "      <th>season</th>\n",
       "      <th>RH</th>\n",
       "      <th>Cloud_base</th>\n",
       "      <th>WCI</th>\n",
       "      <th>D/N</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>9045.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>2.810547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>830.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>15120.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>2.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>831.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4065.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>3.087891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>832.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>26507.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>4.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>833.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6392.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.794922</td>\n",
       "      <td>1019.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.8125</td>\n",
       "      <td>623.5</td>\n",
       "      <td>28.65625</td>\n",
       "      <td>1</td>\n",
       "      <td>3.044922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62cb848a-5885-4d50-9611-9259f3091f3d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-62cb848a-5885-4d50-9611-9259f3091f3d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-62cb848a-5885-4d50-9611-9259f3091f3d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   building_id  meter  primary_use  ...       WCI  D/N         y\n",
       "0         46.0    0.0           11  ...  28.65625    1  2.810547\n",
       "1        830.0    0.0           15  ...  28.65625    1  2.773438\n",
       "2        831.0    0.0            1  ...  28.65625    1  3.087891\n",
       "3        832.0    0.0            9  ...  28.65625    1  4.011719\n",
       "4        833.0    0.0            9  ...  28.65625    1  3.044922\n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = X_train.copy()\n",
    "data['y'] = y_train\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aawPMW7vFAtt"
   },
   "outputs": [],
   "source": [
    "data['square_feet'] = np.log1p(data.square_feet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpELfTM1923k"
   },
   "outputs": [],
   "source": [
    "#Base Model Configuration\n",
    "LGBM_reg_1 = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', leaves = 40,\n",
    "                           n_estimators=1000, n_jobs=50, reg_lambda=0.01, feature_fraction = 0.85,\n",
    "                           learning_rate=0.05)\n",
    "LGBM_reg_2 = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', leaves = 60,\n",
    "                           n_estimators=1500, n_jobs=50, reg_lambda=0.1, feature_fraction = 1,\n",
    "                           learning_rate=0.01)\n",
    "LGBM_reg_3 = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', leaves = 80,\n",
    "                           n_estimators=1800, n_jobs=50, reg_lambda=2, feature_fraction = 0.85,\n",
    "                           learning_rate=0.01)\n",
    "LGBM_reg_4 = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', leaves = 150,\n",
    "                           n_estimators=2100, n_jobs=50, reg_lambda=8, feature_fraction = 1,\n",
    "                           learning_rate=0.01)\n",
    "LGBM_reg_5 = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', leaves = 500,\n",
    "                           n_estimators=2500, n_jobs=50, reg_lambda=10, feature_fraction = 0.85,\n",
    "                           learning_rate=0.01)\n",
    "\n",
    "\n",
    "catboost_reg_1 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 2100,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "catboost_reg_2 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 1500,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "catboost_reg_3 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 1300,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "catboost_reg_4 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 1200,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "catboost_reg_5 = CatBoostRegressor(iterations=None,learning_rate=None, n_estimators = 1000,\n",
    "                                 depth=None,loss_function='RMSE', task_type = 'GPU', verbose=False)\n",
    "\n",
    "\n",
    "#Base Model List\n",
    "base_models = [LGBM_reg_1, LGBM_reg_2, LGBM_reg_3, LGBM_reg_4, LGBM_reg_5, \n",
    "               catboost_reg_1, catboost_reg_2, catboost_reg_3, catboost_reg_4, catboost_reg_5]\n",
    "\n",
    "#Base Model Names\n",
    "#base_model_names = [str(type(i).__name__) + \"_pred\"  for i in base_models ]\n",
    "base_model_names = [\"BM_\" + str(i)  for i in range(len(base_models)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B60epvEH923l"
   },
   "outputs": [],
   "source": [
    "#Attaching target variable to training data before splitting\n",
    "data = X_train.copy()\n",
    "data['y'] = y_train\n",
    "\n",
    "#Diving Train data into two dataframes D1 and D2.\n",
    "D1 = data[:int(len(data)*0.5)]\n",
    "D2 = data[int(len(data)*0.5):]\n",
    "\n",
    "\n",
    "#Setting up target variable for D1\n",
    "y_train_D2 = D2.y\n",
    "X_train_D2 = D2.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn2FyrmV923m"
   },
   "outputs": [],
   "source": [
    "def cust_ensemble(D1, D2 , base_models, sample_frac = 0.6):\n",
    "  \n",
    "  base_model_names = [\"BM_\" + str(i)  for i in range(len(base_models))]\n",
    "\n",
    "  #Setting up target variable for D2\n",
    "  y_train_D2 = D2.y\n",
    "  X_train_D2 = D2.drop(['y'], axis=1)\n",
    "\n",
    "  prediction_df = pd.DataFrame()\n",
    "\n",
    "  #Training Base Learners on D1 \n",
    "  for i in tqdm(range(len(base_models))):\n",
    "\n",
    "    #Generating Sample from D1\n",
    "    sample = D1.sample(n=None, frac= sample_frac, replace=True).reset_index(drop=True)\n",
    "\n",
    "    #Setting up target variable for D1\n",
    "    y_train_D1 = sample.y\n",
    "    X_train_D1 = sample.drop(['y'], axis=1)\n",
    "\n",
    "    #Training Base Models on D1\n",
    "    base_models[i].fit(X_train_D1, y_train_D1)\n",
    "\n",
    "    #Predicting D2\n",
    "    base_model_pred = base_models[i].predict(X_train_D2)\n",
    "\n",
    "    #Attaching basemodel predictions to dataframe\n",
    "    prediction_df[base_model_names[i]] = base_model_pred\n",
    "\n",
    "  base_models_trained = base_models\n",
    "\n",
    "  return base_models_trained, prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4925257,
     "status": "ok",
     "timestamp": 1640505233448,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "T5Wt0iBhb7eR",
    "outputId": "7817b0c3-f752-44eb-ec3a-f981f0b52d34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:22:02<00:00, 492.26s/it]\n"
     ]
    }
   ],
   "source": [
    "base_models_trained, D2_pred_df = cust_ensemble(D1, D2, base_models, sample_frac = 0.7)\n",
    "\n",
    "#Saving Trained models into disk.\n",
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models_Rev02b.pckl\", \"wb\") as file:\n",
    "  pickle.dump(base_models_trained, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BspCaIFa923n"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"/content/drive/MyDrive/Case Study 01/Data/base_models_Rev02b.pckl\"):\n",
    "  base_models_trained, D2_pred_df = cust_ensemble(D1, D2, base_models, sample_frac = 0.7)\n",
    "\n",
    "  #Saving Trained models into disk.\n",
    "  with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models_Rev02b.pckl\", \"wb\") as file:\n",
    "    pickle.dump(base_models_trained, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxwC-Owi923o"
   },
   "outputs": [],
   "source": [
    "def test_pred(test_data, base_models_trained):\n",
    "  test_pred_df = pd.DataFrame()\n",
    "  for i in tqdm(range(len(base_models_trained))):\n",
    "    test_pred = base_models_trained[i].predict(test_data)\n",
    "    test_pred_df[\"BM_\" + str(i)] = test_pred\n",
    "  return test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 824965,
     "status": "ok",
     "timestamp": 1640506058403,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "AhsHVeu9923o",
    "outputId": "04f561d9-3119-4a59-b812-f83e24013794"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [13:44<00:00, 82.44s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models_Rev02b.pckl\" , 'rb') as f:\n",
    "    base_models_trained = pickle.load(f)\n",
    "\n",
    "CV_pred_df = test_pred(X_cv, base_models_trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1640506058408,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "anJTPAUv923p",
    "outputId": "4ed151fa-a456-4c3a-f59c-4127cf5ac734"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9d138f89-3c0d-417a-b677-874c374ca4d2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_0</th>\n",
       "      <th>BM_1</th>\n",
       "      <th>BM_2</th>\n",
       "      <th>BM_3</th>\n",
       "      <th>BM_4</th>\n",
       "      <th>BM_5</th>\n",
       "      <th>BM_6</th>\n",
       "      <th>BM_7</th>\n",
       "      <th>BM_8</th>\n",
       "      <th>BM_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.452112</td>\n",
       "      <td>4.428900</td>\n",
       "      <td>4.539385</td>\n",
       "      <td>4.483145</td>\n",
       "      <td>4.484542</td>\n",
       "      <td>4.368367</td>\n",
       "      <td>4.495602</td>\n",
       "      <td>4.373081</td>\n",
       "      <td>4.618345</td>\n",
       "      <td>4.460286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.483855</td>\n",
       "      <td>4.401315</td>\n",
       "      <td>4.436537</td>\n",
       "      <td>4.479903</td>\n",
       "      <td>4.434703</td>\n",
       "      <td>4.701753</td>\n",
       "      <td>4.644870</td>\n",
       "      <td>4.654875</td>\n",
       "      <td>4.676072</td>\n",
       "      <td>4.684875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.836917</td>\n",
       "      <td>3.752333</td>\n",
       "      <td>3.964703</td>\n",
       "      <td>3.792983</td>\n",
       "      <td>3.989542</td>\n",
       "      <td>3.461438</td>\n",
       "      <td>3.379353</td>\n",
       "      <td>3.450435</td>\n",
       "      <td>3.501292</td>\n",
       "      <td>3.466613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.280728</td>\n",
       "      <td>3.962946</td>\n",
       "      <td>4.138187</td>\n",
       "      <td>4.086804</td>\n",
       "      <td>4.146983</td>\n",
       "      <td>4.489341</td>\n",
       "      <td>4.429124</td>\n",
       "      <td>4.326776</td>\n",
       "      <td>4.438140</td>\n",
       "      <td>4.473698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.894228</td>\n",
       "      <td>3.746428</td>\n",
       "      <td>3.941727</td>\n",
       "      <td>3.785135</td>\n",
       "      <td>3.902116</td>\n",
       "      <td>4.047766</td>\n",
       "      <td>4.118484</td>\n",
       "      <td>3.964527</td>\n",
       "      <td>3.879841</td>\n",
       "      <td>3.900283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d138f89-3c0d-417a-b677-874c374ca4d2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9d138f89-3c0d-417a-b677-874c374ca4d2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9d138f89-3c0d-417a-b677-874c374ca4d2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       BM_0      BM_1      BM_2  ...      BM_7      BM_8      BM_9\n",
       "0  4.452112  4.428900  4.539385  ...  4.373081  4.618345  4.460286\n",
       "1  4.483855  4.401315  4.436537  ...  4.654875  4.676072  4.684875\n",
       "2  3.836917  3.752333  3.964703  ...  3.450435  3.501292  3.466613\n",
       "3  4.280728  3.962946  4.138187  ...  4.326776  4.438140  4.473698\n",
       "4  3.894228  3.746428  3.941727  ...  3.964527  3.879841  3.900283\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmAyJNNC923q"
   },
   "outputs": [],
   "source": [
    "# Config Meta Regressor LightGBM\n",
    "if not os.path.isfile(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg_Rev02.pckl\"):\n",
    "  meta_reg = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', n_estimators=2100)\n",
    "  meta_reg.fit(D2_pred_df, y_train_D2)\n",
    "  with open(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg_Rev02.pckl\", \"wb\") as file:\n",
    "    pickle.dump(meta_reg, file)\n",
    "else:\n",
    "  with open(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg_Rev02.pckl\" , 'rb') as file:\n",
    "    meta_reg = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379804,
     "status": "ok",
     "timestamp": 1640506814679,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "5My6zCvJInF5",
    "outputId": "3fc1c9de-40c0-426e-dc1b-3a2301f89cdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(n_estimators=2100, objective='regression')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_reg = LGBMRegressor(boosting_type='gbdt', num_leaves=31, objective='regression', n_estimators=2100)\n",
    "meta_reg.fit(D2_pred_df, y_train_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xreZKGjA923r"
   },
   "outputs": [],
   "source": [
    "#Predicting test \n",
    "y_CV_pred = meta_reg.predict(CV_pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1640506964200,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "5GjNd-Cr923s",
    "outputId": "7c9af76e-dfed-4b57-f469-79ad6d743447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing RMSE\n",
    "compute_RMSE(y_cv ,y_CV_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aefuodM1UJIz"
   },
   "source": [
    "# **5. Summary:** <br>\n",
    "**Representation of Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1639759878044,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "ay0epD-VIJG0",
    "outputId": "99e6b24f-e45d-4b40-dbea-bc89c31825fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------+------------------------------------------+--------------+-----------+-------------+----------+\n",
      "| Sr. No. |            Model             |               Best Params                | RMSLE(Train) | RMSLE(CV) | RMSE(Train) | RMSE(CV) |\n",
      "+---------+------------------------------+------------------------------------------+--------------+-----------+-------------+----------+\n",
      "|    1.   |          Baseline            |            Linear Regression             |     0.32     |    0.34   |     1.44    |   1.48   |\n",
      "|         |                              |                                          |              |           |             |          |\n",
      "|    2.   |       Decision Trees         |   max_depth=27, min_samples_split=100    |     0.08     |    0.17   |     0.32    |   0.68   |\n",
      "|         | (trained on 2000000 samples) |                                          |              |           |             |          |\n",
      "|         |                              |                                          |              |           |             |          |\n",
      "|    3.   |        Random Forest         | min_samples_split:50, n_estimators:250   |     0.17     |    0.20   |     0.65    |   0.76   |\n",
      "|         |                              |               Max_depth:15               |              |           |             |          |\n",
      "|    4.   |                              |                                          |              |           |             |          |\n",
      "|         |           LightGBM           | boosting=gbdt, 'min_child_samples': 31,  |     0.12     |    0.16   |     0.45    |   0.63   |\n",
      "|         |                              |               'n_est.=2100               |              |           |             |          |\n",
      "|    5.   |                              |                                          |              |           |             |          |\n",
      "|         |           CatBoost           |       boosting=gbdt, 'n_est.=2100        |     0.16     |    0.18   |     0.61    |   0.69   |\n",
      "|         |                              |                                          |              |           |             |          |\n",
      "|    6.   |                              |                                          |              |           |             |          |\n",
      "|         |             MLP              |       loss=mse, Adam, Epoch=300          |     0.14     |    0.18   |     0.56    |   0.71   |\n",
      "|         |                              |                                          |              |           |             |          |\n",
      "|    7.   |                              |                                          |              |           |             |          |\n",
      "|         |             LSTM             |       loss=mse, Adam, Epoch=250          |     0.22     |    0.23   |     0.93    |   0.95   |\n",
      "|         |                              |                                          |              |           |             |          |\n",
      "|    8.   |                              |                                          |              |           |             |          |\n",
      "|         |           Stacking           |            LGM + DT + RF +CB             |     0.13     |    0.17   |     0.48    |   0.63   |\n",
      "|         |                              |                                          |              |           |             |          |\n",
      "+---------+------------------------------+------------------------------------------+--------------+-----------+-------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "results = PrettyTable()\n",
    "results.field_names = [\"Sr. No.\",\"Model\",\"Best Params\", \"RMSLE(Train)\",\"RMSLE(CV)\", \"RMSE(Train)\", \"RMSE(CV)\"]\n",
    "results.add_row([\"1.\",\"Baseline \\n\", \"Linear Regression\" , \"0.32\", \"0.34\", \"1.44\", \"1.48\" ])\n",
    "results.add_row([\"2.\",\"Decision Trees \\n(trained on 2000000 samples)\\n\", \"max_depth=27, min_samples_split=100\", \"0.08\" , \"0.17\", \"0.32\", \"0.68\" ])\n",
    "results.add_row([\"3.\",\"Random Forest\\n\",\"min_samples_split:50, n_estimators:250 \\n Max_depth:15\", \"0.17\", \"0.20\", \"0.65\", \"0.76\" ])\n",
    "results.add_row([\"4.\",\"\\nLightGBM\\n\",\"\\nboosting=gbdt, 'min_child_samples': 31, \\n'n_est.=2100\", \"\\n0.12\", \"\\n0.16\", \"\\n0.45\",\"\\n0.63\"])\n",
    "results.add_row([\"5.\",\"\\nCatBoost\\n \",\"\\nboosting=gbdt, 'n_est.=2100\", \"\\n0.16\", \"\\n0.18\", \"\\n0.61\",\"\\n0.69\"])\n",
    "results.add_row([\"6.\",\"\\nMLP\\n\",\"\\nloss=mse, Adam, Epoch=300  \", \"\\n0.14\", \"\\n0.18\", \"\\n0.56\", \"\\n0.71\"])\n",
    "results.add_row([\"7.\",\"\\nLSTM\\n\",\"\\nloss=mse, Adam, Epoch=250  \", \"\\n0.22\", \"\\n0.23\", \"\\n0.93\", \"\\n0.95\"])\n",
    "results.add_row([\"8.\",\"\\nStacking\\n\",\"\\nLGM + DT + RF +CB \", \"\\n0.13\", \"\\n0.17\", \"\\n0.48\", \"\\n0.63\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw2c4RzPUfv2"
   },
   "source": [
    "# **6. Loading Test Data:**\n",
    "\n",
    "Preparing Test Data to make it compatible with trained models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7to_uDuX3EHr"
   },
   "source": [
    "##### **6.4.1 Test Data - unscaled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A0aYYEqLHZI"
   },
   "outputs": [],
   "source": [
    "#Loading data from the disk\n",
    "test_data = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/test_data_processed.ftr\")\n",
    "site_id_info_test = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/site_id_test_list.ftr\")\n",
    "meter_type_info_test = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/meter_type_test_list.ftr\")\n",
    "\n",
    "#Dropping Unwanted Features\n",
    "test_data = test_data.drop(['site_id', 'dew_temperature', 'year', 'Ea', 'Es'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IePJTRRK3P_6"
   },
   "source": [
    "##### **6.4.2 Test Data- Scaled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIQ5SXf604yx"
   },
   "outputs": [],
   "source": [
    "#Loading data from the disk\n",
    "test_data_scaled = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/test_data_processed_scaled.ftr\")\n",
    "site_id_info_test = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/site_id_test_list.ftr\")\n",
    "meter_type_info_test = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/meter_type_test_list.ftr\")\n",
    "\n",
    "#Dropping Unwanted Features\n",
    "test_data_scaled = test_data_scaled.drop(['site_id', 'dew_temperature', 'year', 'Ea', 'Es'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW4m2h88_fdJ"
   },
   "source": [
    "#### **6.5 Target Inverse Transformation**\n",
    "\n",
    "* As we have used log transformation on the target variable, Need to inverse it using Exponential Function.\n",
    "* For site_0 meter readings, we have converted the origional unites from to kBTU to KW. So before submission, need to convert back in their origional form i.e. in kBTU, i.e. to multiply by 3.4118. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXZQ_jkgP_kc"
   },
   "outputs": [],
   "source": [
    "def target_transf(site_id_info: \"site_ids from the given data \",\n",
    "                  meter_type_info:\"meter types of each datapoint\",\n",
    "                  predicted_op: \"predicted outputs from the model\") -> \"Returns Transfoemed Output\":\n",
    "  '''\n",
    "  1.As we have used log transformation on the target variable, Need to inverse it using Exponential Function.\n",
    "  2.For site_0 Electrical Meter readings, we have converted the origional unites from to kBTU to KW. So before submission,\n",
    "  need to convert back in their origional form i.e. in kBTU, i.e. to multiply by 3.4118.\n",
    "  '''\n",
    "\n",
    "  # Target Transformation\n",
    "  pred_df = site_id_info_test.copy()\n",
    "  pred_df['meter'] = meter_type_info_test\n",
    "  pred_df['test_pred'] = np.expm1(predicted_op)\n",
    "\n",
    "  # Unit Conversion for site_0\n",
    "  pred_df['test_pred']= np.where(np.logical_and(pred_df['site_id'] == 0, pred_df['meter'] == 0),\n",
    "                                  pred_df['test_pred'] * 3.4118,\n",
    "                                  pred_df['test_pred'])\n",
    "\n",
    "  # Dropping site_id feature\n",
    "  pred_df = pred_df.drop(['site_id', 'meter'], axis=1)\n",
    "    \n",
    "  # Adding row_id features are shown in sample_submission file\n",
    "  pred_df = pred_df.reset_index()\n",
    "  pred_df.columns = ['row_id', 'meter_reading']\n",
    "\n",
    "  return pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbXCksDdi2fo"
   },
   "source": [
    "# **7. Target Predictions:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8HJ5w-lQlIs"
   },
   "source": [
    "#### **7.1 Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsUXPfluQiiX"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "Base_model = joblib.load('/content/drive/MyDrive/Case Study 01/Data/base_model_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = Base_model.predict(test_data_scaled)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/Base_model_test_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1639588701599,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "YCGCFXSL9_bc",
    "outputId": "d28e0d3d-36b0-4f72-9a18-dfc3ebaf0940"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>85.594397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>82.763013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>84.379832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>96.248339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>186.050239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>63.575974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>57.234970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>59.981728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>67.814751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>104.523697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      85.594397\n",
       "1                1      82.763013\n",
       "2                2      84.379832\n",
       "3                3      96.248339\n",
       "4                4     186.050239\n",
       "...            ...            ...\n",
       "41697595  41697595      63.575974\n",
       "41697596  41697596      57.234970\n",
       "41697597  41697597      59.981728\n",
       "41697598  41697598      67.814751\n",
       "41697599  41697599     104.523697\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSXvDfYHD7eV"
   },
   "source": [
    "### **7.2 Decision Tree Regressor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ntLLTVvucrv"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "DT_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/dt_reg_best_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = DT_reg_best.predict(test_data)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/DT_test_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1639589743487,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "NHhc3JiHNaNM",
    "outputId": "e05e4631-54fc-4eb1-e339-1774708d9322"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>47.588415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.315888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.302626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>506.677322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>85.832775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>7.207627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>5.477344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>9.969417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>175.087696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>3.782775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      47.588415\n",
       "1                1      17.315888\n",
       "2                2       4.302626\n",
       "3                3     506.677322\n",
       "4                4      85.832775\n",
       "...            ...            ...\n",
       "41697595  41697595       7.207627\n",
       "41697596  41697596       5.477344\n",
       "41697597  41697597       9.969417\n",
       "41697598  41697598     175.087696\n",
       "41697599  41697599       3.782775\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtWjzZTIugzP"
   },
   "source": [
    "### **7.3 Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We85FMRLAwvM"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "RF_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data//RF_reg_depth15_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = RF_reg_best.predict(test_data)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/RF_test_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F49buTv1pPp6"
   },
   "source": [
    "### **7.4 LGB Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CnXsH7NeXfHh"
   },
   "source": [
    "##### **7.4.1 Model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdH0BQIIXsJT"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "LGBM_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = LGBM_reg_best.predict(test_data)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_Model1_pred_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1639593963245,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "7rAwLht3YJxr",
    "outputId": "5438f5a9-b3b0-4284-f294-507df3335391"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>66.762195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>17.998364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.230692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>114.943310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>104.784334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>5.712131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>4.580241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>6.560214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>164.992018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>4.483522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      66.762195\n",
       "1                1      17.998364\n",
       "2                2       1.230692\n",
       "3                3     114.943310\n",
       "4                4     104.784334\n",
       "...            ...            ...\n",
       "41697595  41697595       5.712131\n",
       "41697596  41697596       4.580241\n",
       "41697597  41697597       6.560214\n",
       "41697598  41697598     164.992018\n",
       "41697599  41697599       4.483522\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Em810uAXUE8"
   },
   "source": [
    "##### **7.4.2 Model 2: With Best Model:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1WX3At__ne8"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "LGBM_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/LGBM_reg_best_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = LGBM_reg_best.predict(test_data)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_Best_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VbkLd7kJQ20"
   },
   "outputs": [],
   "source": [
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_Best_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1639623104640,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "GFFbx9XBucmy",
    "outputId": "becf3a60-cc8a-4971-d359-aebb41908867"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36.417987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11.397480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.648718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>61.398948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>229.670441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>5.889718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>3.792710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>5.539892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>163.537119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>3.779553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      36.417987\n",
       "1                1      11.397480\n",
       "2                2       0.648718\n",
       "3                3      61.398948\n",
       "4                4     229.670441\n",
       "...            ...            ...\n",
       "41697595  41697595       5.889718\n",
       "41697596  41697596       3.792710\n",
       "41697597  41697597       5.539892\n",
       "41697598  41697598     163.537119\n",
       "41697599  41697599       3.779553\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auPXiqYrIH7Z"
   },
   "source": [
    "### **7.5 Catboost Regressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WWOkMJIwY4e"
   },
   "source": [
    "##### **7.5.1 Model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlfG9VcPwY43"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "catboost_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/catboost_reg_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = catboost_reg_best.predict(test_data)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/catboost_Model1_pred_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1639650654449,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "ud5OHnwowY44",
    "outputId": "34760ba3-615a-49a4-8c14-3a87fa25a636"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36.401948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16.369875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.990715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>128.592594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>357.549339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>5.991785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>4.900376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>5.666152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>156.571478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>4.337871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      36.401948\n",
       "1                1      16.369875\n",
       "2                2       2.990715\n",
       "3                3     128.592594\n",
       "4                4     357.549339\n",
       "...            ...            ...\n",
       "41697595  41697595       5.991785\n",
       "41697596  41697596       4.900376\n",
       "41697597  41697597       5.666152\n",
       "41697598  41697598     156.571478\n",
       "41697599  41697599       4.337871\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxYggvnWJCAC"
   },
   "source": [
    "##### **7.5.2 Model 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2hNf8yVJCAD"
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "catboost_reg_best = joblib.load('/content/drive/MyDrive/Case Study 01/Data/catboost_Model02_Rev02.sav')\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = catboost_reg_best.predict(test_data)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/catboost_Model2_pred_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1639657328316,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "A1uizPINJCAD",
    "outputId": "78cb21ea-d32b-4380-d801-e2855f90607c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42.662581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.690704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7.666489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>157.936348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>525.704051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>6.145453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>4.377842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>5.531190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>132.073055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>4.954326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      42.662581\n",
       "1                1      15.690704\n",
       "2                2       7.666489\n",
       "3                3     157.936348\n",
       "4                4     525.704051\n",
       "...            ...            ...\n",
       "41697595  41697595       6.145453\n",
       "41697596  41697596       4.377842\n",
       "41697597  41697597       5.531190\n",
       "41697598  41697598     132.073055\n",
       "41697599  41697599       4.954326\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Wqja52Xu73h"
   },
   "source": [
    "### **7.6 MLP Regerssion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bog38RYc6n1y"
   },
   "source": [
    "##### **7.6.1 Model 1: MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9YaE9WZzsIB"
   },
   "outputs": [],
   "source": [
    "#Loading saved model from disk\n",
    "MLP_reg = models.load_model(\"/content/drive/MyDrive/Case Study 01/MLP Output//MLP_reg_normal_Rev02\", custom_objects={'RMSE_error':RMSE_error, 'RMSLE_error':RMSLE_error})\n",
    "\n",
    "#Loading weights from best model\n",
    "MLP_reg.load_weights(\"/content/drive/MyDrive/Case Study 01/MLP Output/MLP_model01.h5\")\n",
    "\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = MLP_reg.predict(test_data_scaled)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/MLP_test_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1639729252453,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "Hr_kOOQNedWc",
    "outputId": "19eb9b53-9e25-4108-9488-6645ce4bdcc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0d81a2b8-ebd2-4887-8046-8448b1e6eb1d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.467510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>28.808020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29.740059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>65.213446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>97.704553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>7.537397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>5.795458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>6.678843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>41.216263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>203.222778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d81a2b8-ebd2-4887-8046-8448b1e6eb1d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0d81a2b8-ebd2-4887-8046-8448b1e6eb1d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0d81a2b8-ebd2-4887-8046-8448b1e6eb1d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      39.467510\n",
       "1                1      28.808020\n",
       "2                2      29.740059\n",
       "3                3      65.213446\n",
       "4                4      97.704553\n",
       "...            ...            ...\n",
       "41697595  41697595       7.537397\n",
       "41697596  41697596       5.795458\n",
       "41697597  41697597       6.678843\n",
       "41697598  41697598      41.216263\n",
       "41697599  41697599     203.222778\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUAbWykx63oh"
   },
   "source": [
    "##### **7.6.2 Model 2: LSTM MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iONlaARH6zy5"
   },
   "outputs": [],
   "source": [
    "#Loading saved model from disk\n",
    "LSTM_MLP_reg = models.load_model(\"/content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_model\", custom_objects={'RMSE_error':RMSE_error, 'RMSLE_error':RMSLE_error})\n",
    "\n",
    "#Loading weights from best model\n",
    "LSTM_MLP_reg.load_weights(\"/content/drive/MyDrive/Case Study 01/MLP Output/LSTM_MLP_best.h5\")\n",
    "\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = LSTM_MLP_reg.predict(test_data_scaled)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/LSTM_test_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1639741661672,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "6IRpOKpR6zy7",
    "outputId": "9b8bbd85-3866-40df-d764-ec858abae682"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f49fffda-2a3c-47d4-8116-5bb690ca44f4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60.013367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>23.319902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>34.759987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>154.458526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>469.002762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>14.877569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>6.396008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>6.350242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>61.509216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>108.344223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f49fffda-2a3c-47d4-8116-5bb690ca44f4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f49fffda-2a3c-47d4-8116-5bb690ca44f4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f49fffda-2a3c-47d4-8116-5bb690ca44f4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      60.013367\n",
       "1                1      23.319902\n",
       "2                2      34.759987\n",
       "3                3     154.458526\n",
       "4                4     469.002762\n",
       "...            ...            ...\n",
       "41697595  41697595      14.877569\n",
       "41697596  41697596       6.396008\n",
       "41697597  41697597       6.350242\n",
       "41697598  41697598      61.509216\n",
       "41697599  41697599     108.344223\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UekwHuzjPkAA"
   },
   "source": [
    "### **7.7 Stacking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhR8_gwzMRkB"
   },
   "outputs": [],
   "source": [
    "#Loading outputs from other models\n",
    "\n",
    "LGBM_test_predictions_1 = pd.read_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_Model1_pred_Rev02.csv')\n",
    "LGBM_test_predictions_2 = pd.read_csv('/content/drive/MyDrive/Case Study 01/Data/LGBM_Best_predictions_Rev02.csv')\n",
    "\n",
    "CatBoost_test_predictions_1 = pd.read_csv('/content/drive/MyDrive/Case Study 01/Data/catboost_Model1_pred_Rev02.csv')\n",
    "CatBoost_test_predictions_2 = pd.read_csv('/content/drive/MyDrive/Case Study 01/Data/catboost_Model2_pred_Rev02.csv')\n",
    "\n",
    "DT_test_predictions = pd.read_csv('/content/drive/MyDrive/Case Study 01/Data/DT_test_predictions_Rev02.csv')\n",
    "RF_test_predictions = pd.read_csv('/content/drive/MyDrive/Case Study 01/Data/RF_test_predictions_Rev02.csv')\n",
    "\n",
    "#Creating Dataframe\n",
    "y_test_pred_df = pd.DataFrame(list(zip(LGBM_test_predictions_1.meter_reading, LGBM_test_predictions_2.meter_reading,\n",
    "                                       CatBoost_test_predictions_1.meter_reading, CatBoost_test_predictions_2.meter_reading,\n",
    "                                       DT_test_predictions.meter_reading, RF_test_predictions.meter_reading)), \n",
    "                                       columns=[ 'y_LGBM_1', 'y_LGBM_2', 'y_CB_1', 'y_CB_2', 'y_DT', 'y_RF'])\n",
    "\n",
    "#Computing average meter reading from predictions\n",
    "y_test_pred_df['y_test_stack'] = y_test_pred_df.mean(axis=1)\n",
    "y_test_stack = pd.DataFrame(y_test_pred_df.y_test_stack)\n",
    "\n",
    "#Adding new feature - 'row_id'\n",
    "y_test_stack = y_test_stack.reset_index()\n",
    "y_test_stack.columns = ['row_id', 'meter_reading']\n",
    "\n",
    "#Saving predictions in disk\n",
    "y_test_stack.to_csv('/content/drive/MyDrive/Case Study 01/Data/Stacking_test_predictions_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1635531935776,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "aQRO1aeT4J8I",
    "outputId": "6e977d1d-153b-4154-8702-5995505b4045"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48.308323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.145976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8.974774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>177.003058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>144.545055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>6.947135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>5.373602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>7.445911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>148.385954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>5.018917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      48.308323\n",
       "1                1      18.145976\n",
       "2                2       8.974774\n",
       "3                3     177.003058\n",
       "4                4     144.545055\n",
       "...            ...            ...\n",
       "41697595  41697595       6.947135\n",
       "41697596  41697596       5.373602\n",
       "41697597  41697597       7.445911\n",
       "41697598  41697598     148.385954\n",
       "41697599  41697599       5.018917\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOaMwgw-Gi6c"
   },
   "source": [
    "### **7.8 Custom Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 997250,
     "status": "ok",
     "timestamp": 1640177728573,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "p0cEeFkKIoyl",
    "outputId": "f0c44a33-a342-49a3-ef65-cbfe637786d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:22<01:04, 10.74s/it][Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed:   50.2s finished\n",
      " 38%|███▊      | 3/8 [01:16<02:30, 30.11s/it][Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=50)]: Done 250 out of 250 | elapsed:   47.6s finished\n",
      "100%|██████████| 8/8 [16:35<00:00, 124.39s/it]\n"
     ]
    }
   ],
   "source": [
    "#Loading trained base_learners from the disk.\n",
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models.pckl\" , 'rb') as f:\n",
    "    base_models_trained = pickle.load(f)\n",
    "\n",
    "test_df = test_pred(test_data, base_models_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Bt5W7zuLkcq"
   },
   "outputs": [],
   "source": [
    "#Storig baselearner outputs\n",
    "test_df.to_feather(\"/content/drive/MyDrive/Case Study 01/Data/baselearner_op_test_df_Rev01.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1640177792620,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "PNJFwLLmNkHo",
    "outputId": "c45ed638-e20e-4bd0-c5f3-a0817954bcd5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1c3e3cfc-ccc2-4485-942c-95cb29b3271a\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_0</th>\n",
       "      <th>BM_1</th>\n",
       "      <th>BM_2</th>\n",
       "      <th>BM_3</th>\n",
       "      <th>BM_4</th>\n",
       "      <th>BM_5</th>\n",
       "      <th>BM_6</th>\n",
       "      <th>BM_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.542256</td>\n",
       "      <td>3.613530</td>\n",
       "      <td>2.464417</td>\n",
       "      <td>2.516501</td>\n",
       "      <td>2.972076</td>\n",
       "      <td>2.996624</td>\n",
       "      <td>3.303654</td>\n",
       "      <td>3.234284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.157096</td>\n",
       "      <td>3.297233</td>\n",
       "      <td>1.981345</td>\n",
       "      <td>1.950636</td>\n",
       "      <td>1.630631</td>\n",
       "      <td>1.741888</td>\n",
       "      <td>2.456361</td>\n",
       "      <td>2.268881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363932</td>\n",
       "      <td>1.943706</td>\n",
       "      <td>1.945412</td>\n",
       "      <td>2.043647</td>\n",
       "      <td>0.973257</td>\n",
       "      <td>1.109406</td>\n",
       "      <td>1.734632</td>\n",
       "      <td>1.979340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.641257</td>\n",
       "      <td>5.011304</td>\n",
       "      <td>3.351634</td>\n",
       "      <td>3.418360</td>\n",
       "      <td>3.354226</td>\n",
       "      <td>3.081380</td>\n",
       "      <td>4.136438</td>\n",
       "      <td>4.291514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.543174</td>\n",
       "      <td>3.370257</td>\n",
       "      <td>4.284699</td>\n",
       "      <td>4.412818</td>\n",
       "      <td>3.681417</td>\n",
       "      <td>3.789504</td>\n",
       "      <td>4.432597</td>\n",
       "      <td>5.057011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c3e3cfc-ccc2-4485-942c-95cb29b3271a')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1c3e3cfc-ccc2-4485-942c-95cb29b3271a button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1c3e3cfc-ccc2-4485-942c-95cb29b3271a');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       BM_0      BM_1      BM_2  ...      BM_5      BM_6      BM_7\n",
       "0  3.542256  3.613530  2.464417  ...  2.996624  3.303654  3.234284\n",
       "1  2.157096  3.297233  1.981345  ...  1.741888  2.456361  2.268881\n",
       "2  0.363932  1.943706  1.945412  ...  1.109406  1.734632  1.979340\n",
       "3  3.641257  5.011304  3.351634  ...  3.081380  4.136438  4.291514\n",
       "4  3.543174  3.370257  4.284699  ...  3.789504  4.432597  5.057011\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQvsSdkDGiUT"
   },
   "outputs": [],
   "source": [
    "#Loading outputs of base learners\n",
    "test_df = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/baselearner_op_test_df.ftr\")\n",
    "\n",
    "#Loading meta_regresser model from the disk\n",
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg.pckl\" , 'rb') as file:\n",
    "  meta_reg = pickle.load(file)\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = meta_reg.predict(test_df)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/custom_reg_prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1640178291616,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "_byGJECkOibC",
    "outputId": "2ddf8e5c-b38f-4b54-eca0-7ab35cccda79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-463c5bdb-f55c-41e5-9d9d-511bbd528ac1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>93.517592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8.585437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.774674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32.773975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>96.949003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>6.627540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>4.380962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>9.183973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>172.022688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>3.758414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-463c5bdb-f55c-41e5-9d9d-511bbd528ac1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-463c5bdb-f55c-41e5-9d9d-511bbd528ac1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-463c5bdb-f55c-41e5-9d9d-511bbd528ac1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      93.517592\n",
       "1                1       8.585437\n",
       "2                2       5.774674\n",
       "3                3      32.773975\n",
       "4                4      96.949003\n",
       "...            ...            ...\n",
       "41697595  41697595       6.627540\n",
       "41697596  41697596       4.380962\n",
       "41697597  41697597       9.183973\n",
       "41697598  41697598     172.022688\n",
       "41697599  41697599       3.758414\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rQnj7ktK4-6"
   },
   "source": [
    "### **7.9 Custom Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7923483,
     "status": "ok",
     "timestamp": 1640522826567,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "2xIu4QLlK4-7",
    "outputId": "32c39d7c-d249-4013-f5b8-3fe94c49fd33"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [2:12:02<00:00, 792.29s/it]\n"
     ]
    }
   ],
   "source": [
    "#Loading trained base_learners from the disk.\n",
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/base_models_Rev02b.pckl\" , 'rb') as f:\n",
    "    base_models_trained = pickle.load(f)\n",
    "\n",
    "test_df = test_pred(test_data, base_models_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1D-07EpK4-8"
   },
   "outputs": [],
   "source": [
    "#Storig baselearner outputs\n",
    "test_df.to_feather(\"/content/drive/MyDrive/Case Study 01/Data/baselearner_op_test_df_Rev02.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2603,
     "status": "ok",
     "timestamp": 1640522849516,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "rsTzhvwPK4-8",
    "outputId": "e8f90126-c8b9-432e-bd4d-8b69519da8a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ae1e90cd-bfcc-473c-b77b-4aabb852d494\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BM_0</th>\n",
       "      <th>BM_1</th>\n",
       "      <th>BM_2</th>\n",
       "      <th>BM_3</th>\n",
       "      <th>BM_4</th>\n",
       "      <th>BM_5</th>\n",
       "      <th>BM_6</th>\n",
       "      <th>BM_7</th>\n",
       "      <th>BM_8</th>\n",
       "      <th>BM_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.829619</td>\n",
       "      <td>2.928431</td>\n",
       "      <td>3.054557</td>\n",
       "      <td>3.195661</td>\n",
       "      <td>3.010272</td>\n",
       "      <td>2.768412</td>\n",
       "      <td>2.345564</td>\n",
       "      <td>2.626149</td>\n",
       "      <td>2.792737</td>\n",
       "      <td>3.189853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.790412</td>\n",
       "      <td>1.284506</td>\n",
       "      <td>1.482207</td>\n",
       "      <td>1.558794</td>\n",
       "      <td>1.752818</td>\n",
       "      <td>2.275566</td>\n",
       "      <td>2.268717</td>\n",
       "      <td>2.303551</td>\n",
       "      <td>1.969577</td>\n",
       "      <td>2.021859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.334717</td>\n",
       "      <td>1.953796</td>\n",
       "      <td>2.057871</td>\n",
       "      <td>2.057718</td>\n",
       "      <td>1.492493</td>\n",
       "      <td>1.808386</td>\n",
       "      <td>1.742917</td>\n",
       "      <td>1.826466</td>\n",
       "      <td>1.661085</td>\n",
       "      <td>1.742554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.238555</td>\n",
       "      <td>3.854506</td>\n",
       "      <td>3.668053</td>\n",
       "      <td>3.767655</td>\n",
       "      <td>3.622490</td>\n",
       "      <td>3.771842</td>\n",
       "      <td>4.306426</td>\n",
       "      <td>3.750273</td>\n",
       "      <td>4.169184</td>\n",
       "      <td>4.125807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.009966</td>\n",
       "      <td>5.414796</td>\n",
       "      <td>5.398893</td>\n",
       "      <td>5.140188</td>\n",
       "      <td>4.922506</td>\n",
       "      <td>4.474683</td>\n",
       "      <td>4.653741</td>\n",
       "      <td>4.821378</td>\n",
       "      <td>4.721032</td>\n",
       "      <td>4.821278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae1e90cd-bfcc-473c-b77b-4aabb852d494')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ae1e90cd-bfcc-473c-b77b-4aabb852d494 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ae1e90cd-bfcc-473c-b77b-4aabb852d494');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       BM_0      BM_1      BM_2  ...      BM_7      BM_8      BM_9\n",
       "0  2.829619  2.928431  3.054557  ...  2.626149  2.792737  3.189853\n",
       "1  1.790412  1.284506  1.482207  ...  2.303551  1.969577  2.021859\n",
       "2  1.334717  1.953796  2.057871  ...  1.826466  1.661085  1.742554\n",
       "3  3.238555  3.854506  3.668053  ...  3.750273  4.169184  4.125807\n",
       "4  4.009966  5.414796  5.398893  ...  4.821378  4.721032  4.821278\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDlJcUsAK4-8"
   },
   "outputs": [],
   "source": [
    "#Loading outputs of base learners\n",
    "test_df = pd.read_feather(\"/content/drive/MyDrive/Case Study 01/Data/baselearner_op_test_df_Rev02.ftr\")\n",
    "\n",
    "#Loading meta_regresser model from the disk\n",
    "with open(\"/content/drive/MyDrive/Case Study 01/Data/meta_reg_Rev02.pckl\" , 'rb') as file:\n",
    "  meta_reg = pickle.load(file)\n",
    "\n",
    "#Predicting Test Data\n",
    "test_pred = meta_reg.predict(test_df)\n",
    "test_pred = target_transf(site_id_info_test, meter_type_info_test, test_pred)\n",
    "\n",
    "#Saving predictions in disk\n",
    "test_pred.to_csv('/content/drive/MyDrive/Case Study 01/Data/custom_reg_prediction_Rev02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1640524564074,
     "user": {
      "displayName": "aaks dab",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghl4m0ptaCuFbtu1hx_ZN634LDh2D3aUTCYy3gDmQ=s64",
      "userId": "15415274412561598053"
     },
     "user_tz": -330
    },
    "id": "WkviqF3-K4-9",
    "outputId": "a69f2817-e2bb-4c19-c631-c80fcc2910cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-30bd09fc-5220-46b9-a3bc-d44736761ce7\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33.279090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.180715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.085724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>91.557403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>47.063574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>7.002379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>3.950683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>5.809380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>245.359243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>3.325953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30bd09fc-5220-46b9-a3bc-d44736761ce7')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-30bd09fc-5220-46b9-a3bc-d44736761ce7 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-30bd09fc-5220-46b9-a3bc-d44736761ce7');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0      33.279090\n",
       "1                1      13.180715\n",
       "2                2       4.085724\n",
       "3                3      91.557403\n",
       "4                4      47.063574\n",
       "...            ...            ...\n",
       "41697595  41697595       7.002379\n",
       "41697596  41697596       3.950683\n",
       "41697597  41697597       5.809380\n",
       "41697598  41697598     245.359243\n",
       "41697599  41697599       3.325953\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F-ExXdU4KRl"
   },
   "source": [
    "# **8. Submitting Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn1glrGXmzJT"
   },
   "source": [
    "**Kaggle Leaderboard Scoring:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e651ejQjO0L1"
   },
   "source": [
    "![Merged_document.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2qkkPtbKh1D"
   },
   "source": [
    "With LightGBM - GBDT model, I got **public score = 1.290** and **private score = 1.595.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpZQdGxSJiJj"
   },
   "source": [
    "# **8.Future Work:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZ1ExQ1VK7re"
   },
   "source": [
    "* As the dataset is huge, hyperparameter tuning and training the models such as SVR is not possible with the limited computational resources I have. Another idea is to train seperate model for each site. I would like to try this whenever I have enough resources available.\n",
    "* Many kaggle participants had used data leaks, which are avaible publicly,to improve the score further. I would like to try this as well with the models which I have trained, to see if the score can be improved further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBdZzszNm9rJ"
   },
   "source": [
    "`CONTD PART C: Post Training Analysis..`"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZQugFjeV19Pp",
    "25Pw3kU2opfu",
    "H9-SeMZ4GlMF",
    "kI2kwB4NGqZU"
   ],
   "machine_shape": "hm",
   "name": "B. CS_1 MODELS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
